{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sytong12/SEEM3650project/blob/main/nanogpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "gbDcw3xdtlpu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZbQmSKwy1Ey",
        "outputId": "81bca32d-f111-4798-87d6-31182601ef2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 686, done.\u001b[K\n",
            "remote: Total 686 (delta 0), reused 0 (delta 0), pack-reused 686 (from 1)\u001b[K\n",
            "Receiving objects: 100% (686/686), 954.03 KiB | 2.87 MiB/s, done.\n",
            "Resolving deltas: 100% (387/387), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUf2Kx61Wjk9",
        "outputId": "98dd4e86-a592-4957-d6bf-a160179e4c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy transformers datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbXmgphb3voo",
        "outputId": "bedd8032-5074-4dbe-e20f-30dd0ae5dda9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m108.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch numpy transformers datasets tiktoken wandb tqdm |grep Version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a5f4fe-630e-4b41-9de1-3ab47ef69d19",
        "id": "SLoi-1k4ZOLj"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Version: 2.6.0+cu124\n",
            "Version: 2.0.2\n",
            "Version 3.1, 31 March 2009\n",
            "                       Version 3, 29 June 2007\n",
            "  5. Conveying Modified Source Versions.\n",
            "  14. Revised Versions of this License.\n",
            "Version: 4.51.3\n",
            "Version: 3.5.1\n",
            "Version: 0.9.0\n",
            "Version: 0.19.10\n",
            "Version: 4.67.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training of model using shakespeare_char dataset"
      ],
      "metadata": {
        "id": "1J53XtlutQwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/shakespeare_char/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhZa6ofbzbzD",
        "outputId": "c14a948c-a9f7-4e3a-a30a-5cd068a957af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 1,115,394\n",
            "all the unique characters: \n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "vocab size: 65\n",
            "train has 1,003,854 tokens\n",
            "val has 111,540 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and sample using default settings"
      ],
      "metadata": {
        "id": "60AdIoLYtzwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --compile=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1Og4Bgw0kmN",
        "outputId": "cbd9e226-0c56-4209-a04a-a928f4892d07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "compile = False\n",
            "\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.65M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,740,096 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2874, val loss 4.2823\n",
            "iter 0: loss 4.2664, time 70064.01ms, mfu -100.00%\n",
            "iter 10: loss 3.1421, time 524.74ms, mfu 0.71%\n",
            "iter 20: loss 2.7345, time 530.23ms, mfu 0.71%\n",
            "iter 30: loss 2.6178, time 531.35ms, mfu 0.71%\n",
            "iter 40: loss 2.5734, time 534.05ms, mfu 0.71%\n",
            "iter 50: loss 2.5265, time 532.07ms, mfu 0.71%\n",
            "iter 60: loss 2.5110, time 530.05ms, mfu 0.71%\n",
            "iter 70: loss 2.4956, time 523.03ms, mfu 0.71%\n",
            "iter 80: loss 2.4940, time 523.85ms, mfu 0.71%\n",
            "iter 90: loss 2.4703, time 520.52ms, mfu 0.71%\n",
            "iter 100: loss 2.4566, time 519.48ms, mfu 0.71%\n",
            "iter 110: loss 2.4512, time 517.95ms, mfu 0.71%\n",
            "iter 120: loss 2.4283, time 515.85ms, mfu 0.71%\n",
            "iter 130: loss 2.4101, time 518.54ms, mfu 0.71%\n",
            "iter 140: loss 2.4184, time 516.28ms, mfu 0.71%\n",
            "iter 150: loss 2.4237, time 518.51ms, mfu 0.71%\n",
            "iter 160: loss 2.3839, time 516.22ms, mfu 0.71%\n",
            "iter 170: loss 2.3574, time 518.30ms, mfu 0.71%\n",
            "iter 180: loss 2.3153, time 521.57ms, mfu 0.71%\n",
            "iter 190: loss 2.2506, time 523.53ms, mfu 0.71%\n",
            "iter 200: loss 2.2119, time 522.98ms, mfu 0.71%\n",
            "iter 210: loss 2.1391, time 522.99ms, mfu 0.71%\n",
            "iter 220: loss 2.1315, time 522.87ms, mfu 0.71%\n",
            "iter 230: loss 2.0739, time 522.42ms, mfu 0.71%\n",
            "iter 240: loss 2.0785, time 524.46ms, mfu 0.71%\n",
            "step 250: train loss 1.9572, val loss 2.0615\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.0310, time 74067.36ms, mfu 0.64%\n",
            "iter 260: loss 1.9766, time 519.46ms, mfu 0.65%\n",
            "iter 270: loss 1.9743, time 521.52ms, mfu 0.66%\n",
            "iter 280: loss 1.9706, time 522.82ms, mfu 0.66%\n",
            "iter 290: loss 1.9249, time 520.53ms, mfu 0.67%\n",
            "iter 300: loss 1.9007, time 519.37ms, mfu 0.67%\n",
            "iter 310: loss 1.8542, time 522.22ms, mfu 0.68%\n",
            "iter 320: loss 1.8442, time 521.07ms, mfu 0.68%\n",
            "iter 330: loss 1.8144, time 520.82ms, mfu 0.68%\n",
            "iter 340: loss 1.7846, time 519.82ms, mfu 0.69%\n",
            "iter 350: loss 1.8244, time 521.32ms, mfu 0.69%\n",
            "iter 360: loss 1.7573, time 521.61ms, mfu 0.69%\n",
            "iter 370: loss 1.7368, time 520.25ms, mfu 0.69%\n",
            "iter 380: loss 1.7255, time 521.15ms, mfu 0.70%\n",
            "iter 390: loss 1.7323, time 520.58ms, mfu 0.70%\n",
            "iter 400: loss 1.7661, time 521.84ms, mfu 0.70%\n",
            "iter 410: loss 1.6972, time 521.68ms, mfu 0.70%\n",
            "iter 420: loss 1.7075, time 520.70ms, mfu 0.70%\n",
            "iter 430: loss 1.6903, time 523.36ms, mfu 0.70%\n",
            "iter 440: loss 1.6565, time 521.22ms, mfu 0.71%\n",
            "iter 450: loss 1.6497, time 520.63ms, mfu 0.71%\n",
            "iter 460: loss 1.5939, time 521.96ms, mfu 0.71%\n",
            "iter 470: loss 1.6546, time 522.21ms, mfu 0.71%\n",
            "iter 480: loss 1.6206, time 523.03ms, mfu 0.71%\n",
            "iter 490: loss 1.6009, time 520.40ms, mfu 0.71%\n",
            "step 500: train loss 1.5255, val loss 1.7345\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 1.6011, time 74290.32ms, mfu 0.64%\n",
            "iter 510: loss 1.6049, time 519.99ms, mfu 0.65%\n",
            "iter 520: loss 1.5938, time 523.49ms, mfu 0.65%\n",
            "iter 530: loss 1.5625, time 521.35ms, mfu 0.66%\n",
            "iter 540: loss 1.6213, time 521.85ms, mfu 0.66%\n",
            "iter 550: loss 1.5650, time 523.48ms, mfu 0.67%\n",
            "iter 560: loss 1.5658, time 524.41ms, mfu 0.67%\n",
            "iter 570: loss 1.5653, time 520.83ms, mfu 0.68%\n",
            "iter 580: loss 1.5399, time 522.97ms, mfu 0.68%\n",
            "iter 590: loss 1.4954, time 519.93ms, mfu 0.68%\n",
            "iter 600: loss 1.5169, time 522.83ms, mfu 0.69%\n",
            "iter 610: loss 1.5409, time 521.86ms, mfu 0.69%\n",
            "iter 620: loss 1.5249, time 522.80ms, mfu 0.69%\n",
            "iter 630: loss 1.5092, time 522.11ms, mfu 0.69%\n",
            "iter 640: loss 1.4691, time 523.69ms, mfu 0.70%\n",
            "iter 650: loss 1.5024, time 520.98ms, mfu 0.70%\n",
            "iter 660: loss 1.5116, time 522.49ms, mfu 0.70%\n",
            "iter 670: loss 1.4494, time 522.59ms, mfu 0.70%\n",
            "iter 680: loss 1.5110, time 521.21ms, mfu 0.70%\n",
            "iter 690: loss 1.4617, time 522.16ms, mfu 0.70%\n",
            "iter 700: loss 1.4864, time 522.68ms, mfu 0.70%\n",
            "iter 710: loss 1.4568, time 522.09ms, mfu 0.71%\n",
            "iter 720: loss 1.4428, time 520.84ms, mfu 0.71%\n",
            "iter 730: loss 1.4228, time 521.68ms, mfu 0.71%\n",
            "iter 740: loss 1.4255, time 521.98ms, mfu 0.71%\n",
            "step 750: train loss 1.3618, val loss 1.5939\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.4324, time 74289.59ms, mfu 0.64%\n",
            "iter 760: loss 1.4374, time 518.89ms, mfu 0.65%\n",
            "iter 770: loss 1.4199, time 522.37ms, mfu 0.65%\n",
            "iter 780: loss 1.4192, time 521.03ms, mfu 0.66%\n",
            "iter 790: loss 1.4182, time 520.57ms, mfu 0.66%\n",
            "iter 800: loss 1.4286, time 521.42ms, mfu 0.67%\n",
            "iter 810: loss 1.3963, time 521.61ms, mfu 0.67%\n",
            "iter 820: loss 1.4067, time 523.81ms, mfu 0.68%\n",
            "iter 830: loss 1.3897, time 522.29ms, mfu 0.68%\n",
            "iter 840: loss 1.3975, time 522.05ms, mfu 0.68%\n",
            "iter 850: loss 1.3888, time 521.90ms, mfu 0.69%\n",
            "iter 860: loss 1.3942, time 521.96ms, mfu 0.69%\n",
            "iter 870: loss 1.3948, time 523.10ms, mfu 0.69%\n",
            "iter 880: loss 1.3734, time 523.42ms, mfu 0.69%\n",
            "iter 890: loss 1.3830, time 520.57ms, mfu 0.70%\n",
            "iter 900: loss 1.3685, time 523.57ms, mfu 0.70%\n",
            "iter 910: loss 1.3202, time 522.79ms, mfu 0.70%\n",
            "iter 920: loss 1.3628, time 521.06ms, mfu 0.70%\n",
            "iter 930: loss 1.3630, time 520.04ms, mfu 0.70%\n",
            "iter 940: loss 1.3375, time 520.65ms, mfu 0.70%\n",
            "iter 950: loss 1.3487, time 522.95ms, mfu 0.70%\n",
            "iter 960: loss 1.3638, time 523.66ms, mfu 0.71%\n",
            "iter 970: loss 1.3546, time 523.48ms, mfu 0.71%\n",
            "iter 980: loss 1.3575, time 520.15ms, mfu 0.71%\n",
            "iter 990: loss 1.3372, time 520.38ms, mfu 0.71%\n",
            "step 1000: train loss 1.2700, val loss 1.5153\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.3323, time 74310.21ms, mfu 0.64%\n",
            "iter 1010: loss 1.3346, time 519.61ms, mfu 0.65%\n",
            "iter 1020: loss 1.3133, time 521.92ms, mfu 0.65%\n",
            "iter 1030: loss 1.3390, time 520.83ms, mfu 0.66%\n",
            "iter 1040: loss 1.3635, time 522.41ms, mfu 0.66%\n",
            "iter 1050: loss 1.2978, time 520.79ms, mfu 0.67%\n",
            "iter 1060: loss 1.3423, time 524.08ms, mfu 0.67%\n",
            "iter 1070: loss 1.3344, time 522.14ms, mfu 0.68%\n",
            "iter 1080: loss 1.3348, time 521.64ms, mfu 0.68%\n",
            "iter 1090: loss 1.3509, time 524.18ms, mfu 0.68%\n",
            "iter 1100: loss 1.3144, time 524.02ms, mfu 0.69%\n",
            "iter 1110: loss 1.2982, time 523.22ms, mfu 0.69%\n",
            "iter 1120: loss 1.3019, time 522.47ms, mfu 0.69%\n",
            "iter 1130: loss 1.2981, time 524.46ms, mfu 0.69%\n",
            "iter 1140: loss 1.2975, time 523.62ms, mfu 0.70%\n",
            "iter 1150: loss 1.3056, time 522.60ms, mfu 0.70%\n",
            "iter 1160: loss 1.3293, time 522.49ms, mfu 0.70%\n",
            "iter 1170: loss 1.2993, time 522.64ms, mfu 0.70%\n",
            "iter 1180: loss 1.3187, time 522.72ms, mfu 0.70%\n",
            "iter 1190: loss 1.2622, time 522.99ms, mfu 0.70%\n",
            "iter 1200: loss 1.2932, time 522.43ms, mfu 0.70%\n",
            "iter 1210: loss 1.2590, time 519.94ms, mfu 0.70%\n",
            "iter 1220: loss 1.3061, time 521.41ms, mfu 0.71%\n",
            "iter 1230: loss 1.2969, time 522.77ms, mfu 0.71%\n",
            "iter 1240: loss 1.3021, time 522.66ms, mfu 0.71%\n",
            "step 1250: train loss 1.2038, val loss 1.4965\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.2697, time 74383.54ms, mfu 0.64%\n",
            "iter 1260: loss 1.2837, time 521.41ms, mfu 0.64%\n",
            "iter 1270: loss 1.2645, time 522.88ms, mfu 0.65%\n",
            "iter 1280: loss 1.2518, time 521.68ms, mfu 0.66%\n",
            "iter 1290: loss 1.2843, time 521.82ms, mfu 0.66%\n",
            "iter 1300: loss 1.3016, time 523.23ms, mfu 0.67%\n",
            "iter 1310: loss 1.2394, time 522.81ms, mfu 0.67%\n",
            "iter 1320: loss 1.3073, time 523.16ms, mfu 0.68%\n",
            "iter 1330: loss 1.2619, time 523.85ms, mfu 0.68%\n",
            "iter 1340: loss 1.2983, time 521.91ms, mfu 0.68%\n",
            "iter 1350: loss 1.2583, time 523.51ms, mfu 0.69%\n",
            "iter 1360: loss 1.2772, time 523.82ms, mfu 0.69%\n",
            "iter 1370: loss 1.2551, time 524.30ms, mfu 0.69%\n",
            "iter 1380: loss 1.2624, time 523.36ms, mfu 0.69%\n",
            "iter 1390: loss 1.2475, time 522.44ms, mfu 0.70%\n",
            "iter 1400: loss 1.2503, time 522.16ms, mfu 0.70%\n",
            "iter 1410: loss 1.2437, time 523.74ms, mfu 0.70%\n",
            "iter 1420: loss 1.2690, time 522.41ms, mfu 0.70%\n",
            "iter 1430: loss 1.2472, time 527.76ms, mfu 0.70%\n",
            "iter 1440: loss 1.2456, time 521.52ms, mfu 0.70%\n",
            "iter 1450: loss 1.2310, time 522.35ms, mfu 0.70%\n",
            "iter 1460: loss 1.2380, time 522.40ms, mfu 0.70%\n",
            "iter 1470: loss 1.2198, time 523.33ms, mfu 0.70%\n",
            "iter 1480: loss 1.2060, time 522.62ms, mfu 0.71%\n",
            "iter 1490: loss 1.2349, time 522.21ms, mfu 0.71%\n",
            "step 1500: train loss 1.1492, val loss 1.4737\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.1842, time 74364.92ms, mfu 0.64%\n",
            "iter 1510: loss 1.2325, time 520.56ms, mfu 0.64%\n",
            "iter 1520: loss 1.2276, time 523.17ms, mfu 0.65%\n",
            "iter 1530: loss 1.2568, time 523.50ms, mfu 0.66%\n",
            "iter 1540: loss 1.1957, time 523.54ms, mfu 0.66%\n",
            "iter 1550: loss 1.2305, time 524.72ms, mfu 0.67%\n",
            "iter 1560: loss 1.2036, time 523.72ms, mfu 0.67%\n",
            "iter 1570: loss 1.2350, time 522.36ms, mfu 0.68%\n",
            "iter 1580: loss 1.2088, time 523.92ms, mfu 0.68%\n",
            "iter 1590: loss 1.1906, time 524.16ms, mfu 0.68%\n",
            "iter 1600: loss 1.1949, time 522.17ms, mfu 0.69%\n",
            "iter 1610: loss 1.2376, time 521.89ms, mfu 0.69%\n",
            "iter 1620: loss 1.1775, time 521.28ms, mfu 0.69%\n",
            "iter 1630: loss 1.2023, time 519.86ms, mfu 0.69%\n",
            "iter 1640: loss 1.1954, time 521.23ms, mfu 0.70%\n",
            "iter 1650: loss 1.1817, time 524.35ms, mfu 0.70%\n",
            "iter 1660: loss 1.2117, time 522.37ms, mfu 0.70%\n",
            "iter 1670: loss 1.2043, time 523.03ms, mfu 0.70%\n",
            "iter 1680: loss 1.2003, time 521.79ms, mfu 0.70%\n",
            "iter 1690: loss 1.2007, time 524.28ms, mfu 0.70%\n",
            "iter 1700: loss 1.1890, time 523.48ms, mfu 0.70%\n",
            "iter 1710: loss 1.1795, time 522.77ms, mfu 0.70%\n",
            "iter 1720: loss 1.1815, time 524.48ms, mfu 0.71%\n",
            "iter 1730: loss 1.2069, time 520.42ms, mfu 0.71%\n",
            "iter 1740: loss 1.1728, time 522.08ms, mfu 0.71%\n",
            "step 1750: train loss 1.1006, val loss 1.4642\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.1908, time 74613.09ms, mfu 0.64%\n",
            "iter 1760: loss 1.1899, time 518.39ms, mfu 0.64%\n",
            "iter 1770: loss 1.1983, time 523.04ms, mfu 0.65%\n",
            "iter 1780: loss 1.1889, time 523.58ms, mfu 0.66%\n",
            "iter 1790: loss 1.1878, time 522.84ms, mfu 0.66%\n",
            "iter 1800: loss 1.1818, time 523.05ms, mfu 0.67%\n",
            "iter 1810: loss 1.1610, time 522.20ms, mfu 0.67%\n",
            "iter 1820: loss 1.1642, time 523.78ms, mfu 0.68%\n",
            "iter 1830: loss 1.1752, time 523.41ms, mfu 0.68%\n",
            "iter 1840: loss 1.1591, time 523.46ms, mfu 0.68%\n",
            "iter 1850: loss 1.1538, time 522.65ms, mfu 0.69%\n",
            "iter 1860: loss 1.1728, time 523.64ms, mfu 0.69%\n",
            "iter 1870: loss 1.1443, time 522.32ms, mfu 0.69%\n",
            "iter 1880: loss 1.1845, time 523.02ms, mfu 0.69%\n",
            "iter 1890: loss 1.1780, time 523.13ms, mfu 0.70%\n",
            "iter 1900: loss 1.1288, time 523.55ms, mfu 0.70%\n",
            "iter 1910: loss 1.1669, time 522.93ms, mfu 0.70%\n",
            "iter 1920: loss 1.1693, time 520.57ms, mfu 0.70%\n",
            "iter 1930: loss 1.1456, time 520.50ms, mfu 0.70%\n",
            "iter 1940: loss 1.1280, time 524.15ms, mfu 0.70%\n",
            "iter 1950: loss 1.1340, time 522.43ms, mfu 0.70%\n",
            "iter 1960: loss 1.1522, time 523.30ms, mfu 0.70%\n",
            "iter 1970: loss 1.1480, time 522.41ms, mfu 0.71%\n",
            "iter 1980: loss 1.1362, time 523.41ms, mfu 0.71%\n",
            "iter 1990: loss 1.1530, time 522.70ms, mfu 0.71%\n",
            "step 2000: train loss 1.0547, val loss 1.4796\n",
            "iter 2000: loss 1.1226, time 74147.51ms, mfu 0.64%\n",
            "iter 2010: loss 1.1255, time 521.12ms, mfu 0.64%\n",
            "iter 2020: loss 1.1243, time 522.18ms, mfu 0.65%\n",
            "iter 2030: loss 1.1584, time 523.78ms, mfu 0.66%\n",
            "iter 2040: loss 1.1426, time 521.38ms, mfu 0.66%\n",
            "iter 2050: loss 1.1130, time 522.20ms, mfu 0.67%\n",
            "iter 2060: loss 1.1011, time 521.54ms, mfu 0.67%\n",
            "iter 2070: loss 1.1274, time 522.55ms, mfu 0.68%\n",
            "iter 2080: loss 1.1194, time 524.72ms, mfu 0.68%\n",
            "iter 2090: loss 1.1280, time 523.05ms, mfu 0.68%\n",
            "iter 2100: loss 1.1346, time 522.59ms, mfu 0.69%\n",
            "iter 2110: loss 1.1318, time 523.24ms, mfu 0.69%\n",
            "iter 2120: loss 1.1221, time 521.88ms, mfu 0.69%\n",
            "iter 2130: loss 1.1357, time 523.23ms, mfu 0.69%\n",
            "iter 2140: loss 1.1401, time 523.11ms, mfu 0.70%\n",
            "iter 2150: loss 1.1197, time 520.36ms, mfu 0.70%\n",
            "iter 2160: loss 1.1464, time 523.40ms, mfu 0.70%\n",
            "iter 2170: loss 1.1307, time 522.49ms, mfu 0.70%\n",
            "iter 2180: loss 1.1100, time 521.29ms, mfu 0.70%\n",
            "iter 2190: loss 1.1047, time 521.32ms, mfu 0.70%\n",
            "iter 2200: loss 1.1236, time 521.05ms, mfu 0.70%\n",
            "iter 2210: loss 1.1144, time 522.89ms, mfu 0.71%\n",
            "iter 2220: loss 1.1241, time 520.84ms, mfu 0.71%\n",
            "iter 2230: loss 1.1171, time 522.69ms, mfu 0.71%\n",
            "iter 2240: loss 1.1257, time 521.73ms, mfu 0.71%\n",
            "step 2250: train loss 1.0074, val loss 1.4860\n",
            "iter 2250: loss 1.1084, time 74100.96ms, mfu 0.64%\n",
            "iter 2260: loss 1.1099, time 523.53ms, mfu 0.64%\n",
            "iter 2270: loss 1.1240, time 523.13ms, mfu 0.65%\n",
            "iter 2280: loss 1.0932, time 521.22ms, mfu 0.66%\n",
            "iter 2290: loss 1.1433, time 521.73ms, mfu 0.66%\n",
            "iter 2300: loss 1.1203, time 522.51ms, mfu 0.67%\n",
            "iter 2310: loss 1.0976, time 522.66ms, mfu 0.67%\n",
            "iter 2320: loss 1.0934, time 522.23ms, mfu 0.68%\n",
            "iter 2330: loss 1.0922, time 525.90ms, mfu 0.68%\n",
            "iter 2340: loss 1.1130, time 523.73ms, mfu 0.68%\n",
            "iter 2350: loss 1.1003, time 522.96ms, mfu 0.69%\n",
            "iter 2360: loss 1.1007, time 523.30ms, mfu 0.69%\n",
            "iter 2370: loss 1.0830, time 523.89ms, mfu 0.69%\n",
            "iter 2380: loss 1.0754, time 522.36ms, mfu 0.69%\n",
            "iter 2390: loss 1.0769, time 523.08ms, mfu 0.70%\n",
            "iter 2400: loss 1.0788, time 522.10ms, mfu 0.70%\n",
            "iter 2410: loss 1.0749, time 523.07ms, mfu 0.70%\n",
            "iter 2420: loss 1.0747, time 522.95ms, mfu 0.70%\n",
            "iter 2430: loss 1.0553, time 523.15ms, mfu 0.70%\n",
            "iter 2440: loss 1.0658, time 520.90ms, mfu 0.70%\n",
            "iter 2450: loss 1.0727, time 522.21ms, mfu 0.70%\n",
            "iter 2460: loss 1.0874, time 523.71ms, mfu 0.70%\n",
            "iter 2470: loss 1.0934, time 524.88ms, mfu 0.71%\n",
            "iter 2480: loss 1.0832, time 523.14ms, mfu 0.71%\n",
            "iter 2490: loss 1.0571, time 522.89ms, mfu 0.71%\n",
            "step 2500: train loss 0.9589, val loss 1.4922\n",
            "iter 2500: loss 1.0830, time 74210.15ms, mfu 0.64%\n",
            "iter 2510: loss 1.0662, time 524.15ms, mfu 0.64%\n",
            "iter 2520: loss 1.0469, time 522.39ms, mfu 0.65%\n",
            "iter 2530: loss 1.0509, time 523.24ms, mfu 0.66%\n",
            "iter 2540: loss 1.0508, time 523.23ms, mfu 0.66%\n",
            "iter 2550: loss 1.0610, time 522.39ms, mfu 0.67%\n",
            "iter 2560: loss 1.0571, time 522.02ms, mfu 0.67%\n",
            "iter 2570: loss 1.0765, time 523.66ms, mfu 0.68%\n",
            "iter 2580: loss 1.0836, time 522.70ms, mfu 0.68%\n",
            "iter 2590: loss 1.0685, time 524.30ms, mfu 0.68%\n",
            "iter 2600: loss 1.0632, time 525.07ms, mfu 0.69%\n",
            "iter 2610: loss 1.0462, time 524.31ms, mfu 0.69%\n",
            "iter 2620: loss 1.0462, time 522.32ms, mfu 0.69%\n",
            "iter 2630: loss 1.0235, time 523.50ms, mfu 0.69%\n",
            "iter 2640: loss 1.0444, time 522.39ms, mfu 0.69%\n",
            "iter 2650: loss 1.0670, time 523.71ms, mfu 0.70%\n",
            "iter 2660: loss 1.0396, time 523.05ms, mfu 0.70%\n",
            "iter 2670: loss 1.0229, time 521.91ms, mfu 0.70%\n",
            "iter 2680: loss 1.0507, time 522.54ms, mfu 0.70%\n",
            "iter 2690: loss 1.0481, time 522.78ms, mfu 0.70%\n",
            "iter 2700: loss 1.0228, time 522.68ms, mfu 0.70%\n",
            "iter 2710: loss 1.0470, time 523.36ms, mfu 0.70%\n",
            "iter 2720: loss 1.0383, time 523.86ms, mfu 0.70%\n",
            "iter 2730: loss 1.0547, time 525.11ms, mfu 0.71%\n",
            "iter 2740: loss 1.0168, time 523.01ms, mfu 0.71%\n",
            "step 2750: train loss 0.9126, val loss 1.5167\n",
            "iter 2750: loss 1.0390, time 74224.32ms, mfu 0.64%\n",
            "iter 2760: loss 1.0322, time 523.67ms, mfu 0.64%\n",
            "iter 2770: loss 1.0277, time 523.60ms, mfu 0.65%\n",
            "iter 2780: loss 1.0257, time 524.54ms, mfu 0.66%\n",
            "iter 2790: loss 1.0345, time 523.75ms, mfu 0.66%\n",
            "iter 2800: loss 1.0121, time 525.01ms, mfu 0.67%\n",
            "iter 2810: loss 1.0404, time 524.17ms, mfu 0.67%\n",
            "iter 2820: loss 1.0232, time 521.26ms, mfu 0.68%\n",
            "iter 2830: loss 1.0233, time 523.29ms, mfu 0.68%\n",
            "iter 2840: loss 0.9875, time 523.03ms, mfu 0.68%\n",
            "iter 2850: loss 1.0264, time 523.40ms, mfu 0.69%\n",
            "iter 2860: loss 1.0144, time 524.55ms, mfu 0.69%\n",
            "iter 2870: loss 1.0071, time 524.51ms, mfu 0.69%\n",
            "iter 2880: loss 1.0320, time 523.51ms, mfu 0.69%\n",
            "iter 2890: loss 1.0060, time 522.50ms, mfu 0.69%\n",
            "iter 2900: loss 0.9901, time 523.54ms, mfu 0.70%\n",
            "iter 2910: loss 1.0350, time 524.31ms, mfu 0.70%\n",
            "iter 2920: loss 1.0090, time 522.05ms, mfu 0.70%\n",
            "iter 2930: loss 0.9903, time 522.92ms, mfu 0.70%\n",
            "iter 2940: loss 0.9862, time 525.05ms, mfu 0.70%\n",
            "iter 2950: loss 1.0284, time 524.40ms, mfu 0.70%\n",
            "iter 2960: loss 1.0035, time 523.95ms, mfu 0.70%\n",
            "iter 2970: loss 0.9926, time 523.84ms, mfu 0.70%\n",
            "iter 2980: loss 0.9981, time 523.66ms, mfu 0.70%\n",
            "iter 2990: loss 0.9831, time 523.20ms, mfu 0.71%\n",
            "step 3000: train loss 0.8664, val loss 1.5314\n",
            "iter 3000: loss 0.9835, time 74100.48ms, mfu 0.64%\n",
            "iter 3010: loss 0.9924, time 522.91ms, mfu 0.64%\n",
            "iter 3020: loss 1.0058, time 522.79ms, mfu 0.65%\n",
            "iter 3030: loss 1.0041, time 523.80ms, mfu 0.66%\n",
            "iter 3040: loss 1.0294, time 524.06ms, mfu 0.66%\n",
            "iter 3050: loss 0.9750, time 524.27ms, mfu 0.67%\n",
            "iter 3060: loss 0.9981, time 523.42ms, mfu 0.67%\n",
            "iter 3070: loss 1.0173, time 524.14ms, mfu 0.68%\n",
            "iter 3080: loss 0.9948, time 521.87ms, mfu 0.68%\n",
            "iter 3090: loss 0.9845, time 522.38ms, mfu 0.68%\n",
            "iter 3100: loss 1.0023, time 522.62ms, mfu 0.69%\n",
            "iter 3110: loss 0.9691, time 522.22ms, mfu 0.69%\n",
            "iter 3120: loss 0.9939, time 522.61ms, mfu 0.69%\n",
            "iter 3130: loss 0.9874, time 522.33ms, mfu 0.69%\n",
            "iter 3140: loss 0.9783, time 522.69ms, mfu 0.70%\n",
            "iter 3150: loss 0.9892, time 522.96ms, mfu 0.70%\n",
            "iter 3160: loss 1.0131, time 522.10ms, mfu 0.70%\n",
            "iter 3170: loss 0.9670, time 523.22ms, mfu 0.70%\n",
            "iter 3180: loss 0.9737, time 524.73ms, mfu 0.70%\n",
            "iter 3190: loss 0.9911, time 521.54ms, mfu 0.70%\n",
            "iter 3200: loss 0.9663, time 521.22ms, mfu 0.70%\n",
            "iter 3210: loss 0.9650, time 522.99ms, mfu 0.70%\n",
            "iter 3220: loss 0.9515, time 522.63ms, mfu 0.71%\n",
            "iter 3230: loss 0.9562, time 521.62ms, mfu 0.71%\n",
            "iter 3240: loss 0.9497, time 521.72ms, mfu 0.71%\n",
            "step 3250: train loss 0.8214, val loss 1.5658\n",
            "iter 3250: loss 0.9667, time 74120.15ms, mfu 0.64%\n",
            "iter 3260: loss 0.9585, time 525.54ms, mfu 0.64%\n",
            "iter 3270: loss 0.9728, time 523.90ms, mfu 0.65%\n",
            "iter 3280: loss 0.9509, time 525.27ms, mfu 0.66%\n",
            "iter 3290: loss 0.9470, time 523.93ms, mfu 0.66%\n",
            "iter 3300: loss 0.9409, time 521.84ms, mfu 0.67%\n",
            "iter 3310: loss 0.9521, time 522.99ms, mfu 0.67%\n",
            "iter 3320: loss 0.9651, time 521.26ms, mfu 0.68%\n",
            "iter 3330: loss 0.9629, time 523.44ms, mfu 0.68%\n",
            "iter 3340: loss 0.9544, time 522.14ms, mfu 0.68%\n",
            "iter 3350: loss 0.9581, time 524.03ms, mfu 0.69%\n",
            "iter 3360: loss 0.9250, time 522.10ms, mfu 0.69%\n",
            "iter 3370: loss 0.9618, time 524.94ms, mfu 0.69%\n",
            "iter 3380: loss 0.9395, time 524.01ms, mfu 0.69%\n",
            "iter 3390: loss 0.9500, time 523.65ms, mfu 0.69%\n",
            "iter 3400: loss 0.9586, time 525.21ms, mfu 0.70%\n",
            "iter 3410: loss 0.9375, time 522.39ms, mfu 0.70%\n",
            "iter 3420: loss 0.9480, time 522.95ms, mfu 0.70%\n",
            "iter 3430: loss 0.9439, time 524.21ms, mfu 0.70%\n",
            "iter 3440: loss 0.9705, time 521.18ms, mfu 0.70%\n",
            "iter 3450: loss 0.9469, time 524.53ms, mfu 0.70%\n",
            "iter 3460: loss 0.9396, time 525.00ms, mfu 0.70%\n",
            "iter 3470: loss 0.9445, time 522.13ms, mfu 0.70%\n",
            "iter 3480: loss 0.9549, time 523.98ms, mfu 0.71%\n",
            "iter 3490: loss 0.9161, time 525.60ms, mfu 0.71%\n",
            "step 3500: train loss 0.7784, val loss 1.5784\n",
            "iter 3500: loss 0.8996, time 74210.56ms, mfu 0.64%\n",
            "iter 3510: loss 0.9173, time 524.94ms, mfu 0.64%\n",
            "iter 3520: loss 0.9231, time 522.35ms, mfu 0.65%\n",
            "iter 3530: loss 0.9523, time 523.45ms, mfu 0.66%\n",
            "iter 3540: loss 0.9297, time 522.31ms, mfu 0.66%\n",
            "iter 3550: loss 0.9277, time 524.40ms, mfu 0.67%\n",
            "iter 3560: loss 0.9519, time 522.54ms, mfu 0.67%\n",
            "iter 3570: loss 0.9348, time 523.40ms, mfu 0.68%\n",
            "iter 3580: loss 0.9352, time 521.67ms, mfu 0.68%\n",
            "iter 3590: loss 0.9228, time 522.60ms, mfu 0.68%\n",
            "iter 3600: loss 0.9247, time 520.45ms, mfu 0.69%\n",
            "iter 3610: loss 0.9158, time 518.70ms, mfu 0.69%\n",
            "iter 3620: loss 0.9081, time 523.49ms, mfu 0.69%\n",
            "iter 3630: loss 0.9248, time 522.93ms, mfu 0.69%\n",
            "iter 3640: loss 0.9096, time 523.45ms, mfu 0.70%\n",
            "iter 3650: loss 0.9084, time 520.94ms, mfu 0.70%\n",
            "iter 3660: loss 0.9387, time 520.72ms, mfu 0.70%\n",
            "iter 3670: loss 0.9388, time 522.28ms, mfu 0.70%\n",
            "iter 3680: loss 0.9101, time 522.28ms, mfu 0.70%\n",
            "iter 3690: loss 0.9351, time 521.58ms, mfu 0.70%\n",
            "iter 3700: loss 0.8688, time 521.17ms, mfu 0.70%\n",
            "iter 3710: loss 0.8783, time 522.86ms, mfu 0.71%\n",
            "iter 3720: loss 0.9033, time 521.44ms, mfu 0.71%\n",
            "iter 3730: loss 0.9000, time 522.05ms, mfu 0.71%\n",
            "iter 3740: loss 0.8988, time 523.03ms, mfu 0.71%\n",
            "step 3750: train loss 0.7395, val loss 1.6072\n",
            "iter 3750: loss 0.8959, time 73888.53ms, mfu 0.64%\n",
            "iter 3760: loss 0.9352, time 523.92ms, mfu 0.64%\n",
            "iter 3770: loss 0.9376, time 518.56ms, mfu 0.65%\n",
            "iter 3780: loss 0.9199, time 520.13ms, mfu 0.66%\n",
            "iter 3790: loss 0.8934, time 520.17ms, mfu 0.66%\n",
            "iter 3800: loss 0.9090, time 522.02ms, mfu 0.67%\n",
            "iter 3810: loss 0.9140, time 520.00ms, mfu 0.67%\n",
            "iter 3820: loss 0.8880, time 521.57ms, mfu 0.68%\n",
            "iter 3830: loss 0.8966, time 519.31ms, mfu 0.68%\n",
            "iter 3840: loss 0.8876, time 521.56ms, mfu 0.69%\n",
            "iter 3850: loss 0.8853, time 522.89ms, mfu 0.69%\n",
            "iter 3860: loss 0.8690, time 519.24ms, mfu 0.69%\n",
            "iter 3870: loss 0.8947, time 522.09ms, mfu 0.69%\n",
            "iter 3880: loss 0.8859, time 522.13ms, mfu 0.70%\n",
            "iter 3890: loss 0.8923, time 521.32ms, mfu 0.70%\n",
            "iter 3900: loss 0.8904, time 524.51ms, mfu 0.70%\n",
            "iter 3910: loss 0.8876, time 521.37ms, mfu 0.70%\n",
            "iter 3920: loss 0.8658, time 520.51ms, mfu 0.70%\n",
            "iter 3930: loss 0.8908, time 521.75ms, mfu 0.70%\n",
            "iter 3940: loss 0.8776, time 522.38ms, mfu 0.70%\n",
            "iter 3950: loss 0.8833, time 519.13ms, mfu 0.71%\n",
            "iter 3960: loss 0.9065, time 520.51ms, mfu 0.71%\n",
            "iter 3970: loss 0.8926, time 520.76ms, mfu 0.71%\n",
            "iter 3980: loss 0.8919, time 525.17ms, mfu 0.71%\n",
            "iter 3990: loss 0.8733, time 523.50ms, mfu 0.71%\n",
            "step 4000: train loss 0.7062, val loss 1.6336\n",
            "iter 4000: loss 0.8601, time 74055.11ms, mfu 0.64%\n",
            "iter 4010: loss 0.8771, time 522.65ms, mfu 0.65%\n",
            "iter 4020: loss 0.8902, time 522.36ms, mfu 0.65%\n",
            "iter 4030: loss 0.8811, time 519.53ms, mfu 0.66%\n",
            "iter 4040: loss 0.8803, time 522.26ms, mfu 0.66%\n",
            "iter 4050: loss 0.8682, time 522.26ms, mfu 0.67%\n",
            "iter 4060: loss 0.8670, time 522.87ms, mfu 0.67%\n",
            "iter 4070: loss 0.8515, time 522.38ms, mfu 0.68%\n",
            "iter 4080: loss 0.8877, time 521.62ms, mfu 0.68%\n",
            "iter 4090: loss 0.8421, time 522.90ms, mfu 0.68%\n",
            "iter 4100: loss 0.8956, time 523.16ms, mfu 0.69%\n",
            "iter 4110: loss 0.8752, time 522.33ms, mfu 0.69%\n",
            "iter 4120: loss 0.8815, time 524.16ms, mfu 0.69%\n",
            "iter 4130: loss 0.8578, time 522.78ms, mfu 0.69%\n",
            "iter 4140: loss 0.8778, time 522.33ms, mfu 0.70%\n",
            "iter 4150: loss 0.8662, time 519.64ms, mfu 0.70%\n",
            "iter 4160: loss 0.8597, time 524.46ms, mfu 0.70%\n",
            "iter 4170: loss 0.8723, time 523.32ms, mfu 0.70%\n",
            "iter 4180: loss 0.8713, time 524.11ms, mfu 0.70%\n",
            "iter 4190: loss 0.8739, time 521.78ms, mfu 0.70%\n",
            "iter 4200: loss 0.8587, time 520.91ms, mfu 0.70%\n",
            "iter 4210: loss 0.8669, time 522.30ms, mfu 0.70%\n",
            "iter 4220: loss 0.8572, time 521.32ms, mfu 0.71%\n",
            "iter 4230: loss 0.8771, time 522.07ms, mfu 0.71%\n",
            "iter 4240: loss 0.8635, time 520.44ms, mfu 0.71%\n",
            "step 4250: train loss 0.6776, val loss 1.6560\n",
            "iter 4250: loss 0.8686, time 73944.64ms, mfu 0.64%\n",
            "iter 4260: loss 0.8603, time 523.09ms, mfu 0.64%\n",
            "iter 4270: loss 0.8625, time 522.49ms, mfu 0.65%\n",
            "iter 4280: loss 0.8573, time 522.97ms, mfu 0.66%\n",
            "iter 4290: loss 0.8313, time 523.79ms, mfu 0.66%\n",
            "iter 4300: loss 0.8227, time 522.66ms, mfu 0.67%\n",
            "iter 4310: loss 0.8470, time 523.50ms, mfu 0.67%\n",
            "iter 4320: loss 0.8359, time 522.23ms, mfu 0.68%\n",
            "iter 4330: loss 0.8571, time 520.93ms, mfu 0.68%\n",
            "iter 4340: loss 0.8316, time 520.28ms, mfu 0.68%\n",
            "iter 4350: loss 0.8394, time 522.08ms, mfu 0.69%\n",
            "iter 4360: loss 0.8504, time 522.30ms, mfu 0.69%\n",
            "iter 4370: loss 0.8527, time 521.78ms, mfu 0.69%\n",
            "iter 4380: loss 0.8333, time 522.36ms, mfu 0.69%\n",
            "iter 4390: loss 0.8641, time 521.94ms, mfu 0.70%\n",
            "iter 4400: loss 0.8411, time 523.48ms, mfu 0.70%\n",
            "iter 4410: loss 0.8611, time 521.42ms, mfu 0.70%\n",
            "iter 4420: loss 0.8723, time 523.16ms, mfu 0.70%\n",
            "iter 4430: loss 0.8490, time 523.23ms, mfu 0.70%\n",
            "iter 4440: loss 0.8564, time 522.29ms, mfu 0.70%\n",
            "iter 4450: loss 0.8425, time 522.01ms, mfu 0.70%\n",
            "iter 4460: loss 0.8305, time 521.95ms, mfu 0.71%\n",
            "iter 4470: loss 0.8549, time 519.52ms, mfu 0.71%\n",
            "iter 4480: loss 0.8381, time 525.34ms, mfu 0.71%\n",
            "iter 4490: loss 0.8393, time 524.18ms, mfu 0.71%\n",
            "step 4500: train loss 0.6517, val loss 1.6735\n",
            "iter 4500: loss 0.8571, time 74083.03ms, mfu 0.64%\n",
            "iter 4510: loss 0.8488, time 522.55ms, mfu 0.64%\n",
            "iter 4520: loss 0.8330, time 524.18ms, mfu 0.65%\n",
            "iter 4530: loss 0.8458, time 524.05ms, mfu 0.66%\n",
            "iter 4540: loss 0.8463, time 523.57ms, mfu 0.66%\n",
            "iter 4550: loss 0.8774, time 523.19ms, mfu 0.67%\n",
            "iter 4560: loss 0.8378, time 523.61ms, mfu 0.67%\n",
            "iter 4570: loss 0.8397, time 523.45ms, mfu 0.68%\n",
            "iter 4580: loss 0.8508, time 521.38ms, mfu 0.68%\n",
            "iter 4590: loss 0.8508, time 522.98ms, mfu 0.68%\n",
            "iter 4600: loss 0.8350, time 520.78ms, mfu 0.69%\n",
            "iter 4610: loss 0.8740, time 521.08ms, mfu 0.69%\n",
            "iter 4620: loss 0.8351, time 523.26ms, mfu 0.69%\n",
            "iter 4630: loss 0.8250, time 522.97ms, mfu 0.69%\n",
            "iter 4640: loss 0.8364, time 522.59ms, mfu 0.70%\n",
            "iter 4650: loss 0.8563, time 521.87ms, mfu 0.70%\n",
            "iter 4660: loss 0.8477, time 523.69ms, mfu 0.70%\n",
            "iter 4670: loss 0.8341, time 525.33ms, mfu 0.70%\n",
            "iter 4680: loss 0.8501, time 523.39ms, mfu 0.70%\n",
            "iter 4690: loss 0.8521, time 522.84ms, mfu 0.70%\n",
            "iter 4700: loss 0.8243, time 524.00ms, mfu 0.70%\n",
            "iter 4710: loss 0.7913, time 525.06ms, mfu 0.70%\n",
            "iter 4720: loss 0.8323, time 523.41ms, mfu 0.70%\n",
            "iter 4730: loss 0.8126, time 523.81ms, mfu 0.71%\n",
            "iter 4740: loss 0.8283, time 522.68ms, mfu 0.71%\n",
            "step 4750: train loss 0.6337, val loss 1.6938\n",
            "iter 4750: loss 0.7967, time 74159.96ms, mfu 0.64%\n",
            "iter 4760: loss 0.8125, time 523.04ms, mfu 0.64%\n",
            "iter 4770: loss 0.8011, time 523.98ms, mfu 0.65%\n",
            "iter 4780: loss 0.8087, time 523.40ms, mfu 0.66%\n",
            "iter 4790: loss 0.8405, time 522.98ms, mfu 0.66%\n",
            "iter 4800: loss 0.8299, time 522.47ms, mfu 0.67%\n",
            "iter 4810: loss 0.8391, time 522.32ms, mfu 0.67%\n",
            "iter 4820: loss 0.8200, time 522.54ms, mfu 0.68%\n",
            "iter 4830: loss 0.8243, time 522.51ms, mfu 0.68%\n",
            "iter 4840: loss 0.8335, time 522.26ms, mfu 0.68%\n",
            "iter 4850: loss 0.8148, time 521.23ms, mfu 0.69%\n",
            "iter 4860: loss 0.8190, time 522.78ms, mfu 0.69%\n",
            "iter 4870: loss 0.8079, time 522.82ms, mfu 0.69%\n",
            "iter 4880: loss 0.8379, time 522.82ms, mfu 0.69%\n",
            "iter 4890: loss 0.8022, time 522.34ms, mfu 0.70%\n",
            "iter 4900: loss 0.8047, time 521.89ms, mfu 0.70%\n",
            "iter 4910: loss 0.8247, time 522.50ms, mfu 0.70%\n",
            "iter 4920: loss 0.8233, time 522.99ms, mfu 0.70%\n",
            "iter 4930: loss 0.8128, time 522.78ms, mfu 0.70%\n",
            "iter 4940: loss 0.8081, time 523.65ms, mfu 0.70%\n",
            "iter 4950: loss 0.8254, time 522.10ms, mfu 0.70%\n",
            "iter 4960: loss 0.8310, time 520.79ms, mfu 0.70%\n",
            "iter 4970: loss 0.7889, time 523.44ms, mfu 0.71%\n",
            "iter 4980: loss 0.8010, time 523.39ms, mfu 0.71%\n",
            "iter 4990: loss 0.8295, time 521.20ms, mfu 0.71%\n",
            "step 5000: train loss 0.6197, val loss 1.7098\n",
            "iter 5000: loss 0.8210, time 74056.75ms, mfu 0.64%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py --out_dir=out-shakespeare-char"
      ],
      "metadata": {
        "id": "d1nlBkoU3nJL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14dab1d7-9f15-4239-b11f-1548c314784b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-shakespeare-char\n",
            "number of parameters: 10.65M\n",
            "Loading meta from data/shakespeare_char/meta.pkl...\n",
            "\n",
            "\n",
            "AUTOLYCUS:\n",
            "O, will you be straight.\n",
            "\n",
            "MENENIUS:\n",
            "Stirrd your gates?\n",
            "\n",
            "CORIOLANUS:\n",
            "You met?\n",
            "\n",
            "MENENIUS:\n",
            "Nay, no more not to be much:\n",
            "If you find it out of much but soils, since I lay you,\n",
            "I'll speak with her person of else no more.\n",
            "\n",
            "AUFIDIUS:\n",
            "I would have show'd your name.\n",
            "Be patient o'er the world that\n",
            "Would so what evil so you would have no precise of mine;\n",
            "Which you have forgot a sign of sheet in all of person\n",
            "The ground.\n",
            "\n",
            "CORIOLANUS:\n",
            "So it was a brace of the viil court:\n",
            "A very ready change to th\n",
            "---------------\n",
            "\n",
            "Men pardon me, you shall have hanged to myself.\n",
            "\n",
            "Lord:\n",
            "We must danger than I have no world.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Why, how now!\n",
            "\n",
            "LUCIO:\n",
            "That which shall be been lost to die?\n",
            "\n",
            "LUCIO:\n",
            "For he that might have breathed the nurse o' the chance\n",
            "the side and entreaty of the bear\n",
            "Of great-brother than the lumb and her sons\n",
            "That this here senses, his head in posterity,\n",
            "To extermity the offices\n",
            "That no steel unmanlips that will perform,\n",
            "Which in me we know not here will be married to be out\n",
            "That I should wish t\n",
            "---------------\n",
            "\n",
            "Messenger:\n",
            "The soft is full of all.\n",
            "\n",
            "LARTIUS:\n",
            "No, but not take the complexion, and\n",
            "I need to thine eyes; but till this show\n",
            "Of much a steed, which with an absence.I Camillo\n",
            "For the poll, the angelong in the prayers of feast\n",
            "And witness what seems to bear the course's last,\n",
            "That with the princely life enough a deep of through,\n",
            "Of this answer laid in steep belowing on the duke:\n",
            "I do not serve you not buy forth you, sir.\n",
            "\n",
            "ANGELO:\n",
            "I am given no tongue.\n",
            "\n",
            "ISABELLA:\n",
            "Well, I would not be gone with him,\n",
            "\n",
            "---------------\n",
            "\n",
            "\n",
            "First Murderer:\n",
            "Why, how now, that I fear? thou art too comes?\n",
            "\n",
            "Second Murderer:\n",
            "But now I am born that prevent dead haste\n",
            "That thou liest remember to her hand,\n",
            "That I was that become the sins and so be\n",
            "That take the blow no end, that flowers that thou commons of the harm,\n",
            "Though make her act the world apoll o' the rest.\n",
            "\n",
            "First Murderer:\n",
            "The sound to prosper the airy of my love,\n",
            "That thy poor hath dismissent my and that Edward\n",
            "When thy parliaments for down's father be supposed.\n",
            "\n",
            "Shepherd:\n",
            "\n",
            "MARI\n",
            "---------------\n",
            "\n",
            "That lamb did spurn my kingdom from their royal remorse,\n",
            "That stole dead must have brought it not been so,\n",
            "Nor I will bear the love of the death.\n",
            "\n",
            "BUSHY:\n",
            "Nothing with mine own bloody time to the oracle\n",
            "That owe them honour that now have been more such\n",
            "To supply dangerous man unto them.\n",
            "\n",
            "Page:\n",
            "Will you hear, and will not put up in his soundly,\n",
            "To make him the mar make him grant. We would have said\n",
            "That fardel and more than our souls the air:\n",
            "If it were not, he were not for ours.\n",
            "\n",
            "ANGELO:\n",
            "We seem \n",
            "---------------\n",
            "\n",
            "\n",
            "MENENIUS:\n",
            "He is the contrary of your height grace,\n",
            "That we will be spoken for your best.\n",
            "\n",
            "First Citizen:\n",
            "Even so your worship.\n",
            "\n",
            "Third Servingman:\n",
            "Marry, I know not.\n",
            "\n",
            "First Senator:\n",
            "You will go with it. Want of your complexions,\n",
            "If you have ready your hands in Paris, it is strange\n",
            "Apollo your propertion.\n",
            "\n",
            "First Servingman:\n",
            "With all the princes: then it is a\n",
            "creature benefitted. So you have not discoverted\n",
            "In the poison of mine eyes, not of the world:\n",
            "And you tell me for him, I will follow him th\n",
            "---------------\n",
            "\n",
            "Shall be your highness, it well not?\n",
            "Here's a sink with you of my breath and their\n",
            "solemnities: and your guestify youth stones\n",
            "proud upon your countrymen. But, I'll have you know\n",
            "your allowing men as your bodies well as you were none;\n",
            "for your foes as heaven your rance as you shall\n",
            "be as fall intelligence.\n",
            "\n",
            "Shepherd:\n",
            "Eleven is the moral of your present\n",
            "or his sweet water young kind with the gates.\n",
            "\n",
            "HERMIONE:\n",
            "You may never call me again. The play-case of my conscience\n",
            "My true son Edward's and thr\n",
            "---------------\n",
            "\n",
            "I have heard me in name, he is it not;\n",
            "But that dangerous and slain itself,\n",
            "And term me that which you are now vile your power:\n",
            "If you, speak to prison it, and I have done:\n",
            "You have a mirth in some your friend\n",
            "More power than your place to come some other,\n",
            "You have strength ten your grace crown young to yours,\n",
            "Which now, you cape the hopeful words your answer.\n",
            "\n",
            "LEONTES:\n",
            "No, go to the part, and with edge your hands;\n",
            "And you are content in it o' the charm.\n",
            "\n",
            "LADY ANNE:\n",
            "I will appear the sun of my l\n",
            "---------------\n",
            "\n",
            "\n",
            "LEONTES:\n",
            "The realm and his parts and place and molesy,\n",
            "All pall these words to be fought in seens,\n",
            "And these strength all scretch, some notions\n",
            "Of my joy thing loss to my mind.\n",
            "\n",
            "LIONTES:\n",
            "Better\n",
            "Than himself, than my partise.\n",
            "\n",
            "BALTHASAR:\n",
            "If it be not, sir, and with my heart\n",
            "To sit and supply not hell and lent him along with you.\n",
            "\n",
            "MARIANA:\n",
            "Yes, my lord,\n",
            "The gracious prisoner of your grace.\n",
            "\n",
            "LEONTES:\n",
            "How now, my lord,\n",
            "I hold him in his present with his child.\n",
            "\n",
            "HERMIONE:\n",
            "Nor I, my lord.\n",
            "\n",
            "LEONTES:\n",
            "I\n",
            "---------------\n",
            "\n",
            "How chance sorrow?\n",
            "\n",
            "Second Servingman:\n",
            "How now! ay, how blest the best flight?\n",
            "\n",
            "Second Servingman:\n",
            "Go some second to bed, sir: by the prince.\n",
            "\n",
            "Second Servingman:\n",
            "I say, there's a royal victory.\n",
            "\n",
            "Third Servingman:\n",
            "Nay, no more stoner.\n",
            "\n",
            "Second Servingman:\n",
            "I have lost.\n",
            "\n",
            "First Murderer:\n",
            "Whey were as a sick of our power as easy\n",
            "follies as fled us hath done to us our mercyance: if\n",
            "we do not for this influity shall be sent, but to fair the\n",
            "good fleship of her brother tears and end; and one a wood\n",
            "man a\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modifications: Layers = 5, Heads ∈ {2, 3, 5, 7}"
      ],
      "metadata": {
        "id": "wOE4h0GBt7lI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --compile=False --log_interval=10 --n_layer=5 --n_head=2 --n_embd=128 --max_iters=3000 --lr_decay_iters=3000"
      ],
      "metadata": {
        "id": "rtTYHk0vQbKD",
        "outputId": "9b3a3a9e-5665-451d-ae4e-5427b986e6c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: compile = False\n",
            "Overriding: log_interval = 10\n",
            "Overriding: n_layer = 5\n",
            "Overriding: n_head = 2\n",
            "Overriding: n_embd = 128\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: lr_decay_iters = 3000\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 0.99M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 22, with 1,024,128 parameters\n",
            "num non-decayed parameter tensors: 11, with 1,408 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2210, val loss 4.2145\n",
            "iter 0: loss 4.2144, time 13115.38ms, mfu -100.00%\n",
            "iter 10: loss 3.7716, time 99.81ms, mfu 0.42%\n",
            "iter 20: loss 3.5924, time 100.16ms, mfu 0.42%\n",
            "iter 30: loss 3.2973, time 101.14ms, mfu 0.42%\n",
            "iter 40: loss 3.0729, time 100.52ms, mfu 0.42%\n",
            "iter 50: loss 2.8825, time 100.27ms, mfu 0.42%\n",
            "iter 60: loss 2.7502, time 101.11ms, mfu 0.42%\n",
            "iter 70: loss 2.6935, time 99.30ms, mfu 0.42%\n",
            "iter 80: loss 2.6333, time 100.62ms, mfu 0.42%\n",
            "iter 90: loss 2.5929, time 101.47ms, mfu 0.41%\n",
            "iter 100: loss 2.5701, time 101.44ms, mfu 0.41%\n",
            "iter 110: loss 2.5437, time 101.29ms, mfu 0.41%\n",
            "iter 120: loss 2.5316, time 100.87ms, mfu 0.41%\n",
            "iter 130: loss 2.5192, time 100.68ms, mfu 0.41%\n",
            "iter 140: loss 2.5014, time 101.84ms, mfu 0.41%\n",
            "iter 150: loss 2.4966, time 101.42ms, mfu 0.41%\n",
            "iter 160: loss 2.4869, time 101.44ms, mfu 0.41%\n",
            "iter 170: loss 2.5064, time 101.85ms, mfu 0.41%\n",
            "iter 180: loss 2.4809, time 102.15ms, mfu 0.41%\n",
            "iter 190: loss 2.4863, time 101.67ms, mfu 0.41%\n",
            "iter 200: loss 2.4622, time 101.49ms, mfu 0.41%\n",
            "iter 210: loss 2.4474, time 101.16ms, mfu 0.41%\n",
            "iter 220: loss 2.4426, time 100.86ms, mfu 0.41%\n",
            "iter 230: loss 2.4673, time 100.73ms, mfu 0.41%\n",
            "iter 240: loss 2.4310, time 100.30ms, mfu 0.41%\n",
            "step 250: train loss 2.4145, val loss 2.4289\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.4251, time 12978.05ms, mfu 0.37%\n",
            "iter 260: loss 2.4658, time 103.85ms, mfu 0.37%\n",
            "iter 270: loss 2.4215, time 104.94ms, mfu 0.38%\n",
            "iter 280: loss 2.4234, time 102.60ms, mfu 0.38%\n",
            "iter 290: loss 2.4268, time 102.80ms, mfu 0.38%\n",
            "iter 300: loss 2.4187, time 104.09ms, mfu 0.38%\n",
            "iter 310: loss 2.4061, time 103.24ms, mfu 0.39%\n",
            "iter 320: loss 2.3925, time 101.94ms, mfu 0.39%\n",
            "iter 330: loss 2.3822, time 105.36ms, mfu 0.39%\n",
            "iter 340: loss 2.3545, time 104.60ms, mfu 0.39%\n",
            "iter 350: loss 2.3614, time 105.55ms, mfu 0.39%\n",
            "iter 360: loss 2.3498, time 102.35ms, mfu 0.39%\n",
            "iter 370: loss 2.3674, time 103.78ms, mfu 0.39%\n",
            "iter 380: loss 2.3355, time 105.72ms, mfu 0.39%\n",
            "iter 390: loss 2.3344, time 105.89ms, mfu 0.39%\n",
            "iter 400: loss 2.3417, time 104.40ms, mfu 0.39%\n",
            "iter 410: loss 2.3354, time 104.18ms, mfu 0.39%\n",
            "iter 420: loss 2.2957, time 105.87ms, mfu 0.39%\n",
            "iter 430: loss 2.3127, time 102.97ms, mfu 0.39%\n",
            "iter 440: loss 2.3115, time 103.89ms, mfu 0.40%\n",
            "iter 450: loss 2.2759, time 103.95ms, mfu 0.40%\n",
            "iter 460: loss 2.2764, time 105.96ms, mfu 0.40%\n",
            "iter 470: loss 2.2683, time 107.47ms, mfu 0.39%\n",
            "iter 480: loss 2.2239, time 105.13ms, mfu 0.39%\n",
            "iter 490: loss 2.2361, time 105.81ms, mfu 0.39%\n",
            "step 500: train loss 2.1101, val loss 2.1635\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 2.2271, time 13447.68ms, mfu 0.36%\n",
            "iter 510: loss 2.2007, time 107.58ms, mfu 0.36%\n",
            "iter 520: loss 2.2051, time 106.74ms, mfu 0.36%\n",
            "iter 530: loss 2.1655, time 104.66ms, mfu 0.37%\n",
            "iter 540: loss 2.1806, time 108.80ms, mfu 0.37%\n",
            "iter 550: loss 2.1849, time 107.75ms, mfu 0.37%\n",
            "iter 560: loss 2.1682, time 106.15ms, mfu 0.37%\n",
            "iter 570: loss 2.1729, time 106.99ms, mfu 0.37%\n",
            "iter 580: loss 2.1520, time 108.78ms, mfu 0.37%\n",
            "iter 590: loss 2.1282, time 107.05ms, mfu 0.38%\n",
            "iter 600: loss 2.1604, time 106.95ms, mfu 0.38%\n",
            "iter 610: loss 2.1121, time 108.86ms, mfu 0.38%\n",
            "iter 620: loss 2.1057, time 109.48ms, mfu 0.38%\n",
            "iter 630: loss 2.0976, time 108.93ms, mfu 0.38%\n",
            "iter 640: loss 2.0759, time 105.97ms, mfu 0.38%\n",
            "iter 650: loss 2.1048, time 108.07ms, mfu 0.38%\n",
            "iter 660: loss 2.0452, time 106.75ms, mfu 0.38%\n",
            "iter 670: loss 2.0459, time 108.43ms, mfu 0.38%\n",
            "iter 680: loss 2.0384, time 109.62ms, mfu 0.38%\n",
            "iter 690: loss 2.0736, time 107.89ms, mfu 0.38%\n",
            "iter 700: loss 2.0200, time 108.05ms, mfu 0.38%\n",
            "iter 710: loss 2.0268, time 108.43ms, mfu 0.38%\n",
            "iter 720: loss 1.9853, time 109.28ms, mfu 0.38%\n",
            "iter 730: loss 1.9966, time 108.67ms, mfu 0.38%\n",
            "iter 740: loss 1.9907, time 108.08ms, mfu 0.38%\n",
            "step 750: train loss 1.8513, val loss 1.9679\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 2.0223, time 13499.89ms, mfu 0.34%\n",
            "iter 760: loss 1.9869, time 109.58ms, mfu 0.35%\n",
            "iter 770: loss 1.9877, time 104.89ms, mfu 0.35%\n",
            "iter 780: loss 1.9666, time 104.52ms, mfu 0.36%\n",
            "iter 790: loss 1.9413, time 105.52ms, mfu 0.36%\n",
            "iter 800: loss 1.9582, time 107.92ms, mfu 0.36%\n",
            "iter 810: loss 1.9365, time 105.42ms, mfu 0.37%\n",
            "iter 820: loss 1.9445, time 105.98ms, mfu 0.37%\n",
            "iter 830: loss 1.9420, time 103.00ms, mfu 0.37%\n",
            "iter 840: loss 1.9480, time 104.45ms, mfu 0.38%\n",
            "iter 850: loss 1.9526, time 107.39ms, mfu 0.38%\n",
            "iter 860: loss 1.9228, time 105.74ms, mfu 0.38%\n",
            "iter 870: loss 1.9243, time 104.98ms, mfu 0.38%\n",
            "iter 880: loss 1.9058, time 104.22ms, mfu 0.38%\n",
            "iter 890: loss 1.8993, time 104.69ms, mfu 0.38%\n",
            "iter 900: loss 1.8989, time 105.85ms, mfu 0.38%\n",
            "iter 910: loss 1.8853, time 105.01ms, mfu 0.39%\n",
            "iter 920: loss 1.9303, time 104.85ms, mfu 0.39%\n",
            "iter 930: loss 1.8929, time 104.51ms, mfu 0.39%\n",
            "iter 940: loss 1.8550, time 103.94ms, mfu 0.39%\n",
            "iter 950: loss 1.8970, time 106.73ms, mfu 0.39%\n",
            "iter 960: loss 1.8681, time 104.29ms, mfu 0.39%\n",
            "iter 970: loss 1.8498, time 107.25ms, mfu 0.39%\n",
            "iter 980: loss 1.8544, time 105.59ms, mfu 0.39%\n",
            "iter 990: loss 1.8259, time 105.05ms, mfu 0.39%\n",
            "step 1000: train loss 1.7136, val loss 1.8873\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.8656, time 13427.04ms, mfu 0.35%\n",
            "iter 1010: loss 1.8380, time 107.26ms, mfu 0.36%\n",
            "iter 1020: loss 1.8428, time 107.81ms, mfu 0.36%\n",
            "iter 1030: loss 1.8597, time 105.73ms, mfu 0.36%\n",
            "iter 1040: loss 1.8925, time 106.60ms, mfu 0.36%\n",
            "iter 1050: loss 1.8215, time 106.71ms, mfu 0.37%\n",
            "iter 1060: loss 1.8297, time 106.33ms, mfu 0.37%\n",
            "iter 1070: loss 1.8369, time 108.68ms, mfu 0.37%\n",
            "iter 1080: loss 1.8519, time 105.87ms, mfu 0.37%\n",
            "iter 1090: loss 1.8046, time 106.31ms, mfu 0.38%\n",
            "iter 1100: loss 1.8327, time 106.14ms, mfu 0.38%\n",
            "iter 1110: loss 1.8064, time 108.43ms, mfu 0.38%\n",
            "iter 1120: loss 1.8138, time 105.50ms, mfu 0.38%\n",
            "iter 1130: loss 1.8582, time 108.66ms, mfu 0.38%\n",
            "iter 1140: loss 1.8237, time 106.46ms, mfu 0.38%\n",
            "iter 1150: loss 1.8112, time 107.11ms, mfu 0.38%\n",
            "iter 1160: loss 1.7933, time 106.66ms, mfu 0.38%\n",
            "iter 1170: loss 1.7478, time 105.59ms, mfu 0.38%\n",
            "iter 1180: loss 1.7894, time 109.33ms, mfu 0.38%\n",
            "iter 1190: loss 1.7897, time 107.37ms, mfu 0.38%\n",
            "iter 1200: loss 1.7870, time 105.38ms, mfu 0.38%\n",
            "iter 1210: loss 1.7614, time 106.63ms, mfu 0.39%\n",
            "iter 1220: loss 1.7866, time 107.61ms, mfu 0.39%\n",
            "iter 1230: loss 1.7673, time 105.26ms, mfu 0.39%\n",
            "iter 1240: loss 1.7854, time 110.44ms, mfu 0.39%\n",
            "step 1250: train loss 1.6285, val loss 1.8108\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.7721, time 13573.17ms, mfu 0.35%\n",
            "iter 1260: loss 1.7715, time 106.80ms, mfu 0.35%\n",
            "iter 1270: loss 1.8059, time 105.76ms, mfu 0.36%\n",
            "iter 1280: loss 1.7644, time 108.42ms, mfu 0.36%\n",
            "iter 1290: loss 1.7502, time 105.39ms, mfu 0.36%\n",
            "iter 1300: loss 1.7818, time 106.55ms, mfu 0.36%\n",
            "iter 1310: loss 1.7799, time 105.92ms, mfu 0.37%\n",
            "iter 1320: loss 1.7664, time 104.61ms, mfu 0.37%\n",
            "iter 1330: loss 1.7386, time 105.57ms, mfu 0.37%\n",
            "iter 1340: loss 1.7629, time 106.93ms, mfu 0.37%\n",
            "iter 1350: loss 1.7189, time 106.80ms, mfu 0.38%\n",
            "iter 1360: loss 1.7735, time 106.73ms, mfu 0.38%\n",
            "iter 1370: loss 1.7162, time 106.30ms, mfu 0.38%\n",
            "iter 1380: loss 1.7136, time 106.33ms, mfu 0.38%\n",
            "iter 1390: loss 1.7512, time 106.65ms, mfu 0.38%\n",
            "iter 1400: loss 1.7452, time 105.99ms, mfu 0.38%\n",
            "iter 1410: loss 1.6878, time 106.57ms, mfu 0.38%\n",
            "iter 1420: loss 1.7322, time 107.32ms, mfu 0.38%\n",
            "iter 1430: loss 1.7436, time 107.21ms, mfu 0.38%\n",
            "iter 1440: loss 1.6894, time 105.76ms, mfu 0.38%\n",
            "iter 1450: loss 1.6912, time 108.14ms, mfu 0.38%\n",
            "iter 1460: loss 1.6808, time 106.46ms, mfu 0.39%\n",
            "iter 1470: loss 1.7288, time 109.46ms, mfu 0.38%\n",
            "iter 1480: loss 1.7071, time 105.36ms, mfu 0.39%\n",
            "iter 1490: loss 1.7054, time 105.88ms, mfu 0.39%\n",
            "step 1500: train loss 1.5690, val loss 1.7569\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.6889, time 13448.81ms, mfu 0.35%\n",
            "iter 1510: loss 1.7016, time 103.50ms, mfu 0.35%\n",
            "iter 1520: loss 1.7360, time 103.51ms, mfu 0.36%\n",
            "iter 1530: loss 1.7228, time 108.37ms, mfu 0.36%\n",
            "iter 1540: loss 1.7098, time 105.37ms, mfu 0.36%\n",
            "iter 1550: loss 1.7206, time 107.35ms, mfu 0.37%\n",
            "iter 1560: loss 1.6826, time 106.79ms, mfu 0.37%\n",
            "iter 1570: loss 1.6844, time 107.21ms, mfu 0.37%\n",
            "iter 1580: loss 1.6669, time 108.31ms, mfu 0.37%\n",
            "iter 1590: loss 1.7077, time 107.38ms, mfu 0.37%\n",
            "iter 1600: loss 1.6836, time 105.24ms, mfu 0.38%\n",
            "iter 1610: loss 1.7123, time 104.45ms, mfu 0.38%\n",
            "iter 1620: loss 1.6967, time 106.37ms, mfu 0.38%\n",
            "iter 1630: loss 1.7046, time 104.73ms, mfu 0.38%\n",
            "iter 1640: loss 1.6842, time 104.63ms, mfu 0.38%\n",
            "iter 1650: loss 1.6691, time 108.81ms, mfu 0.38%\n",
            "iter 1660: loss 1.6530, time 105.15ms, mfu 0.38%\n",
            "iter 1670: loss 1.6874, time 107.11ms, mfu 0.38%\n",
            "iter 1680: loss 1.6562, time 104.04ms, mfu 0.39%\n",
            "iter 1690: loss 1.6880, time 108.68ms, mfu 0.39%\n",
            "iter 1700: loss 1.6705, time 103.67ms, mfu 0.39%\n",
            "iter 1710: loss 1.6881, time 106.55ms, mfu 0.39%\n",
            "iter 1720: loss 1.6626, time 105.36ms, mfu 0.39%\n",
            "iter 1730: loss 1.6752, time 106.95ms, mfu 0.39%\n",
            "iter 1740: loss 1.6706, time 106.80ms, mfu 0.39%\n",
            "step 1750: train loss 1.5262, val loss 1.7157\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.6416, time 13481.94ms, mfu 0.35%\n",
            "iter 1760: loss 1.6772, time 105.62ms, mfu 0.35%\n",
            "iter 1770: loss 1.6579, time 104.76ms, mfu 0.36%\n",
            "iter 1780: loss 1.6346, time 106.55ms, mfu 0.36%\n",
            "iter 1790: loss 1.6722, time 106.94ms, mfu 0.36%\n",
            "iter 1800: loss 1.6439, time 106.68ms, mfu 0.37%\n",
            "iter 1810: loss 1.6479, time 107.22ms, mfu 0.37%\n",
            "iter 1820: loss 1.6197, time 104.79ms, mfu 0.37%\n",
            "iter 1830: loss 1.6915, time 106.46ms, mfu 0.37%\n",
            "iter 1840: loss 1.6116, time 109.31ms, mfu 0.37%\n",
            "iter 1850: loss 1.6300, time 106.28ms, mfu 0.38%\n",
            "iter 1860: loss 1.6694, time 107.96ms, mfu 0.38%\n",
            "iter 1870: loss 1.6284, time 107.15ms, mfu 0.38%\n",
            "iter 1880: loss 1.6675, time 106.82ms, mfu 0.38%\n",
            "iter 1890: loss 1.6707, time 106.33ms, mfu 0.38%\n",
            "iter 1900: loss 1.6630, time 106.65ms, mfu 0.38%\n",
            "iter 1910: loss 1.6346, time 107.97ms, mfu 0.38%\n",
            "iter 1920: loss 1.6505, time 103.96ms, mfu 0.38%\n",
            "iter 1930: loss 1.6479, time 105.34ms, mfu 0.38%\n",
            "iter 1940: loss 1.6048, time 107.48ms, mfu 0.39%\n",
            "iter 1950: loss 1.6438, time 105.76ms, mfu 0.39%\n",
            "iter 1960: loss 1.6400, time 105.96ms, mfu 0.39%\n",
            "iter 1970: loss 1.6405, time 107.05ms, mfu 0.39%\n",
            "iter 1980: loss 1.6206, time 108.67ms, mfu 0.39%\n",
            "iter 1990: loss 1.6221, time 105.59ms, mfu 0.39%\n",
            "step 2000: train loss 1.4972, val loss 1.6826\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.6168, time 13506.15ms, mfu 0.35%\n",
            "iter 2010: loss 1.6383, time 108.43ms, mfu 0.35%\n",
            "iter 2020: loss 1.6018, time 106.03ms, mfu 0.36%\n",
            "iter 2030: loss 1.6451, time 104.81ms, mfu 0.36%\n",
            "iter 2040: loss 1.6492, time 105.53ms, mfu 0.36%\n",
            "iter 2050: loss 1.6098, time 106.82ms, mfu 0.37%\n",
            "iter 2060: loss 1.6351, time 106.32ms, mfu 0.37%\n",
            "iter 2070: loss 1.5993, time 107.78ms, mfu 0.37%\n",
            "iter 2080: loss 1.6081, time 105.41ms, mfu 0.37%\n",
            "iter 2090: loss 1.6143, time 106.43ms, mfu 0.37%\n",
            "iter 2100: loss 1.6494, time 105.59ms, mfu 0.38%\n",
            "iter 2110: loss 1.6170, time 106.96ms, mfu 0.38%\n",
            "iter 2120: loss 1.6043, time 106.46ms, mfu 0.38%\n",
            "iter 2130: loss 1.5983, time 106.17ms, mfu 0.38%\n",
            "iter 2140: loss 1.6003, time 106.53ms, mfu 0.38%\n",
            "iter 2150: loss 1.6180, time 105.23ms, mfu 0.38%\n",
            "iter 2160: loss 1.6310, time 105.55ms, mfu 0.38%\n",
            "iter 2170: loss 1.6242, time 104.53ms, mfu 0.39%\n",
            "iter 2180: loss 1.5870, time 105.88ms, mfu 0.39%\n",
            "iter 2190: loss 1.6331, time 105.90ms, mfu 0.39%\n",
            "iter 2200: loss 1.6027, time 109.42ms, mfu 0.39%\n",
            "iter 2210: loss 1.6048, time 106.95ms, mfu 0.39%\n",
            "iter 2220: loss 1.6401, time 107.03ms, mfu 0.39%\n",
            "iter 2230: loss 1.6084, time 104.40ms, mfu 0.39%\n",
            "iter 2240: loss 1.6253, time 105.07ms, mfu 0.39%\n",
            "step 2250: train loss 1.4774, val loss 1.6690\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.5983, time 13481.28ms, mfu 0.35%\n",
            "iter 2260: loss 1.6029, time 106.68ms, mfu 0.35%\n",
            "iter 2270: loss 1.5841, time 106.57ms, mfu 0.36%\n",
            "iter 2280: loss 1.6178, time 108.67ms, mfu 0.36%\n",
            "iter 2290: loss 1.5918, time 105.26ms, mfu 0.36%\n",
            "iter 2300: loss 1.6283, time 107.09ms, mfu 0.37%\n",
            "iter 2310: loss 1.6408, time 105.34ms, mfu 0.37%\n",
            "iter 2320: loss 1.6093, time 105.15ms, mfu 0.37%\n",
            "iter 2330: loss 1.6062, time 106.10ms, mfu 0.37%\n",
            "iter 2340: loss 1.5929, time 105.90ms, mfu 0.38%\n",
            "iter 2350: loss 1.5460, time 105.97ms, mfu 0.38%\n",
            "iter 2360: loss 1.5992, time 108.24ms, mfu 0.38%\n",
            "iter 2370: loss 1.6607, time 105.73ms, mfu 0.38%\n",
            "iter 2380: loss 1.5897, time 104.90ms, mfu 0.38%\n",
            "iter 2390: loss 1.6053, time 108.44ms, mfu 0.38%\n",
            "iter 2400: loss 1.6017, time 104.24ms, mfu 0.38%\n",
            "iter 2410: loss 1.5873, time 105.71ms, mfu 0.38%\n",
            "iter 2420: loss 1.5677, time 106.95ms, mfu 0.38%\n",
            "iter 2430: loss 1.6596, time 106.71ms, mfu 0.39%\n",
            "iter 2440: loss 1.5829, time 105.66ms, mfu 0.39%\n",
            "iter 2450: loss 1.5860, time 107.94ms, mfu 0.39%\n",
            "iter 2460: loss 1.5995, time 106.87ms, mfu 0.39%\n",
            "iter 2470: loss 1.5932, time 108.36ms, mfu 0.39%\n",
            "iter 2480: loss 1.6228, time 104.70ms, mfu 0.39%\n",
            "iter 2490: loss 1.6060, time 107.12ms, mfu 0.39%\n",
            "step 2500: train loss 1.4598, val loss 1.6500\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.6178, time 13469.03ms, mfu 0.35%\n",
            "iter 2510: loss 1.5588, time 107.22ms, mfu 0.35%\n",
            "iter 2520: loss 1.6088, time 106.87ms, mfu 0.36%\n",
            "iter 2530: loss 1.5485, time 106.76ms, mfu 0.36%\n",
            "iter 2540: loss 1.5706, time 105.89ms, mfu 0.36%\n",
            "iter 2550: loss 1.5922, time 108.91ms, mfu 0.37%\n",
            "iter 2560: loss 1.5732, time 104.81ms, mfu 0.37%\n",
            "iter 2570: loss 1.5888, time 108.73ms, mfu 0.37%\n",
            "iter 2580: loss 1.5541, time 105.16ms, mfu 0.37%\n",
            "iter 2590: loss 1.5983, time 107.26ms, mfu 0.37%\n",
            "iter 2600: loss 1.6068, time 104.72ms, mfu 0.38%\n",
            "iter 2610: loss 1.5980, time 106.59ms, mfu 0.38%\n",
            "iter 2620: loss 1.5859, time 107.18ms, mfu 0.38%\n",
            "iter 2630: loss 1.5664, time 106.91ms, mfu 0.38%\n",
            "iter 2640: loss 1.5990, time 104.58ms, mfu 0.38%\n",
            "iter 2650: loss 1.5629, time 104.35ms, mfu 0.38%\n",
            "iter 2660: loss 1.6003, time 108.59ms, mfu 0.38%\n",
            "iter 2670: loss 1.5968, time 105.24ms, mfu 0.38%\n",
            "iter 2680: loss 1.5747, time 106.03ms, mfu 0.39%\n",
            "iter 2690: loss 1.5821, time 105.85ms, mfu 0.39%\n",
            "iter 2700: loss 1.6016, time 106.00ms, mfu 0.39%\n",
            "iter 2710: loss 1.5813, time 106.82ms, mfu 0.39%\n",
            "iter 2720: loss 1.5916, time 105.62ms, mfu 0.39%\n",
            "iter 2730: loss 1.6097, time 104.54ms, mfu 0.39%\n",
            "iter 2740: loss 1.5974, time 105.92ms, mfu 0.39%\n",
            "step 2750: train loss 1.4496, val loss 1.6450\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.5552, time 13479.07ms, mfu 0.35%\n",
            "iter 2760: loss 1.6336, time 107.02ms, mfu 0.35%\n",
            "iter 2770: loss 1.5808, time 107.32ms, mfu 0.36%\n",
            "iter 2780: loss 1.5508, time 107.16ms, mfu 0.36%\n",
            "iter 2790: loss 1.5806, time 105.90ms, mfu 0.36%\n",
            "iter 2800: loss 1.5600, time 106.92ms, mfu 0.37%\n",
            "iter 2810: loss 1.5718, time 106.19ms, mfu 0.37%\n",
            "iter 2820: loss 1.5861, time 106.27ms, mfu 0.37%\n",
            "iter 2830: loss 1.5655, time 105.20ms, mfu 0.37%\n",
            "iter 2840: loss 1.5833, time 106.57ms, mfu 0.38%\n",
            "iter 2850: loss 1.5667, time 106.27ms, mfu 0.38%\n",
            "iter 2860: loss 1.5893, time 108.23ms, mfu 0.38%\n",
            "iter 2870: loss 1.5589, time 108.01ms, mfu 0.38%\n",
            "iter 2880: loss 1.5731, time 106.20ms, mfu 0.38%\n",
            "iter 2890: loss 1.5390, time 107.28ms, mfu 0.38%\n",
            "iter 2900: loss 1.5324, time 107.54ms, mfu 0.38%\n",
            "iter 2910: loss 1.5815, time 107.47ms, mfu 0.38%\n",
            "iter 2920: loss 1.5736, time 105.62ms, mfu 0.38%\n",
            "iter 2930: loss 1.5215, time 108.85ms, mfu 0.38%\n",
            "iter 2940: loss 1.5504, time 104.77ms, mfu 0.38%\n",
            "iter 2950: loss 1.5704, time 106.40ms, mfu 0.39%\n",
            "iter 2960: loss 1.5363, time 106.91ms, mfu 0.39%\n",
            "iter 2970: loss 1.5724, time 106.45ms, mfu 0.39%\n",
            "iter 2980: loss 1.5761, time 106.22ms, mfu 0.39%\n",
            "iter 2990: loss 1.5869, time 106.61ms, mfu 0.39%\n",
            "step 3000: train loss 1.4390, val loss 1.6355\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.5507, time 13492.26ms, mfu 0.35%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --compile=False --log_interval=10 --n_layer=5 --n_head=3 --n_embd=129 --max_iters=3000 --lr_decay_iters=3000"
      ],
      "metadata": {
        "id": "QSGZ37t6Tv_t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa282e61-02d9-4898-aac8-01fe22db6218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: compile = False\n",
            "Overriding: log_interval = 10\n",
            "Overriding: n_layer = 5\n",
            "Overriding: n_head = 3\n",
            "Overriding: n_embd = 129\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: lr_decay_iters = 3000\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.01M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 22, with 1,039,869 parameters\n",
            "num non-decayed parameter tensors: 11, with 1,419 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2043, val loss 4.2027\n",
            "iter 0: loss 4.2022, time 19458.44ms, mfu -100.00%\n",
            "iter 10: loss 3.7600, time 128.86ms, mfu 0.33%\n",
            "iter 20: loss 3.5330, time 131.11ms, mfu 0.33%\n",
            "iter 30: loss 3.2783, time 129.11ms, mfu 0.33%\n",
            "iter 40: loss 3.0530, time 130.66ms, mfu 0.33%\n",
            "iter 50: loss 2.8568, time 130.87ms, mfu 0.33%\n",
            "iter 60: loss 2.7420, time 131.84ms, mfu 0.33%\n",
            "iter 70: loss 2.6823, time 127.77ms, mfu 0.33%\n",
            "iter 80: loss 2.6241, time 131.31ms, mfu 0.33%\n",
            "iter 90: loss 2.5929, time 131.70ms, mfu 0.32%\n",
            "iter 100: loss 2.5713, time 132.89ms, mfu 0.32%\n",
            "iter 110: loss 2.5383, time 131.96ms, mfu 0.32%\n",
            "iter 120: loss 2.5415, time 134.08ms, mfu 0.32%\n",
            "iter 130: loss 2.5260, time 132.29ms, mfu 0.32%\n",
            "iter 140: loss 2.5071, time 134.35ms, mfu 0.32%\n",
            "iter 150: loss 2.5045, time 133.66ms, mfu 0.32%\n",
            "iter 160: loss 2.4863, time 138.53ms, mfu 0.32%\n",
            "iter 170: loss 2.4931, time 135.18ms, mfu 0.32%\n",
            "iter 180: loss 2.4865, time 133.62ms, mfu 0.32%\n",
            "iter 190: loss 2.4681, time 134.58ms, mfu 0.32%\n",
            "iter 200: loss 2.4625, time 134.76ms, mfu 0.32%\n",
            "iter 210: loss 2.4470, time 134.59ms, mfu 0.32%\n",
            "iter 220: loss 2.4446, time 133.98ms, mfu 0.32%\n",
            "iter 230: loss 2.4507, time 135.43ms, mfu 0.32%\n",
            "iter 240: loss 2.4435, time 135.60ms, mfu 0.32%\n",
            "step 250: train loss 2.3889, val loss 2.4005\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.4164, time 20355.22ms, mfu 0.28%\n",
            "iter 260: loss 2.4303, time 135.91ms, mfu 0.29%\n",
            "iter 270: loss 2.4280, time 136.65ms, mfu 0.29%\n",
            "iter 280: loss 2.4211, time 135.06ms, mfu 0.29%\n",
            "iter 290: loss 2.4085, time 135.82ms, mfu 0.29%\n",
            "iter 300: loss 2.3952, time 136.31ms, mfu 0.29%\n",
            "iter 310: loss 2.3728, time 133.96ms, mfu 0.30%\n",
            "iter 320: loss 2.3921, time 134.94ms, mfu 0.30%\n",
            "iter 330: loss 2.3590, time 135.95ms, mfu 0.30%\n",
            "iter 340: loss 2.3885, time 134.85ms, mfu 0.30%\n",
            "iter 350: loss 2.3421, time 134.39ms, mfu 0.30%\n",
            "iter 360: loss 2.3405, time 134.54ms, mfu 0.30%\n",
            "iter 370: loss 2.3369, time 134.78ms, mfu 0.30%\n",
            "iter 380: loss 2.3343, time 133.37ms, mfu 0.31%\n",
            "iter 390: loss 2.3149, time 135.70ms, mfu 0.31%\n",
            "iter 400: loss 2.2923, time 134.05ms, mfu 0.31%\n",
            "iter 410: loss 2.3046, time 136.00ms, mfu 0.31%\n",
            "iter 420: loss 2.3036, time 135.09ms, mfu 0.31%\n",
            "iter 430: loss 2.2827, time 134.19ms, mfu 0.31%\n",
            "iter 440: loss 2.2913, time 133.94ms, mfu 0.31%\n",
            "iter 450: loss 2.2663, time 133.49ms, mfu 0.31%\n",
            "iter 460: loss 2.2906, time 134.85ms, mfu 0.31%\n",
            "iter 470: loss 2.2420, time 133.98ms, mfu 0.31%\n",
            "iter 480: loss 2.2539, time 134.04ms, mfu 0.31%\n",
            "iter 490: loss 2.2354, time 134.65ms, mfu 0.31%\n",
            "step 500: train loss 2.1040, val loss 2.1541\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 2.2311, time 19841.57ms, mfu 0.28%\n",
            "iter 510: loss 2.2087, time 134.58ms, mfu 0.28%\n",
            "iter 520: loss 2.1847, time 133.41ms, mfu 0.29%\n",
            "iter 530: loss 2.1481, time 133.43ms, mfu 0.29%\n",
            "iter 540: loss 2.1668, time 134.49ms, mfu 0.29%\n",
            "iter 550: loss 2.1457, time 134.85ms, mfu 0.29%\n",
            "iter 560: loss 2.1866, time 134.66ms, mfu 0.30%\n",
            "iter 570: loss 2.1141, time 134.74ms, mfu 0.30%\n",
            "iter 580: loss 2.1224, time 136.52ms, mfu 0.30%\n",
            "iter 590: loss 2.1320, time 132.56ms, mfu 0.30%\n",
            "iter 600: loss 2.1037, time 135.12ms, mfu 0.30%\n",
            "iter 610: loss 2.0836, time 135.17ms, mfu 0.30%\n",
            "iter 620: loss 2.0760, time 135.01ms, mfu 0.30%\n",
            "iter 630: loss 2.0550, time 134.12ms, mfu 0.30%\n",
            "iter 640: loss 2.0375, time 135.07ms, mfu 0.31%\n",
            "iter 650: loss 2.0616, time 134.83ms, mfu 0.31%\n",
            "iter 660: loss 2.0686, time 135.29ms, mfu 0.31%\n",
            "iter 670: loss 2.0325, time 134.89ms, mfu 0.31%\n",
            "iter 680: loss 2.0255, time 136.62ms, mfu 0.31%\n",
            "iter 690: loss 2.0043, time 135.78ms, mfu 0.31%\n",
            "iter 700: loss 2.0218, time 134.73ms, mfu 0.31%\n",
            "iter 710: loss 2.0181, time 135.19ms, mfu 0.31%\n",
            "iter 720: loss 2.0014, time 136.23ms, mfu 0.31%\n",
            "iter 730: loss 1.9998, time 134.88ms, mfu 0.31%\n",
            "iter 740: loss 1.9878, time 135.23ms, mfu 0.31%\n",
            "step 750: train loss 1.8386, val loss 1.9655\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.9745, time 20098.74ms, mfu 0.28%\n",
            "iter 760: loss 1.9574, time 134.61ms, mfu 0.28%\n",
            "iter 770: loss 1.9771, time 135.18ms, mfu 0.29%\n",
            "iter 780: loss 1.9625, time 133.47ms, mfu 0.29%\n",
            "iter 790: loss 1.9312, time 134.45ms, mfu 0.29%\n",
            "iter 800: loss 1.9378, time 135.56ms, mfu 0.29%\n",
            "iter 810: loss 1.9406, time 133.94ms, mfu 0.30%\n",
            "iter 820: loss 1.9045, time 134.75ms, mfu 0.30%\n",
            "iter 830: loss 1.9413, time 135.04ms, mfu 0.30%\n",
            "iter 840: loss 1.9231, time 132.76ms, mfu 0.30%\n",
            "iter 850: loss 1.9116, time 135.22ms, mfu 0.30%\n",
            "iter 860: loss 1.8955, time 136.24ms, mfu 0.30%\n",
            "iter 870: loss 1.9011, time 133.88ms, mfu 0.30%\n",
            "iter 880: loss 1.8813, time 134.57ms, mfu 0.30%\n",
            "iter 890: loss 1.8828, time 134.06ms, mfu 0.31%\n",
            "iter 900: loss 1.9068, time 133.35ms, mfu 0.31%\n",
            "iter 910: loss 1.8805, time 134.35ms, mfu 0.31%\n",
            "iter 920: loss 1.8798, time 134.98ms, mfu 0.31%\n",
            "iter 930: loss 1.8785, time 134.10ms, mfu 0.31%\n",
            "iter 940: loss 1.8637, time 133.64ms, mfu 0.31%\n",
            "iter 950: loss 1.8685, time 134.24ms, mfu 0.31%\n",
            "iter 960: loss 1.8656, time 134.85ms, mfu 0.31%\n",
            "iter 970: loss 1.8614, time 132.62ms, mfu 0.31%\n",
            "iter 980: loss 1.8605, time 134.88ms, mfu 0.31%\n",
            "iter 990: loss 1.8260, time 133.94ms, mfu 0.31%\n",
            "step 1000: train loss 1.6895, val loss 1.8551\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.8106, time 20026.78ms, mfu 0.28%\n",
            "iter 1010: loss 1.8430, time 132.67ms, mfu 0.28%\n",
            "iter 1020: loss 1.8106, time 136.15ms, mfu 0.29%\n",
            "iter 1030: loss 1.8296, time 135.66ms, mfu 0.29%\n",
            "iter 1040: loss 1.7847, time 135.29ms, mfu 0.29%\n",
            "iter 1050: loss 1.7994, time 133.54ms, mfu 0.29%\n",
            "iter 1060: loss 1.8197, time 134.88ms, mfu 0.30%\n",
            "iter 1070: loss 1.7882, time 135.86ms, mfu 0.30%\n",
            "iter 1080: loss 1.7781, time 133.29ms, mfu 0.30%\n",
            "iter 1090: loss 1.7900, time 133.99ms, mfu 0.30%\n",
            "iter 1100: loss 1.7992, time 134.90ms, mfu 0.30%\n",
            "iter 1110: loss 1.8084, time 135.83ms, mfu 0.30%\n",
            "iter 1120: loss 1.7733, time 136.61ms, mfu 0.30%\n",
            "iter 1130: loss 1.7805, time 135.18ms, mfu 0.30%\n",
            "iter 1140: loss 1.7964, time 134.60ms, mfu 0.31%\n",
            "iter 1150: loss 1.7525, time 133.99ms, mfu 0.31%\n",
            "iter 1160: loss 1.7599, time 134.80ms, mfu 0.31%\n",
            "iter 1170: loss 1.7849, time 135.81ms, mfu 0.31%\n",
            "iter 1180: loss 1.7539, time 134.89ms, mfu 0.31%\n",
            "iter 1190: loss 1.7526, time 135.13ms, mfu 0.31%\n",
            "iter 1200: loss 1.7851, time 134.33ms, mfu 0.31%\n",
            "iter 1210: loss 1.7343, time 137.44ms, mfu 0.31%\n",
            "iter 1220: loss 1.7400, time 135.42ms, mfu 0.31%\n",
            "iter 1230: loss 1.7370, time 134.54ms, mfu 0.31%\n",
            "iter 1240: loss 1.7809, time 135.55ms, mfu 0.31%\n",
            "step 1250: train loss 1.6110, val loss 1.7858\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.8113, time 20055.99ms, mfu 0.28%\n",
            "iter 1260: loss 1.7586, time 134.07ms, mfu 0.28%\n",
            "iter 1270: loss 1.7468, time 135.44ms, mfu 0.29%\n",
            "iter 1280: loss 1.7267, time 134.66ms, mfu 0.29%\n",
            "iter 1290: loss 1.7173, time 134.89ms, mfu 0.29%\n",
            "iter 1300: loss 1.7236, time 134.63ms, mfu 0.29%\n",
            "iter 1310: loss 1.7206, time 135.32ms, mfu 0.29%\n",
            "iter 1320: loss 1.7375, time 133.39ms, mfu 0.30%\n",
            "iter 1330: loss 1.7301, time 135.50ms, mfu 0.30%\n",
            "iter 1340: loss 1.7366, time 133.86ms, mfu 0.30%\n",
            "iter 1350: loss 1.6741, time 135.47ms, mfu 0.30%\n",
            "iter 1360: loss 1.7638, time 134.17ms, mfu 0.30%\n",
            "iter 1370: loss 1.7065, time 135.74ms, mfu 0.30%\n",
            "iter 1380: loss 1.7178, time 134.66ms, mfu 0.30%\n",
            "iter 1390: loss 1.7200, time 134.89ms, mfu 0.31%\n",
            "iter 1400: loss 1.6767, time 135.37ms, mfu 0.31%\n",
            "iter 1410: loss 1.6837, time 133.23ms, mfu 0.31%\n",
            "iter 1420: loss 1.6915, time 134.43ms, mfu 0.31%\n",
            "iter 1430: loss 1.7072, time 133.78ms, mfu 0.31%\n",
            "iter 1440: loss 1.6906, time 135.50ms, mfu 0.31%\n",
            "iter 1450: loss 1.6856, time 131.76ms, mfu 0.31%\n",
            "iter 1460: loss 1.6912, time 134.96ms, mfu 0.31%\n",
            "iter 1470: loss 1.6962, time 135.30ms, mfu 0.31%\n",
            "iter 1480: loss 1.6380, time 132.39ms, mfu 0.31%\n",
            "iter 1490: loss 1.6922, time 135.02ms, mfu 0.31%\n",
            "step 1500: train loss 1.5513, val loss 1.7293\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.6925, time 20038.84ms, mfu 0.28%\n",
            "iter 1510: loss 1.6609, time 134.12ms, mfu 0.28%\n",
            "iter 1520: loss 1.6838, time 136.04ms, mfu 0.29%\n",
            "iter 1530: loss 1.6690, time 135.03ms, mfu 0.29%\n",
            "iter 1540: loss 1.6944, time 135.07ms, mfu 0.29%\n",
            "iter 1550: loss 1.6615, time 134.84ms, mfu 0.29%\n",
            "iter 1560: loss 1.6503, time 136.50ms, mfu 0.29%\n",
            "iter 1570: loss 1.6649, time 134.87ms, mfu 0.30%\n",
            "iter 1580: loss 1.6651, time 134.01ms, mfu 0.30%\n",
            "iter 1590: loss 1.6677, time 134.06ms, mfu 0.30%\n",
            "iter 1600: loss 1.6645, time 135.24ms, mfu 0.30%\n",
            "iter 1610: loss 1.6742, time 134.66ms, mfu 0.30%\n",
            "iter 1620: loss 1.6544, time 134.18ms, mfu 0.30%\n",
            "iter 1630: loss 1.6505, time 134.82ms, mfu 0.30%\n",
            "iter 1640: loss 1.6755, time 135.10ms, mfu 0.31%\n",
            "iter 1650: loss 1.6581, time 135.13ms, mfu 0.31%\n",
            "iter 1660: loss 1.6585, time 135.84ms, mfu 0.31%\n",
            "iter 1670: loss 1.6230, time 134.17ms, mfu 0.31%\n",
            "iter 1680: loss 1.6510, time 135.80ms, mfu 0.31%\n",
            "iter 1690: loss 1.6402, time 135.43ms, mfu 0.31%\n",
            "iter 1700: loss 1.6526, time 134.92ms, mfu 0.31%\n",
            "iter 1710: loss 1.6461, time 134.35ms, mfu 0.31%\n",
            "iter 1720: loss 1.6589, time 133.67ms, mfu 0.31%\n",
            "iter 1730: loss 1.6095, time 134.78ms, mfu 0.31%\n",
            "iter 1740: loss 1.6317, time 135.09ms, mfu 0.31%\n",
            "step 1750: train loss 1.5050, val loss 1.7007\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.6001, time 20029.89ms, mfu 0.28%\n",
            "iter 1760: loss 1.6344, time 132.92ms, mfu 0.28%\n",
            "iter 1770: loss 1.6148, time 136.18ms, mfu 0.29%\n",
            "iter 1780: loss 1.6185, time 134.74ms, mfu 0.29%\n",
            "iter 1790: loss 1.6451, time 133.78ms, mfu 0.29%\n",
            "iter 1800: loss 1.6494, time 135.02ms, mfu 0.29%\n",
            "iter 1810: loss 1.6221, time 135.07ms, mfu 0.30%\n",
            "iter 1820: loss 1.6648, time 135.94ms, mfu 0.30%\n",
            "iter 1830: loss 1.6327, time 134.09ms, mfu 0.30%\n",
            "iter 1840: loss 1.6192, time 135.59ms, mfu 0.30%\n",
            "iter 1850: loss 1.6422, time 135.73ms, mfu 0.30%\n",
            "iter 1860: loss 1.6290, time 133.54ms, mfu 0.30%\n",
            "iter 1870: loss 1.6058, time 134.54ms, mfu 0.30%\n",
            "iter 1880: loss 1.6204, time 135.05ms, mfu 0.30%\n",
            "iter 1890: loss 1.6243, time 134.13ms, mfu 0.31%\n",
            "iter 1900: loss 1.5712, time 135.25ms, mfu 0.31%\n",
            "iter 1910: loss 1.6347, time 133.46ms, mfu 0.31%\n",
            "iter 1920: loss 1.6129, time 133.98ms, mfu 0.31%\n",
            "iter 1930: loss 1.6073, time 136.21ms, mfu 0.31%\n",
            "iter 1940: loss 1.6457, time 134.16ms, mfu 0.31%\n",
            "iter 1950: loss 1.6107, time 134.61ms, mfu 0.31%\n",
            "iter 1960: loss 1.6237, time 134.47ms, mfu 0.31%\n",
            "iter 1970: loss 1.5888, time 133.94ms, mfu 0.31%\n",
            "iter 1980: loss 1.6419, time 135.32ms, mfu 0.31%\n",
            "iter 1990: loss 1.5924, time 134.88ms, mfu 0.31%\n",
            "step 2000: train loss 1.4740, val loss 1.6671\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.6117, time 20114.36ms, mfu 0.28%\n",
            "iter 2010: loss 1.6185, time 135.69ms, mfu 0.28%\n",
            "iter 2020: loss 1.5424, time 136.13ms, mfu 0.29%\n",
            "iter 2030: loss 1.5943, time 134.28ms, mfu 0.29%\n",
            "iter 2040: loss 1.6185, time 135.33ms, mfu 0.29%\n",
            "iter 2050: loss 1.6523, time 135.03ms, mfu 0.29%\n",
            "iter 2060: loss 1.6307, time 133.22ms, mfu 0.30%\n",
            "iter 2070: loss 1.6255, time 134.55ms, mfu 0.30%\n",
            "iter 2080: loss 1.5836, time 134.22ms, mfu 0.30%\n",
            "iter 2090: loss 1.6069, time 133.65ms, mfu 0.30%\n",
            "iter 2100: loss 1.5812, time 135.18ms, mfu 0.30%\n",
            "iter 2110: loss 1.5850, time 134.92ms, mfu 0.30%\n",
            "iter 2120: loss 1.5795, time 134.77ms, mfu 0.30%\n",
            "iter 2130: loss 1.5977, time 136.50ms, mfu 0.30%\n",
            "iter 2140: loss 1.5472, time 135.19ms, mfu 0.30%\n",
            "iter 2150: loss 1.5363, time 134.02ms, mfu 0.31%\n",
            "iter 2160: loss 1.6223, time 135.63ms, mfu 0.31%\n",
            "iter 2170: loss 1.5942, time 134.87ms, mfu 0.31%\n",
            "iter 2180: loss 1.5663, time 134.41ms, mfu 0.31%\n",
            "iter 2190: loss 1.5909, time 134.83ms, mfu 0.31%\n",
            "iter 2200: loss 1.5922, time 135.83ms, mfu 0.31%\n",
            "iter 2210: loss 1.5814, time 135.06ms, mfu 0.31%\n",
            "iter 2220: loss 1.6165, time 135.27ms, mfu 0.31%\n",
            "iter 2230: loss 1.5696, time 133.97ms, mfu 0.31%\n",
            "iter 2240: loss 1.5692, time 135.61ms, mfu 0.31%\n",
            "step 2250: train loss 1.4516, val loss 1.6481\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.5524, time 20050.72ms, mfu 0.28%\n",
            "iter 2260: loss 1.5828, time 134.86ms, mfu 0.28%\n",
            "iter 2270: loss 1.5518, time 135.21ms, mfu 0.29%\n",
            "iter 2280: loss 1.5801, time 135.85ms, mfu 0.29%\n",
            "iter 2290: loss 1.5645, time 132.59ms, mfu 0.29%\n",
            "iter 2300: loss 1.5670, time 135.51ms, mfu 0.29%\n",
            "iter 2310: loss 1.5751, time 136.31ms, mfu 0.29%\n",
            "iter 2320: loss 1.5751, time 134.85ms, mfu 0.30%\n",
            "iter 2330: loss 1.5710, time 133.60ms, mfu 0.30%\n",
            "iter 2340: loss 1.5803, time 134.76ms, mfu 0.30%\n",
            "iter 2350: loss 1.5774, time 134.75ms, mfu 0.30%\n",
            "iter 2360: loss 1.5703, time 136.50ms, mfu 0.30%\n",
            "iter 2370: loss 1.5529, time 134.74ms, mfu 0.30%\n",
            "iter 2380: loss 1.5880, time 133.47ms, mfu 0.30%\n",
            "iter 2390: loss 1.5866, time 134.94ms, mfu 0.31%\n",
            "iter 2400: loss 1.5428, time 134.73ms, mfu 0.31%\n",
            "iter 2410: loss 1.5383, time 134.84ms, mfu 0.31%\n",
            "iter 2420: loss 1.5876, time 134.83ms, mfu 0.31%\n",
            "iter 2430: loss 1.6136, time 135.75ms, mfu 0.31%\n",
            "iter 2440: loss 1.5369, time 134.75ms, mfu 0.31%\n",
            "iter 2450: loss 1.5252, time 134.31ms, mfu 0.31%\n",
            "iter 2460: loss 1.5376, time 135.77ms, mfu 0.31%\n",
            "iter 2470: loss 1.5875, time 134.73ms, mfu 0.31%\n",
            "iter 2480: loss 1.5734, time 134.49ms, mfu 0.31%\n",
            "iter 2490: loss 1.5543, time 135.02ms, mfu 0.31%\n",
            "step 2500: train loss 1.4371, val loss 1.6406\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.5513, time 20117.83ms, mfu 0.28%\n",
            "iter 2510: loss 1.5553, time 135.69ms, mfu 0.28%\n",
            "iter 2520: loss 1.5821, time 135.52ms, mfu 0.29%\n",
            "iter 2530: loss 1.5694, time 134.22ms, mfu 0.29%\n",
            "iter 2540: loss 1.5281, time 134.30ms, mfu 0.29%\n",
            "iter 2550: loss 1.5529, time 134.85ms, mfu 0.29%\n",
            "iter 2560: loss 1.5766, time 135.45ms, mfu 0.29%\n",
            "iter 2570: loss 1.5446, time 135.17ms, mfu 0.30%\n",
            "iter 2580: loss 1.5974, time 133.30ms, mfu 0.30%\n",
            "iter 2590: loss 1.5435, time 135.45ms, mfu 0.30%\n",
            "iter 2600: loss 1.5723, time 133.27ms, mfu 0.30%\n",
            "iter 2610: loss 1.5340, time 134.95ms, mfu 0.30%\n",
            "iter 2620: loss 1.5304, time 135.34ms, mfu 0.30%\n",
            "iter 2630: loss 1.5409, time 134.13ms, mfu 0.30%\n",
            "iter 2640: loss 1.5611, time 135.93ms, mfu 0.31%\n",
            "iter 2650: loss 1.5382, time 135.43ms, mfu 0.31%\n",
            "iter 2660: loss 1.5556, time 132.66ms, mfu 0.31%\n",
            "iter 2670: loss 1.5596, time 136.12ms, mfu 0.31%\n",
            "iter 2680: loss 1.5494, time 134.38ms, mfu 0.31%\n",
            "iter 2690: loss 1.5534, time 134.13ms, mfu 0.31%\n",
            "iter 2700: loss 1.5173, time 135.78ms, mfu 0.31%\n",
            "iter 2710: loss 1.5569, time 133.83ms, mfu 0.31%\n",
            "iter 2720: loss 1.5465, time 135.91ms, mfu 0.31%\n",
            "iter 2730: loss 1.5429, time 134.61ms, mfu 0.31%\n",
            "iter 2740: loss 1.5561, time 134.96ms, mfu 0.31%\n",
            "step 2750: train loss 1.4231, val loss 1.6243\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.5835, time 20052.01ms, mfu 0.28%\n",
            "iter 2760: loss 1.5412, time 133.46ms, mfu 0.28%\n",
            "iter 2770: loss 1.5594, time 135.38ms, mfu 0.29%\n",
            "iter 2780: loss 1.5317, time 135.47ms, mfu 0.29%\n",
            "iter 2790: loss 1.5375, time 134.15ms, mfu 0.29%\n",
            "iter 2800: loss 1.5344, time 135.41ms, mfu 0.29%\n",
            "iter 2810: loss 1.5311, time 134.84ms, mfu 0.30%\n",
            "iter 2820: loss 1.5740, time 133.52ms, mfu 0.30%\n",
            "iter 2830: loss 1.5311, time 133.81ms, mfu 0.30%\n",
            "iter 2840: loss 1.5621, time 135.30ms, mfu 0.30%\n",
            "iter 2850: loss 1.5427, time 132.78ms, mfu 0.30%\n",
            "iter 2860: loss 1.5486, time 134.22ms, mfu 0.30%\n",
            "iter 2870: loss 1.5184, time 135.35ms, mfu 0.30%\n",
            "iter 2880: loss 1.5323, time 136.16ms, mfu 0.30%\n",
            "iter 2890: loss 1.5239, time 135.16ms, mfu 0.31%\n",
            "iter 2900: loss 1.5207, time 135.65ms, mfu 0.31%\n",
            "iter 2910: loss 1.5231, time 134.96ms, mfu 0.31%\n",
            "iter 2920: loss 1.5409, time 133.87ms, mfu 0.31%\n",
            "iter 2930: loss 1.5206, time 133.22ms, mfu 0.31%\n",
            "iter 2940: loss 1.5298, time 134.99ms, mfu 0.31%\n",
            "iter 2950: loss 1.5423, time 134.33ms, mfu 0.31%\n",
            "iter 2960: loss 1.5706, time 135.70ms, mfu 0.31%\n",
            "iter 2970: loss 1.5464, time 134.65ms, mfu 0.31%\n",
            "iter 2980: loss 1.5479, time 135.23ms, mfu 0.31%\n",
            "iter 2990: loss 1.5469, time 134.06ms, mfu 0.31%\n",
            "step 3000: train loss 1.4165, val loss 1.6190\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.5529, time 20009.84ms, mfu 0.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --compile=False --log_interval=10 --n_layer=5 --n_head=5 --n_embd=130 --max_iters=3000 --lr_decay_iters=3000"
      ],
      "metadata": {
        "id": "E5Yh9M0RTwSS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e97def04-cc82-48e7-a35d-fd1304fb3ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: compile = False\n",
            "Overriding: log_interval = 10\n",
            "Overriding: n_layer = 5\n",
            "Overriding: n_head = 5\n",
            "Overriding: n_embd = 130\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: lr_decay_iters = 3000\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.02M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 22, with 1,055,730 parameters\n",
            "num non-decayed parameter tensors: 11, with 1,430 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2156, val loss 4.2102\n",
            "iter 0: loss 4.2172, time 23510.58ms, mfu -100.00%\n",
            "iter 10: loss 3.7812, time 154.00ms, mfu 0.28%\n",
            "iter 20: loss 3.5745, time 154.25ms, mfu 0.28%\n",
            "iter 30: loss 3.2893, time 153.46ms, mfu 0.28%\n",
            "iter 40: loss 3.0641, time 155.43ms, mfu 0.28%\n",
            "iter 50: loss 2.8601, time 154.72ms, mfu 0.28%\n",
            "iter 60: loss 2.7376, time 153.76ms, mfu 0.28%\n",
            "iter 70: loss 2.6675, time 154.28ms, mfu 0.28%\n",
            "iter 80: loss 2.6223, time 153.16ms, mfu 0.28%\n",
            "iter 90: loss 2.5960, time 155.79ms, mfu 0.28%\n",
            "iter 100: loss 2.5913, time 153.11ms, mfu 0.28%\n",
            "iter 110: loss 2.5655, time 154.73ms, mfu 0.28%\n",
            "iter 120: loss 2.5490, time 154.30ms, mfu 0.28%\n",
            "iter 130: loss 2.5273, time 153.47ms, mfu 0.28%\n",
            "iter 140: loss 2.5220, time 153.91ms, mfu 0.28%\n",
            "iter 150: loss 2.4986, time 153.47ms, mfu 0.28%\n",
            "iter 160: loss 2.5019, time 154.00ms, mfu 0.28%\n",
            "iter 170: loss 2.4997, time 156.14ms, mfu 0.28%\n",
            "iter 180: loss 2.4741, time 153.11ms, mfu 0.28%\n",
            "iter 190: loss 2.4808, time 153.04ms, mfu 0.28%\n",
            "iter 200: loss 2.4862, time 153.53ms, mfu 0.28%\n",
            "iter 210: loss 2.4638, time 152.60ms, mfu 0.28%\n",
            "iter 220: loss 2.4567, time 152.99ms, mfu 0.28%\n",
            "iter 230: loss 2.4453, time 153.21ms, mfu 0.28%\n",
            "iter 240: loss 2.4532, time 152.48ms, mfu 0.28%\n",
            "step 250: train loss 2.4117, val loss 2.4196\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.4513, time 23411.83ms, mfu 0.25%\n",
            "iter 260: loss 2.4297, time 153.73ms, mfu 0.25%\n",
            "iter 270: loss 2.4032, time 155.34ms, mfu 0.26%\n",
            "iter 280: loss 2.4292, time 152.74ms, mfu 0.26%\n",
            "iter 290: loss 2.4252, time 154.05ms, mfu 0.26%\n",
            "iter 300: loss 2.4122, time 153.48ms, mfu 0.26%\n",
            "iter 310: loss 2.4099, time 152.43ms, mfu 0.26%\n",
            "iter 320: loss 2.3896, time 153.28ms, mfu 0.27%\n",
            "iter 330: loss 2.3719, time 151.99ms, mfu 0.27%\n",
            "iter 340: loss 2.3675, time 153.28ms, mfu 0.27%\n",
            "iter 350: loss 2.3706, time 154.16ms, mfu 0.27%\n",
            "iter 360: loss 2.3444, time 153.14ms, mfu 0.27%\n",
            "iter 370: loss 2.3515, time 154.88ms, mfu 0.27%\n",
            "iter 380: loss 2.3199, time 154.97ms, mfu 0.27%\n",
            "iter 390: loss 2.3278, time 157.74ms, mfu 0.27%\n",
            "iter 400: loss 2.3015, time 155.68ms, mfu 0.27%\n",
            "iter 410: loss 2.3043, time 154.04ms, mfu 0.27%\n",
            "iter 420: loss 2.2945, time 152.90ms, mfu 0.27%\n",
            "iter 430: loss 2.2896, time 153.36ms, mfu 0.27%\n",
            "iter 440: loss 2.2753, time 154.07ms, mfu 0.27%\n",
            "iter 450: loss 2.2691, time 155.10ms, mfu 0.27%\n",
            "iter 460: loss 2.2529, time 156.04ms, mfu 0.27%\n",
            "iter 470: loss 2.2567, time 154.82ms, mfu 0.27%\n",
            "iter 480: loss 2.2347, time 156.15ms, mfu 0.27%\n",
            "iter 490: loss 2.2334, time 154.98ms, mfu 0.27%\n",
            "step 500: train loss 2.1266, val loss 2.1870\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 2.2310, time 23482.83ms, mfu 0.25%\n",
            "iter 510: loss 2.2410, time 153.16ms, mfu 0.25%\n",
            "iter 520: loss 2.1774, time 154.30ms, mfu 0.25%\n",
            "iter 530: loss 2.2013, time 153.94ms, mfu 0.26%\n",
            "iter 540: loss 2.1971, time 153.93ms, mfu 0.26%\n",
            "iter 550: loss 2.1781, time 154.38ms, mfu 0.26%\n",
            "iter 560: loss 2.1884, time 154.50ms, mfu 0.26%\n",
            "iter 570: loss 2.1689, time 154.08ms, mfu 0.26%\n",
            "iter 580: loss 2.1396, time 153.24ms, mfu 0.26%\n",
            "iter 590: loss 2.1265, time 153.74ms, mfu 0.27%\n",
            "iter 600: loss 2.1254, time 154.18ms, mfu 0.27%\n",
            "iter 610: loss 2.1156, time 155.59ms, mfu 0.27%\n",
            "iter 620: loss 2.0924, time 154.07ms, mfu 0.27%\n",
            "iter 630: loss 2.0940, time 154.84ms, mfu 0.27%\n",
            "iter 640: loss 2.0952, time 152.79ms, mfu 0.27%\n",
            "iter 650: loss 2.0876, time 154.55ms, mfu 0.27%\n",
            "iter 660: loss 2.0619, time 152.20ms, mfu 0.27%\n",
            "iter 670: loss 2.0875, time 153.73ms, mfu 0.27%\n",
            "iter 680: loss 2.0301, time 153.05ms, mfu 0.27%\n",
            "iter 690: loss 2.0372, time 153.41ms, mfu 0.27%\n",
            "iter 700: loss 1.9980, time 153.87ms, mfu 0.27%\n",
            "iter 710: loss 2.0232, time 153.39ms, mfu 0.27%\n",
            "iter 720: loss 2.0292, time 152.60ms, mfu 0.28%\n",
            "iter 730: loss 2.0312, time 154.49ms, mfu 0.28%\n",
            "iter 740: loss 1.9952, time 154.61ms, mfu 0.28%\n",
            "step 750: train loss 1.8533, val loss 1.9659\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.9955, time 23441.43ms, mfu 0.25%\n",
            "iter 760: loss 1.9853, time 153.51ms, mfu 0.25%\n",
            "iter 770: loss 1.9515, time 153.36ms, mfu 0.25%\n",
            "iter 780: loss 1.9406, time 154.37ms, mfu 0.26%\n",
            "iter 790: loss 1.9757, time 155.01ms, mfu 0.26%\n",
            "iter 800: loss 1.9533, time 154.41ms, mfu 0.26%\n",
            "iter 810: loss 1.9564, time 153.81ms, mfu 0.26%\n",
            "iter 820: loss 1.9477, time 153.97ms, mfu 0.26%\n",
            "iter 830: loss 1.9597, time 154.19ms, mfu 0.26%\n",
            "iter 840: loss 1.9420, time 153.38ms, mfu 0.27%\n",
            "iter 850: loss 1.9284, time 154.42ms, mfu 0.27%\n",
            "iter 860: loss 1.8943, time 154.41ms, mfu 0.27%\n",
            "iter 870: loss 1.8878, time 153.39ms, mfu 0.27%\n",
            "iter 880: loss 1.8851, time 152.55ms, mfu 0.27%\n",
            "iter 890: loss 1.9151, time 151.50ms, mfu 0.27%\n",
            "iter 900: loss 1.8946, time 153.22ms, mfu 0.27%\n",
            "iter 910: loss 1.8847, time 153.25ms, mfu 0.27%\n",
            "iter 920: loss 1.8648, time 153.07ms, mfu 0.27%\n",
            "iter 930: loss 1.8687, time 155.00ms, mfu 0.27%\n",
            "iter 940: loss 1.8749, time 154.40ms, mfu 0.27%\n",
            "iter 950: loss 1.8781, time 152.02ms, mfu 0.27%\n",
            "iter 960: loss 1.8292, time 152.05ms, mfu 0.28%\n",
            "iter 970: loss 1.8517, time 152.32ms, mfu 0.28%\n",
            "iter 980: loss 1.8518, time 153.39ms, mfu 0.28%\n",
            "iter 990: loss 1.8713, time 152.85ms, mfu 0.28%\n",
            "step 1000: train loss 1.7060, val loss 1.8637\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.8529, time 23384.39ms, mfu 0.25%\n",
            "iter 1010: loss 1.8437, time 155.10ms, mfu 0.25%\n",
            "iter 1020: loss 1.8681, time 155.88ms, mfu 0.25%\n",
            "iter 1030: loss 1.8313, time 152.17ms, mfu 0.26%\n",
            "iter 1040: loss 1.8467, time 154.62ms, mfu 0.26%\n",
            "iter 1050: loss 1.8167, time 152.83ms, mfu 0.26%\n",
            "iter 1060: loss 1.8398, time 154.41ms, mfu 0.26%\n",
            "iter 1070: loss 1.8326, time 155.02ms, mfu 0.26%\n",
            "iter 1080: loss 1.8230, time 153.38ms, mfu 0.27%\n",
            "iter 1090: loss 1.8217, time 153.96ms, mfu 0.27%\n",
            "iter 1100: loss 1.8146, time 152.13ms, mfu 0.27%\n",
            "iter 1110: loss 1.8017, time 153.49ms, mfu 0.27%\n",
            "iter 1120: loss 1.7971, time 154.53ms, mfu 0.27%\n",
            "iter 1130: loss 1.7796, time 152.40ms, mfu 0.27%\n",
            "iter 1140: loss 1.7862, time 153.04ms, mfu 0.27%\n",
            "iter 1150: loss 1.7837, time 154.10ms, mfu 0.27%\n",
            "iter 1160: loss 1.7578, time 152.96ms, mfu 0.27%\n",
            "iter 1170: loss 1.7820, time 152.68ms, mfu 0.27%\n",
            "iter 1180: loss 1.7548, time 152.10ms, mfu 0.27%\n",
            "iter 1190: loss 1.7627, time 152.73ms, mfu 0.27%\n",
            "iter 1200: loss 1.7876, time 152.78ms, mfu 0.28%\n",
            "iter 1210: loss 1.7059, time 152.94ms, mfu 0.28%\n",
            "iter 1220: loss 1.7328, time 154.08ms, mfu 0.28%\n",
            "iter 1230: loss 1.7585, time 154.66ms, mfu 0.28%\n",
            "iter 1240: loss 1.7280, time 152.66ms, mfu 0.28%\n",
            "step 1250: train loss 1.6158, val loss 1.7915\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.7637, time 23488.01ms, mfu 0.25%\n",
            "iter 1260: loss 1.7546, time 154.36ms, mfu 0.25%\n",
            "iter 1270: loss 1.7389, time 153.14ms, mfu 0.25%\n",
            "iter 1280: loss 1.7182, time 154.17ms, mfu 0.26%\n",
            "iter 1290: loss 1.7328, time 152.95ms, mfu 0.26%\n",
            "iter 1300: loss 1.7452, time 154.68ms, mfu 0.26%\n",
            "iter 1310: loss 1.7164, time 153.58ms, mfu 0.26%\n",
            "iter 1320: loss 1.7171, time 152.60ms, mfu 0.26%\n",
            "iter 1330: loss 1.7303, time 153.93ms, mfu 0.27%\n",
            "iter 1340: loss 1.7160, time 156.32ms, mfu 0.27%\n",
            "iter 1350: loss 1.7009, time 152.56ms, mfu 0.27%\n",
            "iter 1360: loss 1.7012, time 152.40ms, mfu 0.27%\n",
            "iter 1370: loss 1.7458, time 153.18ms, mfu 0.27%\n",
            "iter 1380: loss 1.7145, time 156.22ms, mfu 0.27%\n",
            "iter 1390: loss 1.6699, time 152.57ms, mfu 0.27%\n",
            "iter 1400: loss 1.7138, time 153.71ms, mfu 0.27%\n",
            "iter 1410: loss 1.7146, time 153.79ms, mfu 0.27%\n",
            "iter 1420: loss 1.7066, time 154.46ms, mfu 0.27%\n",
            "iter 1430: loss 1.6904, time 153.94ms, mfu 0.27%\n",
            "iter 1440: loss 1.6783, time 154.63ms, mfu 0.27%\n",
            "iter 1450: loss 1.6771, time 153.23ms, mfu 0.27%\n",
            "iter 1460: loss 1.6955, time 152.07ms, mfu 0.28%\n",
            "iter 1470: loss 1.7064, time 153.63ms, mfu 0.28%\n",
            "iter 1480: loss 1.6735, time 153.00ms, mfu 0.28%\n",
            "iter 1490: loss 1.6940, time 151.94ms, mfu 0.28%\n",
            "step 1500: train loss 1.5477, val loss 1.7363\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.6852, time 23495.22ms, mfu 0.25%\n",
            "iter 1510: loss 1.6894, time 152.61ms, mfu 0.25%\n",
            "iter 1520: loss 1.6673, time 153.71ms, mfu 0.25%\n",
            "iter 1530: loss 1.6613, time 152.53ms, mfu 0.26%\n",
            "iter 1540: loss 1.6611, time 152.90ms, mfu 0.26%\n",
            "iter 1550: loss 1.6499, time 154.60ms, mfu 0.26%\n",
            "iter 1560: loss 1.6764, time 154.60ms, mfu 0.26%\n",
            "iter 1570: loss 1.6909, time 154.08ms, mfu 0.26%\n",
            "iter 1580: loss 1.6982, time 152.30ms, mfu 0.27%\n",
            "iter 1590: loss 1.6905, time 154.15ms, mfu 0.27%\n",
            "iter 1600: loss 1.6028, time 154.14ms, mfu 0.27%\n",
            "iter 1610: loss 1.6560, time 153.78ms, mfu 0.27%\n",
            "iter 1620: loss 1.6382, time 153.54ms, mfu 0.27%\n",
            "iter 1630: loss 1.6214, time 154.32ms, mfu 0.27%\n",
            "iter 1640: loss 1.6476, time 154.64ms, mfu 0.27%\n",
            "iter 1650: loss 1.6701, time 153.33ms, mfu 0.27%\n",
            "iter 1660: loss 1.6216, time 154.04ms, mfu 0.27%\n",
            "iter 1670: loss 1.6329, time 154.10ms, mfu 0.27%\n",
            "iter 1680: loss 1.6756, time 153.20ms, mfu 0.27%\n",
            "iter 1690: loss 1.6507, time 155.65ms, mfu 0.27%\n",
            "iter 1700: loss 1.6204, time 155.54ms, mfu 0.27%\n",
            "iter 1710: loss 1.6405, time 155.09ms, mfu 0.27%\n",
            "iter 1720: loss 1.6801, time 154.34ms, mfu 0.27%\n",
            "iter 1730: loss 1.6296, time 154.69ms, mfu 0.27%\n",
            "iter 1740: loss 1.6280, time 153.56ms, mfu 0.27%\n",
            "step 1750: train loss 1.5097, val loss 1.7040\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.6297, time 23465.76ms, mfu 0.25%\n",
            "iter 1760: loss 1.6506, time 152.87ms, mfu 0.25%\n",
            "iter 1770: loss 1.6522, time 152.97ms, mfu 0.25%\n",
            "iter 1780: loss 1.5981, time 153.41ms, mfu 0.26%\n",
            "iter 1790: loss 1.6429, time 153.12ms, mfu 0.26%\n",
            "iter 1800: loss 1.6049, time 152.81ms, mfu 0.26%\n",
            "iter 1810: loss 1.6505, time 153.37ms, mfu 0.26%\n",
            "iter 1820: loss 1.6154, time 153.75ms, mfu 0.26%\n",
            "iter 1830: loss 1.6242, time 153.14ms, mfu 0.27%\n",
            "iter 1840: loss 1.6101, time 154.59ms, mfu 0.27%\n",
            "iter 1850: loss 1.6602, time 154.78ms, mfu 0.27%\n",
            "iter 1860: loss 1.6238, time 154.03ms, mfu 0.27%\n",
            "iter 1870: loss 1.6068, time 154.26ms, mfu 0.27%\n",
            "iter 1880: loss 1.6023, time 155.49ms, mfu 0.27%\n",
            "iter 1890: loss 1.5623, time 155.47ms, mfu 0.27%\n",
            "iter 1900: loss 1.6244, time 154.62ms, mfu 0.27%\n",
            "iter 1910: loss 1.6039, time 154.37ms, mfu 0.27%\n",
            "iter 1920: loss 1.6340, time 153.17ms, mfu 0.27%\n",
            "iter 1930: loss 1.5943, time 153.81ms, mfu 0.27%\n",
            "iter 1940: loss 1.6244, time 152.64ms, mfu 0.27%\n",
            "iter 1950: loss 1.5973, time 154.45ms, mfu 0.27%\n",
            "iter 1960: loss 1.6433, time 153.47ms, mfu 0.27%\n",
            "iter 1970: loss 1.6007, time 156.28ms, mfu 0.27%\n",
            "iter 1980: loss 1.5989, time 153.34ms, mfu 0.27%\n",
            "iter 1990: loss 1.5982, time 153.21ms, mfu 0.28%\n",
            "step 2000: train loss 1.4784, val loss 1.6727\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.6191, time 23523.68ms, mfu 0.25%\n",
            "iter 2010: loss 1.6059, time 153.02ms, mfu 0.25%\n",
            "iter 2020: loss 1.5640, time 153.69ms, mfu 0.25%\n",
            "iter 2030: loss 1.5986, time 154.05ms, mfu 0.26%\n",
            "iter 2040: loss 1.6111, time 153.06ms, mfu 0.26%\n",
            "iter 2050: loss 1.6187, time 153.45ms, mfu 0.26%\n",
            "iter 2060: loss 1.5926, time 153.43ms, mfu 0.26%\n",
            "iter 2070: loss 1.6078, time 153.29ms, mfu 0.26%\n",
            "iter 2080: loss 1.5895, time 153.36ms, mfu 0.27%\n",
            "iter 2090: loss 1.6047, time 153.46ms, mfu 0.27%\n",
            "iter 2100: loss 1.5873, time 155.20ms, mfu 0.27%\n",
            "iter 2110: loss 1.5859, time 153.52ms, mfu 0.27%\n",
            "iter 2120: loss 1.5980, time 153.59ms, mfu 0.27%\n",
            "iter 2130: loss 1.5764, time 153.28ms, mfu 0.27%\n",
            "iter 2140: loss 1.5826, time 153.36ms, mfu 0.27%\n",
            "iter 2150: loss 1.5883, time 152.94ms, mfu 0.27%\n",
            "iter 2160: loss 1.5984, time 153.56ms, mfu 0.27%\n",
            "iter 2170: loss 1.5533, time 154.53ms, mfu 0.27%\n",
            "iter 2180: loss 1.5890, time 154.82ms, mfu 0.27%\n",
            "iter 2190: loss 1.5862, time 154.15ms, mfu 0.27%\n",
            "iter 2200: loss 1.5619, time 154.01ms, mfu 0.27%\n",
            "iter 2210: loss 1.5828, time 154.04ms, mfu 0.27%\n",
            "iter 2220: loss 1.5598, time 153.16ms, mfu 0.28%\n",
            "iter 2230: loss 1.5841, time 152.61ms, mfu 0.28%\n",
            "iter 2240: loss 1.5802, time 152.93ms, mfu 0.28%\n",
            "step 2250: train loss 1.4573, val loss 1.6535\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.5888, time 23425.08ms, mfu 0.25%\n",
            "iter 2260: loss 1.5868, time 154.64ms, mfu 0.25%\n",
            "iter 2270: loss 1.5819, time 156.22ms, mfu 0.25%\n",
            "iter 2280: loss 1.5599, time 154.71ms, mfu 0.26%\n",
            "iter 2290: loss 1.5769, time 155.02ms, mfu 0.26%\n",
            "iter 2300: loss 1.5648, time 152.86ms, mfu 0.26%\n",
            "iter 2310: loss 1.5886, time 152.78ms, mfu 0.26%\n",
            "iter 2320: loss 1.5583, time 153.61ms, mfu 0.26%\n",
            "iter 2330: loss 1.5224, time 152.94ms, mfu 0.27%\n",
            "iter 2340: loss 1.5682, time 154.39ms, mfu 0.27%\n",
            "iter 2350: loss 1.5885, time 156.42ms, mfu 0.27%\n",
            "iter 2360: loss 1.5955, time 153.77ms, mfu 0.27%\n",
            "iter 2370: loss 1.5474, time 152.63ms, mfu 0.27%\n",
            "iter 2380: loss 1.5652, time 154.29ms, mfu 0.27%\n",
            "iter 2390: loss 1.5844, time 155.15ms, mfu 0.27%\n",
            "iter 2400: loss 1.5670, time 154.44ms, mfu 0.27%\n",
            "iter 2410: loss 1.5555, time 152.91ms, mfu 0.27%\n",
            "iter 2420: loss 1.5394, time 154.57ms, mfu 0.27%\n",
            "iter 2430: loss 1.5572, time 152.81ms, mfu 0.27%\n",
            "iter 2440: loss 1.5175, time 153.64ms, mfu 0.27%\n",
            "iter 2450: loss 1.5591, time 153.49ms, mfu 0.27%\n",
            "iter 2460: loss 1.5601, time 152.84ms, mfu 0.27%\n",
            "iter 2470: loss 1.5670, time 154.61ms, mfu 0.27%\n",
            "iter 2480: loss 1.5814, time 153.15ms, mfu 0.28%\n",
            "iter 2490: loss 1.5740, time 152.25ms, mfu 0.28%\n",
            "step 2500: train loss 1.4376, val loss 1.6317\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.5305, time 23411.63ms, mfu 0.25%\n",
            "iter 2510: loss 1.5402, time 154.52ms, mfu 0.25%\n",
            "iter 2520: loss 1.5613, time 152.89ms, mfu 0.25%\n",
            "iter 2530: loss 1.5669, time 155.34ms, mfu 0.26%\n",
            "iter 2540: loss 1.5911, time 154.08ms, mfu 0.26%\n",
            "iter 2550: loss 1.5604, time 154.56ms, mfu 0.26%\n",
            "iter 2560: loss 1.5721, time 153.31ms, mfu 0.26%\n",
            "iter 2570: loss 1.5346, time 153.56ms, mfu 0.26%\n",
            "iter 2580: loss 1.5753, time 152.77ms, mfu 0.27%\n",
            "iter 2590: loss 1.5514, time 153.45ms, mfu 0.27%\n",
            "iter 2600: loss 1.5517, time 153.20ms, mfu 0.27%\n",
            "iter 2610: loss 1.5465, time 153.72ms, mfu 0.27%\n",
            "iter 2620: loss 1.5297, time 153.07ms, mfu 0.27%\n",
            "iter 2630: loss 1.5227, time 152.79ms, mfu 0.27%\n",
            "iter 2640: loss 1.5786, time 153.21ms, mfu 0.27%\n",
            "iter 2650: loss 1.5696, time 152.49ms, mfu 0.27%\n",
            "iter 2660: loss 1.5357, time 153.04ms, mfu 0.27%\n",
            "iter 2670: loss 1.5408, time 153.86ms, mfu 0.27%\n",
            "iter 2680: loss 1.5259, time 153.67ms, mfu 0.27%\n",
            "iter 2690: loss 1.5360, time 154.52ms, mfu 0.27%\n",
            "iter 2700: loss 1.5419, time 154.72ms, mfu 0.27%\n",
            "iter 2710: loss 1.5429, time 154.64ms, mfu 0.27%\n",
            "iter 2720: loss 1.5512, time 154.56ms, mfu 0.27%\n",
            "iter 2730: loss 1.5457, time 153.77ms, mfu 0.28%\n",
            "iter 2740: loss 1.5670, time 152.77ms, mfu 0.28%\n",
            "step 2750: train loss 1.4287, val loss 1.6261\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.5569, time 23383.55ms, mfu 0.25%\n",
            "iter 2760: loss 1.5141, time 155.34ms, mfu 0.25%\n",
            "iter 2770: loss 1.5288, time 154.50ms, mfu 0.25%\n",
            "iter 2780: loss 1.5195, time 153.41ms, mfu 0.26%\n",
            "iter 2790: loss 1.4951, time 152.40ms, mfu 0.26%\n",
            "iter 2800: loss 1.5439, time 153.37ms, mfu 0.26%\n",
            "iter 2810: loss 1.5214, time 152.92ms, mfu 0.26%\n",
            "iter 2820: loss 1.5593, time 154.70ms, mfu 0.26%\n",
            "iter 2830: loss 1.5639, time 153.53ms, mfu 0.27%\n",
            "iter 2840: loss 1.5324, time 155.09ms, mfu 0.27%\n",
            "iter 2850: loss 1.4896, time 155.91ms, mfu 0.27%\n",
            "iter 2860: loss 1.5227, time 154.60ms, mfu 0.27%\n",
            "iter 2870: loss 1.5365, time 153.07ms, mfu 0.27%\n",
            "iter 2880: loss 1.5557, time 152.73ms, mfu 0.27%\n",
            "iter 2890: loss 1.5210, time 153.50ms, mfu 0.27%\n",
            "iter 2900: loss 1.5565, time 153.26ms, mfu 0.27%\n",
            "iter 2910: loss 1.5482, time 155.90ms, mfu 0.27%\n",
            "iter 2920: loss 1.5540, time 156.36ms, mfu 0.27%\n",
            "iter 2930: loss 1.5232, time 155.14ms, mfu 0.27%\n",
            "iter 2940: loss 1.5130, time 155.61ms, mfu 0.27%\n",
            "iter 2950: loss 1.5322, time 156.13ms, mfu 0.27%\n",
            "iter 2960: loss 1.5596, time 152.10ms, mfu 0.27%\n",
            "iter 2970: loss 1.5167, time 152.91ms, mfu 0.27%\n",
            "iter 2980: loss 1.5334, time 152.99ms, mfu 0.27%\n",
            "iter 2990: loss 1.5389, time 152.53ms, mfu 0.28%\n",
            "step 3000: train loss 1.4227, val loss 1.6204\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.5552, time 23432.64ms, mfu 0.25%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py config/train_shakespeare_char.py --compile=False --log_interval=10 --n_layer=5 --n_head=7 --n_embd=133 --max_iters=3000 --lr_decay_iters=3000"
      ],
      "metadata": {
        "id": "ox_ThV47TweL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf9bb384-1256-4cc1-acee-3a01c08fbbd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_shakespeare_char.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-shakespeare-char'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'shakespeare_char'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: compile = False\n",
            "Overriding: log_interval = 10\n",
            "Overriding: n_layer = 5\n",
            "Overriding: n_head = 7\n",
            "Overriding: n_embd = 133\n",
            "Overriding: max_iters = 3000\n",
            "Overriding: lr_decay_iters = 3000\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 65 (inside data/shakespeare_char/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 1.07M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 22, with 1,104,033 parameters\n",
            "num non-decayed parameter tensors: 11, with 1,463 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.2026, val loss 4.1964\n",
            "iter 0: loss 4.1992, time 27048.56ms, mfu -100.00%\n",
            "iter 10: loss 3.7179, time 175.38ms, mfu 0.25%\n",
            "iter 20: loss 3.5120, time 174.52ms, mfu 0.25%\n",
            "iter 30: loss 3.2364, time 176.16ms, mfu 0.25%\n",
            "iter 40: loss 3.0112, time 173.68ms, mfu 0.25%\n",
            "iter 50: loss 2.8482, time 175.86ms, mfu 0.25%\n",
            "iter 60: loss 2.7399, time 175.28ms, mfu 0.25%\n",
            "iter 70: loss 2.6612, time 175.12ms, mfu 0.25%\n",
            "iter 80: loss 2.6130, time 175.22ms, mfu 0.25%\n",
            "iter 90: loss 2.5847, time 173.09ms, mfu 0.25%\n",
            "iter 100: loss 2.5747, time 174.10ms, mfu 0.25%\n",
            "iter 110: loss 2.5445, time 175.06ms, mfu 0.25%\n",
            "iter 120: loss 2.5409, time 174.48ms, mfu 0.25%\n",
            "iter 130: loss 2.5272, time 173.74ms, mfu 0.25%\n",
            "iter 140: loss 2.5192, time 173.95ms, mfu 0.25%\n",
            "iter 150: loss 2.4963, time 174.52ms, mfu 0.25%\n",
            "iter 160: loss 2.4870, time 174.71ms, mfu 0.25%\n",
            "iter 170: loss 2.4743, time 173.40ms, mfu 0.25%\n",
            "iter 180: loss 2.4701, time 174.43ms, mfu 0.25%\n",
            "iter 190: loss 2.4660, time 174.49ms, mfu 0.25%\n",
            "iter 200: loss 2.4605, time 173.75ms, mfu 0.25%\n",
            "iter 210: loss 2.4600, time 173.99ms, mfu 0.26%\n",
            "iter 220: loss 2.4473, time 174.04ms, mfu 0.26%\n",
            "iter 230: loss 2.4525, time 174.37ms, mfu 0.26%\n",
            "iter 240: loss 2.4521, time 173.57ms, mfu 0.26%\n",
            "step 250: train loss 2.3898, val loss 2.4083\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 250: loss 2.4287, time 26944.99ms, mfu 0.23%\n",
            "iter 260: loss 2.4342, time 173.77ms, mfu 0.23%\n",
            "iter 270: loss 2.4131, time 172.99ms, mfu 0.23%\n",
            "iter 280: loss 2.4060, time 173.65ms, mfu 0.24%\n",
            "iter 290: loss 2.3964, time 173.23ms, mfu 0.24%\n",
            "iter 300: loss 2.3779, time 175.22ms, mfu 0.24%\n",
            "iter 310: loss 2.3760, time 173.66ms, mfu 0.24%\n",
            "iter 320: loss 2.3663, time 174.30ms, mfu 0.24%\n",
            "iter 330: loss 2.3579, time 174.86ms, mfu 0.24%\n",
            "iter 340: loss 2.3445, time 175.59ms, mfu 0.25%\n",
            "iter 350: loss 2.3610, time 174.10ms, mfu 0.25%\n",
            "iter 360: loss 2.3351, time 175.98ms, mfu 0.25%\n",
            "iter 370: loss 2.2959, time 174.20ms, mfu 0.25%\n",
            "iter 380: loss 2.3086, time 174.13ms, mfu 0.25%\n",
            "iter 390: loss 2.3157, time 175.28ms, mfu 0.25%\n",
            "iter 400: loss 2.3044, time 174.03ms, mfu 0.25%\n",
            "iter 410: loss 2.2806, time 174.79ms, mfu 0.25%\n",
            "iter 420: loss 2.2841, time 174.07ms, mfu 0.25%\n",
            "iter 430: loss 2.2672, time 175.25ms, mfu 0.25%\n",
            "iter 440: loss 2.2674, time 174.41ms, mfu 0.25%\n",
            "iter 450: loss 2.2642, time 175.48ms, mfu 0.25%\n",
            "iter 460: loss 2.2612, time 175.70ms, mfu 0.25%\n",
            "iter 470: loss 2.2292, time 174.64ms, mfu 0.25%\n",
            "iter 480: loss 2.1952, time 174.38ms, mfu 0.25%\n",
            "iter 490: loss 2.2025, time 173.02ms, mfu 0.25%\n",
            "step 500: train loss 2.1021, val loss 2.1535\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 500: loss 2.1700, time 27004.23ms, mfu 0.23%\n",
            "iter 510: loss 2.1756, time 173.06ms, mfu 0.23%\n",
            "iter 520: loss 2.1664, time 174.49ms, mfu 0.23%\n",
            "iter 530: loss 2.1645, time 175.09ms, mfu 0.24%\n",
            "iter 540: loss 2.1324, time 174.04ms, mfu 0.24%\n",
            "iter 550: loss 2.1337, time 174.51ms, mfu 0.24%\n",
            "iter 560: loss 2.1390, time 173.88ms, mfu 0.24%\n",
            "iter 570: loss 2.1194, time 174.84ms, mfu 0.24%\n",
            "iter 580: loss 2.1026, time 174.94ms, mfu 0.24%\n",
            "iter 590: loss 2.1007, time 175.00ms, mfu 0.24%\n",
            "iter 600: loss 2.0924, time 174.75ms, mfu 0.25%\n",
            "iter 610: loss 2.0911, time 174.53ms, mfu 0.25%\n",
            "iter 620: loss 2.0696, time 174.08ms, mfu 0.25%\n",
            "iter 630: loss 2.0404, time 174.85ms, mfu 0.25%\n",
            "iter 640: loss 2.0329, time 174.29ms, mfu 0.25%\n",
            "iter 650: loss 2.0327, time 174.07ms, mfu 0.25%\n",
            "iter 660: loss 2.0609, time 173.60ms, mfu 0.25%\n",
            "iter 670: loss 2.0178, time 175.29ms, mfu 0.25%\n",
            "iter 680: loss 2.0053, time 175.05ms, mfu 0.25%\n",
            "iter 690: loss 2.0029, time 175.11ms, mfu 0.25%\n",
            "iter 700: loss 2.0018, time 176.81ms, mfu 0.25%\n",
            "iter 710: loss 1.9832, time 174.74ms, mfu 0.25%\n",
            "iter 720: loss 1.9641, time 174.79ms, mfu 0.25%\n",
            "iter 730: loss 1.9827, time 175.04ms, mfu 0.25%\n",
            "iter 740: loss 1.9570, time 175.05ms, mfu 0.25%\n",
            "step 750: train loss 1.8258, val loss 1.9592\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 750: loss 1.9619, time 27016.61ms, mfu 0.23%\n",
            "iter 760: loss 1.9633, time 174.36ms, mfu 0.23%\n",
            "iter 770: loss 1.9269, time 173.49ms, mfu 0.23%\n",
            "iter 780: loss 1.9399, time 174.87ms, mfu 0.23%\n",
            "iter 790: loss 1.9289, time 174.99ms, mfu 0.24%\n",
            "iter 800: loss 1.9069, time 174.59ms, mfu 0.24%\n",
            "iter 810: loss 1.9131, time 172.97ms, mfu 0.24%\n",
            "iter 820: loss 1.9118, time 175.05ms, mfu 0.24%\n",
            "iter 830: loss 1.9049, time 175.24ms, mfu 0.24%\n",
            "iter 840: loss 1.8995, time 175.87ms, mfu 0.24%\n",
            "iter 850: loss 1.9016, time 174.39ms, mfu 0.25%\n",
            "iter 860: loss 1.8946, time 175.21ms, mfu 0.25%\n",
            "iter 870: loss 1.8565, time 174.36ms, mfu 0.25%\n",
            "iter 880: loss 1.8178, time 173.46ms, mfu 0.25%\n",
            "iter 890: loss 1.8595, time 174.99ms, mfu 0.25%\n",
            "iter 900: loss 1.8449, time 174.79ms, mfu 0.25%\n",
            "iter 910: loss 1.8394, time 174.27ms, mfu 0.25%\n",
            "iter 920: loss 1.8311, time 174.40ms, mfu 0.25%\n",
            "iter 930: loss 1.8628, time 173.25ms, mfu 0.25%\n",
            "iter 940: loss 1.8620, time 174.19ms, mfu 0.25%\n",
            "iter 950: loss 1.8304, time 172.88ms, mfu 0.25%\n",
            "iter 960: loss 1.8146, time 174.55ms, mfu 0.25%\n",
            "iter 970: loss 1.7936, time 174.28ms, mfu 0.25%\n",
            "iter 980: loss 1.8236, time 175.21ms, mfu 0.25%\n",
            "iter 990: loss 1.8229, time 173.79ms, mfu 0.25%\n",
            "step 1000: train loss 1.6679, val loss 1.8419\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1000: loss 1.8177, time 26982.45ms, mfu 0.23%\n",
            "iter 1010: loss 1.8204, time 173.97ms, mfu 0.23%\n",
            "iter 1020: loss 1.7914, time 174.45ms, mfu 0.23%\n",
            "iter 1030: loss 1.7545, time 174.86ms, mfu 0.24%\n",
            "iter 1040: loss 1.8216, time 173.87ms, mfu 0.24%\n",
            "iter 1050: loss 1.7922, time 174.74ms, mfu 0.24%\n",
            "iter 1060: loss 1.7803, time 175.28ms, mfu 0.24%\n",
            "iter 1070: loss 1.7878, time 174.88ms, mfu 0.24%\n",
            "iter 1080: loss 1.7591, time 174.92ms, mfu 0.24%\n",
            "iter 1090: loss 1.7719, time 175.30ms, mfu 0.24%\n",
            "iter 1100: loss 1.7664, time 175.41ms, mfu 0.25%\n",
            "iter 1110: loss 1.7416, time 175.84ms, mfu 0.25%\n",
            "iter 1120: loss 1.7428, time 174.65ms, mfu 0.25%\n",
            "iter 1130: loss 1.7718, time 174.09ms, mfu 0.25%\n",
            "iter 1140: loss 1.7314, time 175.88ms, mfu 0.25%\n",
            "iter 1150: loss 1.7466, time 174.09ms, mfu 0.25%\n",
            "iter 1160: loss 1.7647, time 175.79ms, mfu 0.25%\n",
            "iter 1170: loss 1.7115, time 174.59ms, mfu 0.25%\n",
            "iter 1180: loss 1.7258, time 174.04ms, mfu 0.25%\n",
            "iter 1190: loss 1.7305, time 175.81ms, mfu 0.25%\n",
            "iter 1200: loss 1.7148, time 174.34ms, mfu 0.25%\n",
            "iter 1210: loss 1.7385, time 175.50ms, mfu 0.25%\n",
            "iter 1220: loss 1.7247, time 174.60ms, mfu 0.25%\n",
            "iter 1230: loss 1.6989, time 174.58ms, mfu 0.25%\n",
            "iter 1240: loss 1.6898, time 174.81ms, mfu 0.25%\n",
            "step 1250: train loss 1.5752, val loss 1.7665\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1250: loss 1.7107, time 27017.37ms, mfu 0.23%\n",
            "iter 1260: loss 1.7412, time 173.41ms, mfu 0.23%\n",
            "iter 1270: loss 1.7059, time 174.89ms, mfu 0.23%\n",
            "iter 1280: loss 1.6770, time 174.01ms, mfu 0.23%\n",
            "iter 1290: loss 1.7019, time 174.36ms, mfu 0.24%\n",
            "iter 1300: loss 1.7383, time 174.22ms, mfu 0.24%\n",
            "iter 1310: loss 1.6739, time 174.27ms, mfu 0.24%\n",
            "iter 1320: loss 1.6476, time 173.07ms, mfu 0.24%\n",
            "iter 1330: loss 1.6748, time 173.76ms, mfu 0.24%\n",
            "iter 1340: loss 1.6661, time 176.94ms, mfu 0.24%\n",
            "iter 1350: loss 1.6837, time 174.80ms, mfu 0.25%\n",
            "iter 1360: loss 1.7146, time 172.70ms, mfu 0.25%\n",
            "iter 1370: loss 1.6475, time 174.79ms, mfu 0.25%\n",
            "iter 1380: loss 1.6489, time 174.58ms, mfu 0.25%\n",
            "iter 1390: loss 1.7200, time 174.84ms, mfu 0.25%\n",
            "iter 1400: loss 1.6759, time 174.00ms, mfu 0.25%\n",
            "iter 1410: loss 1.6445, time 172.53ms, mfu 0.25%\n",
            "iter 1420: loss 1.6763, time 173.65ms, mfu 0.25%\n",
            "iter 1430: loss 1.6352, time 171.74ms, mfu 0.25%\n",
            "iter 1440: loss 1.6757, time 175.24ms, mfu 0.25%\n",
            "iter 1450: loss 1.6557, time 175.69ms, mfu 0.25%\n",
            "iter 1460: loss 1.6680, time 174.33ms, mfu 0.25%\n",
            "iter 1470: loss 1.6433, time 175.27ms, mfu 0.25%\n",
            "iter 1480: loss 1.6246, time 172.27ms, mfu 0.25%\n",
            "iter 1490: loss 1.6383, time 174.16ms, mfu 0.25%\n",
            "step 1500: train loss 1.5205, val loss 1.7051\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1500: loss 1.6629, time 26978.40ms, mfu 0.23%\n",
            "iter 1510: loss 1.6532, time 175.56ms, mfu 0.23%\n",
            "iter 1520: loss 1.6305, time 174.42ms, mfu 0.23%\n",
            "iter 1530: loss 1.6084, time 173.11ms, mfu 0.24%\n",
            "iter 1540: loss 1.6270, time 173.94ms, mfu 0.24%\n",
            "iter 1550: loss 1.6115, time 174.61ms, mfu 0.24%\n",
            "iter 1560: loss 1.6362, time 175.12ms, mfu 0.24%\n",
            "iter 1570: loss 1.6233, time 175.53ms, mfu 0.24%\n",
            "iter 1580: loss 1.6187, time 174.50ms, mfu 0.24%\n",
            "iter 1590: loss 1.6266, time 174.97ms, mfu 0.24%\n",
            "iter 1600: loss 1.6405, time 175.27ms, mfu 0.25%\n",
            "iter 1610: loss 1.6066, time 173.55ms, mfu 0.25%\n",
            "iter 1620: loss 1.6275, time 174.70ms, mfu 0.25%\n",
            "iter 1630: loss 1.6219, time 176.55ms, mfu 0.25%\n",
            "iter 1640: loss 1.6038, time 174.45ms, mfu 0.25%\n",
            "iter 1650: loss 1.5945, time 173.72ms, mfu 0.25%\n",
            "iter 1660: loss 1.6079, time 174.20ms, mfu 0.25%\n",
            "iter 1670: loss 1.6129, time 175.73ms, mfu 0.25%\n",
            "iter 1680: loss 1.5892, time 174.07ms, mfu 0.25%\n",
            "iter 1690: loss 1.6492, time 173.68ms, mfu 0.25%\n",
            "iter 1700: loss 1.5855, time 173.35ms, mfu 0.25%\n",
            "iter 1710: loss 1.6287, time 174.85ms, mfu 0.25%\n",
            "iter 1720: loss 1.5922, time 173.92ms, mfu 0.25%\n",
            "iter 1730: loss 1.5984, time 175.16ms, mfu 0.25%\n",
            "iter 1740: loss 1.6037, time 174.93ms, mfu 0.25%\n",
            "step 1750: train loss 1.4730, val loss 1.6767\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 1750: loss 1.6118, time 27049.58ms, mfu 0.23%\n",
            "iter 1760: loss 1.5762, time 174.63ms, mfu 0.23%\n",
            "iter 1770: loss 1.6142, time 174.66ms, mfu 0.23%\n",
            "iter 1780: loss 1.5895, time 174.84ms, mfu 0.23%\n",
            "iter 1790: loss 1.5950, time 174.55ms, mfu 0.24%\n",
            "iter 1800: loss 1.6121, time 174.39ms, mfu 0.24%\n",
            "iter 1810: loss 1.5813, time 174.26ms, mfu 0.24%\n",
            "iter 1820: loss 1.6022, time 175.32ms, mfu 0.24%\n",
            "iter 1830: loss 1.5937, time 173.73ms, mfu 0.24%\n",
            "iter 1840: loss 1.5857, time 174.79ms, mfu 0.24%\n",
            "iter 1850: loss 1.5684, time 176.10ms, mfu 0.25%\n",
            "iter 1860: loss 1.5701, time 174.55ms, mfu 0.25%\n",
            "iter 1870: loss 1.6016, time 174.28ms, mfu 0.25%\n",
            "iter 1880: loss 1.5730, time 173.89ms, mfu 0.25%\n",
            "iter 1890: loss 1.5726, time 174.20ms, mfu 0.25%\n",
            "iter 1900: loss 1.5861, time 174.75ms, mfu 0.25%\n",
            "iter 1910: loss 1.5829, time 174.69ms, mfu 0.25%\n",
            "iter 1920: loss 1.5935, time 174.35ms, mfu 0.25%\n",
            "iter 1930: loss 1.5601, time 173.28ms, mfu 0.25%\n",
            "iter 1940: loss 1.5205, time 175.16ms, mfu 0.25%\n",
            "iter 1950: loss 1.5751, time 175.08ms, mfu 0.25%\n",
            "iter 1960: loss 1.5921, time 174.72ms, mfu 0.25%\n",
            "iter 1970: loss 1.5558, time 174.27ms, mfu 0.25%\n",
            "iter 1980: loss 1.5879, time 175.31ms, mfu 0.25%\n",
            "iter 1990: loss 1.5330, time 175.41ms, mfu 0.25%\n",
            "step 2000: train loss 1.4486, val loss 1.6545\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2000: loss 1.5554, time 27120.13ms, mfu 0.23%\n",
            "iter 2010: loss 1.5507, time 173.85ms, mfu 0.23%\n",
            "iter 2020: loss 1.5521, time 173.29ms, mfu 0.23%\n",
            "iter 2030: loss 1.5461, time 174.01ms, mfu 0.24%\n",
            "iter 2040: loss 1.5537, time 176.31ms, mfu 0.24%\n",
            "iter 2050: loss 1.5157, time 173.56ms, mfu 0.24%\n",
            "iter 2060: loss 1.6047, time 174.11ms, mfu 0.24%\n",
            "iter 2070: loss 1.5486, time 174.10ms, mfu 0.24%\n",
            "iter 2080: loss 1.5748, time 174.79ms, mfu 0.24%\n",
            "iter 2090: loss 1.5264, time 175.12ms, mfu 0.24%\n",
            "iter 2100: loss 1.5668, time 174.19ms, mfu 0.25%\n",
            "iter 2110: loss 1.5364, time 174.72ms, mfu 0.25%\n",
            "iter 2120: loss 1.5551, time 174.51ms, mfu 0.25%\n",
            "iter 2130: loss 1.5697, time 175.19ms, mfu 0.25%\n",
            "iter 2140: loss 1.5266, time 176.04ms, mfu 0.25%\n",
            "iter 2150: loss 1.5658, time 174.16ms, mfu 0.25%\n",
            "iter 2160: loss 1.5191, time 173.13ms, mfu 0.25%\n",
            "iter 2170: loss 1.5297, time 174.09ms, mfu 0.25%\n",
            "iter 2180: loss 1.5537, time 174.28ms, mfu 0.25%\n",
            "iter 2190: loss 1.5347, time 175.55ms, mfu 0.25%\n",
            "iter 2200: loss 1.5390, time 174.85ms, mfu 0.25%\n",
            "iter 2210: loss 1.5175, time 174.27ms, mfu 0.25%\n",
            "iter 2220: loss 1.5384, time 173.57ms, mfu 0.25%\n",
            "iter 2230: loss 1.5609, time 174.56ms, mfu 0.25%\n",
            "iter 2240: loss 1.5485, time 174.63ms, mfu 0.25%\n",
            "step 2250: train loss 1.4248, val loss 1.6188\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2250: loss 1.5068, time 27027.33ms, mfu 0.23%\n",
            "iter 2260: loss 1.5179, time 174.46ms, mfu 0.23%\n",
            "iter 2270: loss 1.5427, time 174.30ms, mfu 0.23%\n",
            "iter 2280: loss 1.5380, time 174.45ms, mfu 0.24%\n",
            "iter 2290: loss 1.5289, time 173.13ms, mfu 0.24%\n",
            "iter 2300: loss 1.5110, time 176.04ms, mfu 0.24%\n",
            "iter 2310: loss 1.5457, time 173.42ms, mfu 0.24%\n",
            "iter 2320: loss 1.5043, time 174.54ms, mfu 0.24%\n",
            "iter 2330: loss 1.5409, time 175.33ms, mfu 0.24%\n",
            "iter 2340: loss 1.5035, time 175.00ms, mfu 0.24%\n",
            "iter 2350: loss 1.5331, time 173.93ms, mfu 0.25%\n",
            "iter 2360: loss 1.5553, time 174.25ms, mfu 0.25%\n",
            "iter 2370: loss 1.5473, time 173.20ms, mfu 0.25%\n",
            "iter 2380: loss 1.5502, time 174.12ms, mfu 0.25%\n",
            "iter 2390: loss 1.5205, time 174.55ms, mfu 0.25%\n",
            "iter 2400: loss 1.5577, time 172.66ms, mfu 0.25%\n",
            "iter 2410: loss 1.5084, time 172.95ms, mfu 0.25%\n",
            "iter 2420: loss 1.5354, time 174.27ms, mfu 0.25%\n",
            "iter 2430: loss 1.5496, time 174.08ms, mfu 0.25%\n",
            "iter 2440: loss 1.5102, time 173.70ms, mfu 0.25%\n",
            "iter 2450: loss 1.5166, time 173.92ms, mfu 0.25%\n",
            "iter 2460: loss 1.5498, time 173.57ms, mfu 0.25%\n",
            "iter 2470: loss 1.5223, time 174.19ms, mfu 0.25%\n",
            "iter 2480: loss 1.4974, time 173.45ms, mfu 0.25%\n",
            "iter 2490: loss 1.5447, time 174.16ms, mfu 0.25%\n",
            "step 2500: train loss 1.4064, val loss 1.6097\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2500: loss 1.5406, time 26994.19ms, mfu 0.23%\n",
            "iter 2510: loss 1.5054, time 175.40ms, mfu 0.23%\n",
            "iter 2520: loss 1.4914, time 174.67ms, mfu 0.23%\n",
            "iter 2530: loss 1.5216, time 175.36ms, mfu 0.24%\n",
            "iter 2540: loss 1.5079, time 174.85ms, mfu 0.24%\n",
            "iter 2550: loss 1.5182, time 175.14ms, mfu 0.24%\n",
            "iter 2560: loss 1.5062, time 175.83ms, mfu 0.24%\n",
            "iter 2570: loss 1.5387, time 174.59ms, mfu 0.24%\n",
            "iter 2580: loss 1.4886, time 175.92ms, mfu 0.24%\n",
            "iter 2590: loss 1.5061, time 176.10ms, mfu 0.24%\n",
            "iter 2600: loss 1.5136, time 175.03ms, mfu 0.24%\n",
            "iter 2610: loss 1.5116, time 174.52ms, mfu 0.25%\n",
            "iter 2620: loss 1.5136, time 175.89ms, mfu 0.25%\n",
            "iter 2630: loss 1.5259, time 174.67ms, mfu 0.25%\n",
            "iter 2640: loss 1.5208, time 175.73ms, mfu 0.25%\n",
            "iter 2650: loss 1.4850, time 173.94ms, mfu 0.25%\n",
            "iter 2660: loss 1.5380, time 175.00ms, mfu 0.25%\n",
            "iter 2670: loss 1.4908, time 174.87ms, mfu 0.25%\n",
            "iter 2680: loss 1.5185, time 175.15ms, mfu 0.25%\n",
            "iter 2690: loss 1.5072, time 175.42ms, mfu 0.25%\n",
            "iter 2700: loss 1.5464, time 174.42ms, mfu 0.25%\n",
            "iter 2710: loss 1.5565, time 172.10ms, mfu 0.25%\n",
            "iter 2720: loss 1.5021, time 176.16ms, mfu 0.25%\n",
            "iter 2730: loss 1.5167, time 173.03ms, mfu 0.25%\n",
            "iter 2740: loss 1.5228, time 173.42ms, mfu 0.25%\n",
            "step 2750: train loss 1.4006, val loss 1.6063\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 2750: loss 1.5016, time 27012.53ms, mfu 0.23%\n",
            "iter 2760: loss 1.5044, time 173.43ms, mfu 0.23%\n",
            "iter 2770: loss 1.5231, time 176.49ms, mfu 0.23%\n",
            "iter 2780: loss 1.5236, time 173.96ms, mfu 0.24%\n",
            "iter 2790: loss 1.5281, time 175.53ms, mfu 0.24%\n",
            "iter 2800: loss 1.5040, time 173.90ms, mfu 0.24%\n",
            "iter 2810: loss 1.5089, time 175.59ms, mfu 0.24%\n",
            "iter 2820: loss 1.4993, time 175.32ms, mfu 0.24%\n",
            "iter 2830: loss 1.5331, time 173.71ms, mfu 0.24%\n",
            "iter 2840: loss 1.5046, time 174.81ms, mfu 0.24%\n",
            "iter 2850: loss 1.5281, time 172.96ms, mfu 0.25%\n",
            "iter 2860: loss 1.5483, time 173.11ms, mfu 0.25%\n",
            "iter 2870: loss 1.5157, time 175.84ms, mfu 0.25%\n",
            "iter 2880: loss 1.4769, time 175.46ms, mfu 0.25%\n",
            "iter 2890: loss 1.4952, time 173.63ms, mfu 0.25%\n",
            "iter 2900: loss 1.5239, time 175.03ms, mfu 0.25%\n",
            "iter 2910: loss 1.5248, time 175.19ms, mfu 0.25%\n",
            "iter 2920: loss 1.4951, time 173.76ms, mfu 0.25%\n",
            "iter 2930: loss 1.5078, time 175.02ms, mfu 0.25%\n",
            "iter 2940: loss 1.5069, time 175.03ms, mfu 0.25%\n",
            "iter 2950: loss 1.5169, time 173.04ms, mfu 0.25%\n",
            "iter 2960: loss 1.5302, time 174.41ms, mfu 0.25%\n",
            "iter 2970: loss 1.5159, time 174.70ms, mfu 0.25%\n",
            "iter 2980: loss 1.4993, time 174.74ms, mfu 0.25%\n",
            "iter 2990: loss 1.4771, time 174.00ms, mfu 0.25%\n",
            "step 3000: train loss 1.3894, val loss 1.5973\n",
            "saving checkpoint to out-shakespeare-char\n",
            "iter 3000: loss 1.5064, time 27007.23ms, mfu 0.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "output of performance comparison"
      ],
      "metadata": {
        "id": "DbOAABnPuHtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('figures')"
      ],
      "metadata": {
        "id": "GbCZODG5j4Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "heads = [2, 3, 5, 7]\n",
        "losses = [1.5507, 1.5529, 1.5552, 1.5064]\n",
        "plt.plot(heads, losses, marker='o')\n",
        "plt.xlabel('Number of Heads')\n",
        "plt.ylabel('Loss at Iteration 3000')\n",
        "\n",
        "plt.grid()\n",
        "plt.savefig('figures/loss_changes.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "UXMxjm4GZ-Wr",
        "outputId": "2448b6d2-dab7-4aa2-90a4-3542bf02fd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training BabyGPT for Code Generation"
      ],
      "metadata": {
        "id": "o5oDFAKSuPH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create new dataset for data/code generation\n",
        "!cp -r data/shakespeare_char data/code_generation\n",
        "!wget https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/run.c -O data/code_generation/input.txt\n",
        "!wget -O - https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/runq.c >> data/code_generation/input.txt\n",
        "!wget -O - https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/test.c >> data/code_generation/input.txt\n",
        "!wget -O - https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/win.c >> data/code_generation/input.txt\n",
        "!wget -O - https://raw.githubusercontent.com/karpathy/llm.c/refs/heads/master/test_gpt2.c >> data/code_generation/input.txt\n",
        "!wget -O - https://raw.githubusercontent.com/karpathy/llm.c/refs/heads/master/train_gpt2.c >> data/code_generation/input.txt"
      ],
      "metadata": {
        "id": "hox22ROhXpER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e888042-762f-4f11-ef50-d329707e1461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-01 06:59:29--  https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/run.c\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 38545 (38K) [text/plain]\n",
            "Saving to: ‘data/code_generation/input.txt’\n",
            "\n",
            "data/code_generatio 100%[===================>]  37.64K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2025-05-01 06:59:29 (6.76 MB/s) - ‘data/code_generation/input.txt’ saved [38545/38545]\n",
            "\n",
            "--2025-05-01 06:59:29--  https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/runq.c\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 43357 (42K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]  42.34K  --.-KB/s    in 0.008s  \n",
            "\n",
            "2025-05-01 06:59:30 (5.42 MB/s) - written to stdout [43357/43357]\n",
            "\n",
            "--2025-05-01 06:59:30--  https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/test.c\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3574 (3.5K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   3.49K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-01 06:59:30 (44.3 MB/s) - written to stdout [3574/3574]\n",
            "\n",
            "--2025-05-01 06:59:30--  https://raw.githubusercontent.com/karpathy/llama2.c/refs/heads/master/win.c\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4269 (4.2K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   4.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-01 06:59:30 (48.7 MB/s) - written to stdout [4269/4269]\n",
            "\n",
            "--2025-05-01 06:59:30--  https://raw.githubusercontent.com/karpathy/llm.c/refs/heads/master/test_gpt2.c\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8030 (7.8K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   7.84K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-05-01 06:59:31 (74.6 MB/s) - written to stdout [8030/8030]\n",
            "\n",
            "--2025-05-01 06:59:31--  https://raw.githubusercontent.com/karpathy/llm.c/refs/heads/master/train_gpt2.c\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 50680 (49K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]  49.49K  --.-KB/s    in 0.007s  \n",
            "\n",
            "2025-05-01 06:59:31 (6.66 MB/s) - written to stdout [50680/50680]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python data/code_generation/prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oiYSmRLlsg3",
        "outputId": "877ce80c-7c9b-4852-ee8c-01e385a6a8ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters: 148,266\n",
            "all the unique characters: \t\n",
            " !\"#%&'()*+,-./0123456789:;<=>?@ABCDEFGHIKLMNOPQRSTUVWXY[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~éσ↔\n",
            "vocab size: 97\n",
            "train has 133,439 tokens\n",
            "val has 14,827 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create new config file\n",
        "!cp -r config/train_shakespeare_char.py config/train_code_generation.py\n",
        "!sed -i \"s/out_dir = 'out-shakespeare-char'/out_dir = 'out-code-generation'/\" config/train_code_generation.py\n",
        "!sed -i \"s/dataset = 'shakespeare_char'/dataset = 'code_generation'/\" config/train_code_generation.py"
      ],
      "metadata": {
        "id": "nneNCT8PoNfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training data/code generation model\n",
        "!python train.py config/train_code_generation.py --compile=False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ho9XIFIMu4ij",
        "outputId": "fd9c7526-302f-4e47-9bbd-d13ea46aa74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding config with config/train_code_generation.py:\n",
            "# train a miniature character-level shakespeare model\n",
            "# good for debugging and playing on macbooks and such\n",
            "\n",
            "out_dir = 'out-code-generation'\n",
            "eval_interval = 250 # keep frequent because we'll overfit\n",
            "eval_iters = 200\n",
            "log_interval = 10 # don't print too too often\n",
            "\n",
            "# we expect to overfit on this small dataset, so only save when val improves\n",
            "always_save_checkpoint = False\n",
            "\n",
            "wandb_log = False # override via command line if you like\n",
            "wandb_project = 'shakespeare-char'\n",
            "wandb_run_name = 'mini-gpt'\n",
            "\n",
            "dataset = 'code_generation'\n",
            "gradient_accumulation_steps = 1\n",
            "batch_size = 64\n",
            "block_size = 256 # context of up to 256 previous characters\n",
            "\n",
            "# baby GPT model :)\n",
            "n_layer = 6\n",
            "n_head = 6\n",
            "n_embd = 384\n",
            "dropout = 0.2\n",
            "\n",
            "learning_rate = 1e-3 # with baby networks can afford to go a bit higher\n",
            "max_iters = 5000\n",
            "lr_decay_iters = 5000 # make equal to max_iters usually\n",
            "min_lr = 1e-4 # learning_rate / 10 usually\n",
            "beta2 = 0.99 # make a bit bigger because number of tokens per iter is small\n",
            "\n",
            "warmup_iters = 100 # not super necessary potentially\n",
            "\n",
            "# on macbook also add\n",
            "# device = 'cpu'  # run on cpu only\n",
            "# compile = False # do not torch compile the model\n",
            "\n",
            "Overriding: compile = False\n",
            "tokens per iteration will be: 16,384\n",
            "found vocab_size = 97 (inside data/code_generation/meta.pkl)\n",
            "Initializing a new model from scratch\n",
            "number of parameters: 10.66M\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 26, with 10,752,384 parameters\n",
            "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.4038, val loss 4.4278\n",
            "iter 0: loss 4.4493, time 74544.03ms, mfu -100.00%\n",
            "iter 10: loss 3.3198, time 524.50ms, mfu 0.71%\n",
            "iter 20: loss 2.8847, time 523.52ms, mfu 0.71%\n",
            "iter 30: loss 2.6672, time 519.00ms, mfu 0.71%\n",
            "iter 40: loss 2.5660, time 520.41ms, mfu 0.71%\n",
            "iter 50: loss 2.4890, time 521.36ms, mfu 0.71%\n",
            "iter 60: loss 2.4030, time 524.70ms, mfu 0.71%\n",
            "iter 70: loss 2.3613, time 528.23ms, mfu 0.71%\n",
            "iter 80: loss 2.2975, time 525.56ms, mfu 0.71%\n",
            "iter 90: loss 2.2340, time 527.03ms, mfu 0.71%\n",
            "iter 100: loss 2.2559, time 532.96ms, mfu 0.71%\n",
            "iter 110: loss 2.0868, time 530.92ms, mfu 0.71%\n",
            "iter 120: loss 1.9945, time 531.22ms, mfu 0.71%\n",
            "iter 130: loss 2.0731, time 531.82ms, mfu 0.71%\n",
            "iter 140: loss 1.9440, time 529.52ms, mfu 0.71%\n",
            "iter 150: loss 1.8571, time 529.26ms, mfu 0.71%\n",
            "iter 160: loss 1.7935, time 527.67ms, mfu 0.71%\n",
            "iter 170: loss 1.7373, time 526.55ms, mfu 0.71%\n",
            "iter 180: loss 1.6803, time 524.75ms, mfu 0.71%\n",
            "iter 190: loss 1.6226, time 528.04ms, mfu 0.71%\n",
            "iter 200: loss 1.5621, time 525.23ms, mfu 0.71%\n",
            "iter 210: loss 1.4291, time 525.97ms, mfu 0.71%\n",
            "iter 220: loss 1.4907, time 528.36ms, mfu 0.71%\n",
            "iter 230: loss 1.4257, time 529.98ms, mfu 0.71%\n",
            "iter 240: loss 1.3308, time 529.26ms, mfu 0.71%\n",
            "step 250: train loss 1.1061, val loss 1.5213\n",
            "saving checkpoint to out-code-generation\n",
            "iter 250: loss 1.3787, time 74709.30ms, mfu 0.64%\n",
            "iter 260: loss 1.2117, time 524.16ms, mfu 0.64%\n",
            "iter 270: loss 1.1611, time 529.62ms, mfu 0.65%\n",
            "iter 280: loss 1.1574, time 527.38ms, mfu 0.66%\n",
            "iter 290: loss 1.1216, time 528.69ms, mfu 0.66%\n",
            "iter 300: loss 1.0738, time 528.34ms, mfu 0.67%\n",
            "iter 310: loss 0.9852, time 530.35ms, mfu 0.67%\n",
            "iter 320: loss 0.8864, time 531.33ms, mfu 0.67%\n",
            "iter 330: loss 0.9531, time 529.12ms, mfu 0.68%\n",
            "iter 340: loss 0.8544, time 530.68ms, mfu 0.68%\n",
            "iter 350: loss 0.8412, time 527.32ms, mfu 0.68%\n",
            "iter 360: loss 0.7891, time 530.89ms, mfu 0.68%\n",
            "iter 370: loss 0.8199, time 527.16ms, mfu 0.69%\n",
            "iter 380: loss 0.6796, time 528.31ms, mfu 0.69%\n",
            "iter 390: loss 0.7871, time 527.47ms, mfu 0.69%\n",
            "iter 400: loss 0.7395, time 529.78ms, mfu 0.69%\n",
            "iter 410: loss 0.6623, time 528.96ms, mfu 0.69%\n",
            "iter 420: loss 0.6615, time 529.79ms, mfu 0.69%\n",
            "iter 430: loss 0.6651, time 526.05ms, mfu 0.70%\n",
            "iter 440: loss 0.6383, time 527.57ms, mfu 0.70%\n",
            "iter 450: loss 0.5606, time 527.98ms, mfu 0.70%\n",
            "iter 460: loss 0.5612, time 527.59ms, mfu 0.70%\n",
            "iter 470: loss 0.5686, time 527.35ms, mfu 0.70%\n",
            "iter 480: loss 0.5770, time 525.65ms, mfu 0.70%\n",
            "iter 490: loss 0.5302, time 526.07ms, mfu 0.70%\n",
            "step 500: train loss 0.3387, val loss 1.2349\n",
            "saving checkpoint to out-code-generation\n",
            "iter 500: loss 0.5415, time 74942.48ms, mfu 0.63%\n",
            "iter 510: loss 0.4940, time 525.88ms, mfu 0.64%\n",
            "iter 520: loss 0.4997, time 529.61ms, mfu 0.65%\n",
            "iter 530: loss 0.4417, time 527.70ms, mfu 0.65%\n",
            "iter 540: loss 0.4490, time 528.48ms, mfu 0.66%\n",
            "iter 550: loss 0.3999, time 529.13ms, mfu 0.66%\n",
            "iter 560: loss 0.4198, time 526.09ms, mfu 0.67%\n",
            "iter 570: loss 0.4101, time 524.49ms, mfu 0.67%\n",
            "iter 580: loss 0.4172, time 525.57ms, mfu 0.68%\n",
            "iter 590: loss 0.3893, time 528.38ms, mfu 0.68%\n",
            "iter 600: loss 0.3613, time 528.06ms, mfu 0.68%\n",
            "iter 610: loss 0.3835, time 527.98ms, mfu 0.68%\n",
            "iter 620: loss 0.3341, time 529.98ms, mfu 0.69%\n",
            "iter 630: loss 0.3182, time 527.58ms, mfu 0.69%\n",
            "iter 640: loss 0.3365, time 524.50ms, mfu 0.69%\n",
            "iter 650: loss 0.3310, time 522.64ms, mfu 0.69%\n",
            "iter 660: loss 0.3012, time 525.26ms, mfu 0.69%\n",
            "iter 670: loss 0.3345, time 527.54ms, mfu 0.70%\n",
            "iter 680: loss 0.3333, time 528.76ms, mfu 0.70%\n",
            "iter 690: loss 0.3083, time 526.46ms, mfu 0.70%\n",
            "iter 700: loss 0.3073, time 525.55ms, mfu 0.70%\n",
            "iter 710: loss 0.3001, time 528.33ms, mfu 0.70%\n",
            "iter 720: loss 0.2968, time 527.13ms, mfu 0.70%\n",
            "iter 730: loss 0.2684, time 524.77ms, mfu 0.70%\n",
            "iter 740: loss 0.2682, time 526.20ms, mfu 0.70%\n",
            "step 750: train loss 0.1645, val loss 1.3708\n",
            "iter 750: loss 0.2755, time 74564.95ms, mfu 0.63%\n",
            "iter 760: loss 0.2773, time 529.38ms, mfu 0.64%\n",
            "iter 770: loss 0.2659, time 530.03ms, mfu 0.65%\n",
            "iter 780: loss 0.2595, time 527.21ms, mfu 0.65%\n",
            "iter 790: loss 0.2605, time 529.42ms, mfu 0.66%\n",
            "iter 800: loss 0.2430, time 531.21ms, mfu 0.66%\n",
            "iter 810: loss 0.2367, time 534.26ms, mfu 0.67%\n",
            "iter 820: loss 0.2282, time 531.50ms, mfu 0.67%\n",
            "iter 830: loss 0.2536, time 527.32ms, mfu 0.67%\n",
            "iter 840: loss 0.2287, time 529.50ms, mfu 0.68%\n",
            "iter 850: loss 0.2189, time 529.38ms, mfu 0.68%\n",
            "iter 860: loss 0.2268, time 527.91ms, mfu 0.68%\n",
            "iter 870: loss 0.2173, time 525.25ms, mfu 0.68%\n",
            "iter 880: loss 0.2303, time 524.66ms, mfu 0.69%\n",
            "iter 890: loss 0.2161, time 526.88ms, mfu 0.69%\n",
            "iter 900: loss 0.2159, time 524.85ms, mfu 0.69%\n",
            "iter 910: loss 0.2360, time 527.61ms, mfu 0.69%\n",
            "iter 920: loss 0.2229, time 524.66ms, mfu 0.69%\n",
            "iter 930: loss 0.1974, time 527.56ms, mfu 0.70%\n",
            "iter 940: loss 0.2152, time 525.92ms, mfu 0.70%\n",
            "iter 950: loss 0.2406, time 530.83ms, mfu 0.70%\n",
            "iter 960: loss 0.2138, time 528.94ms, mfu 0.70%\n",
            "iter 970: loss 0.2117, time 526.14ms, mfu 0.70%\n",
            "iter 980: loss 0.2162, time 524.93ms, mfu 0.70%\n",
            "iter 990: loss 0.1918, time 526.23ms, mfu 0.70%\n",
            "step 1000: train loss 0.1210, val loss 1.4176\n",
            "iter 1000: loss 0.2155, time 74459.88ms, mfu 0.63%\n",
            "iter 1010: loss 0.1977, time 528.97ms, mfu 0.64%\n",
            "iter 1020: loss 0.2066, time 530.12ms, mfu 0.65%\n",
            "iter 1030: loss 0.2017, time 532.37ms, mfu 0.65%\n",
            "iter 1040: loss 0.1795, time 530.29ms, mfu 0.66%\n",
            "iter 1050: loss 0.1855, time 531.29ms, mfu 0.66%\n",
            "iter 1060: loss 0.2055, time 530.36ms, mfu 0.67%\n",
            "iter 1070: loss 0.1814, time 529.79ms, mfu 0.67%\n",
            "iter 1080: loss 0.1750, time 528.29ms, mfu 0.67%\n",
            "iter 1090: loss 0.1808, time 529.40ms, mfu 0.68%\n",
            "iter 1100: loss 0.1864, time 526.50ms, mfu 0.68%\n",
            "iter 1110: loss 0.1693, time 526.17ms, mfu 0.68%\n",
            "iter 1120: loss 0.1705, time 525.77ms, mfu 0.68%\n",
            "iter 1130: loss 0.1772, time 528.81ms, mfu 0.69%\n",
            "iter 1140: loss 0.1718, time 528.01ms, mfu 0.69%\n",
            "iter 1150: loss 0.1666, time 527.08ms, mfu 0.69%\n",
            "iter 1160: loss 0.1650, time 529.50ms, mfu 0.69%\n",
            "iter 1170: loss 0.1743, time 528.38ms, mfu 0.69%\n",
            "iter 1180: loss 0.1835, time 522.33ms, mfu 0.70%\n",
            "iter 1190: loss 0.1650, time 524.51ms, mfu 0.70%\n",
            "iter 1200: loss 0.1802, time 528.46ms, mfu 0.70%\n",
            "iter 1210: loss 0.1677, time 530.84ms, mfu 0.70%\n",
            "iter 1220: loss 0.1694, time 530.70ms, mfu 0.70%\n",
            "iter 1230: loss 0.1593, time 529.24ms, mfu 0.70%\n",
            "iter 1240: loss 0.1684, time 528.09ms, mfu 0.70%\n",
            "step 1250: train loss 0.0987, val loss 1.5499\n",
            "iter 1250: loss 0.1688, time 74581.01ms, mfu 0.63%\n",
            "iter 1260: loss 0.1505, time 527.36ms, mfu 0.64%\n",
            "iter 1270: loss 0.1694, time 528.52ms, mfu 0.65%\n",
            "iter 1280: loss 0.1663, time 524.92ms, mfu 0.65%\n",
            "iter 1290: loss 0.1620, time 525.99ms, mfu 0.66%\n",
            "iter 1300: loss 0.1476, time 528.12ms, mfu 0.66%\n",
            "iter 1310: loss 0.1523, time 528.95ms, mfu 0.67%\n",
            "iter 1320: loss 0.1464, time 527.20ms, mfu 0.67%\n",
            "iter 1330: loss 0.1533, time 528.59ms, mfu 0.67%\n",
            "iter 1340: loss 0.1608, time 527.47ms, mfu 0.68%\n",
            "iter 1350: loss 0.1443, time 526.45ms, mfu 0.68%\n",
            "iter 1360: loss 0.1622, time 527.80ms, mfu 0.68%\n",
            "iter 1370: loss 0.1374, time 529.59ms, mfu 0.69%\n",
            "iter 1380: loss 0.1539, time 526.61ms, mfu 0.69%\n",
            "iter 1390: loss 0.1406, time 528.95ms, mfu 0.69%\n",
            "iter 1400: loss 0.1427, time 525.36ms, mfu 0.69%\n",
            "iter 1410: loss 0.1478, time 527.15ms, mfu 0.69%\n",
            "iter 1420: loss 0.1484, time 527.69ms, mfu 0.69%\n",
            "iter 1430: loss 0.1502, time 526.15ms, mfu 0.70%\n",
            "iter 1440: loss 0.1429, time 530.13ms, mfu 0.70%\n",
            "iter 1450: loss 0.1329, time 525.97ms, mfu 0.70%\n",
            "iter 1460: loss 0.1404, time 527.60ms, mfu 0.70%\n",
            "iter 1470: loss 0.1517, time 529.41ms, mfu 0.70%\n",
            "iter 1480: loss 0.1562, time 529.75ms, mfu 0.70%\n",
            "iter 1490: loss 0.1414, time 529.46ms, mfu 0.70%\n",
            "step 1500: train loss 0.0828, val loss 1.5616\n",
            "iter 1500: loss 0.1285, time 74563.06ms, mfu 0.63%\n",
            "iter 1510: loss 0.1504, time 525.59ms, mfu 0.64%\n",
            "iter 1520: loss 0.1420, time 526.47ms, mfu 0.65%\n",
            "iter 1530: loss 0.1409, time 529.41ms, mfu 0.65%\n",
            "iter 1540: loss 0.1282, time 533.51ms, mfu 0.66%\n",
            "iter 1550: loss 0.1199, time 524.55ms, mfu 0.66%\n",
            "iter 1560: loss 0.1337, time 529.96ms, mfu 0.67%\n",
            "iter 1570: loss 0.1449, time 527.37ms, mfu 0.67%\n",
            "iter 1580: loss 0.1310, time 528.30ms, mfu 0.67%\n",
            "iter 1590: loss 0.1307, time 528.64ms, mfu 0.68%\n",
            "iter 1600: loss 0.1225, time 529.64ms, mfu 0.68%\n",
            "iter 1610: loss 0.1311, time 529.91ms, mfu 0.68%\n",
            "iter 1620: loss 0.1217, time 532.19ms, mfu 0.68%\n",
            "iter 1630: loss 0.1321, time 527.49ms, mfu 0.69%\n",
            "iter 1640: loss 0.1286, time 530.05ms, mfu 0.69%\n",
            "iter 1650: loss 0.1351, time 530.53ms, mfu 0.69%\n",
            "iter 1660: loss 0.1093, time 526.18ms, mfu 0.69%\n",
            "iter 1670: loss 0.1201, time 528.47ms, mfu 0.69%\n",
            "iter 1680: loss 0.1216, time 531.70ms, mfu 0.69%\n",
            "iter 1690: loss 0.1259, time 528.81ms, mfu 0.69%\n",
            "iter 1700: loss 0.1184, time 526.55ms, mfu 0.70%\n",
            "iter 1710: loss 0.1102, time 528.72ms, mfu 0.70%\n",
            "iter 1720: loss 0.1298, time 530.77ms, mfu 0.70%\n",
            "iter 1730: loss 0.1090, time 529.73ms, mfu 0.70%\n",
            "iter 1740: loss 0.1211, time 532.04ms, mfu 0.70%\n",
            "step 1750: train loss 0.0745, val loss 1.6495\n",
            "iter 1750: loss 0.1167, time 74623.06ms, mfu 0.63%\n",
            "iter 1760: loss 0.1239, time 528.07ms, mfu 0.64%\n",
            "iter 1770: loss 0.1110, time 527.29ms, mfu 0.64%\n",
            "iter 1780: loss 0.1137, time 525.94ms, mfu 0.65%\n",
            "iter 1790: loss 0.1059, time 524.57ms, mfu 0.66%\n",
            "iter 1800: loss 0.1143, time 531.00ms, mfu 0.66%\n",
            "iter 1810: loss 0.1146, time 527.50ms, mfu 0.67%\n",
            "iter 1820: loss 0.1167, time 526.97ms, mfu 0.67%\n",
            "iter 1830: loss 0.1097, time 525.17ms, mfu 0.67%\n",
            "iter 1840: loss 0.1080, time 526.05ms, mfu 0.68%\n",
            "iter 1850: loss 0.1140, time 528.67ms, mfu 0.68%\n",
            "iter 1860: loss 0.1145, time 526.18ms, mfu 0.68%\n",
            "iter 1870: loss 0.1090, time 529.90ms, mfu 0.69%\n",
            "iter 1880: loss 0.1048, time 530.40ms, mfu 0.69%\n",
            "iter 1890: loss 0.1119, time 527.05ms, mfu 0.69%\n",
            "iter 1900: loss 0.1092, time 528.92ms, mfu 0.69%\n",
            "iter 1910: loss 0.1137, time 526.27ms, mfu 0.69%\n",
            "iter 1920: loss 0.1017, time 527.04ms, mfu 0.69%\n",
            "iter 1930: loss 0.1071, time 528.75ms, mfu 0.70%\n",
            "iter 1940: loss 0.1131, time 525.27ms, mfu 0.70%\n",
            "iter 1950: loss 0.1137, time 526.58ms, mfu 0.70%\n",
            "iter 1960: loss 0.1041, time 524.49ms, mfu 0.70%\n",
            "iter 1970: loss 0.1119, time 526.08ms, mfu 0.70%\n",
            "iter 1980: loss 0.1033, time 527.31ms, mfu 0.70%\n",
            "iter 1990: loss 0.1010, time 529.60ms, mfu 0.70%\n",
            "step 2000: train loss 0.0689, val loss 1.6853\n",
            "iter 2000: loss 0.1088, time 74420.44ms, mfu 0.63%\n",
            "iter 2010: loss 0.0971, time 525.23ms, mfu 0.64%\n",
            "iter 2020: loss 0.0989, time 526.09ms, mfu 0.65%\n",
            "iter 2030: loss 0.0979, time 528.68ms, mfu 0.65%\n",
            "iter 2040: loss 0.1069, time 526.52ms, mfu 0.66%\n",
            "iter 2050: loss 0.1049, time 525.33ms, mfu 0.66%\n",
            "iter 2060: loss 0.0992, time 526.27ms, mfu 0.67%\n",
            "iter 2070: loss 0.1084, time 526.71ms, mfu 0.67%\n",
            "iter 2080: loss 0.0979, time 529.19ms, mfu 0.68%\n",
            "iter 2090: loss 0.1051, time 527.79ms, mfu 0.68%\n",
            "iter 2100: loss 0.0962, time 527.70ms, mfu 0.68%\n",
            "iter 2110: loss 0.1076, time 529.99ms, mfu 0.68%\n",
            "iter 2120: loss 0.0959, time 527.98ms, mfu 0.69%\n",
            "iter 2130: loss 0.0942, time 529.42ms, mfu 0.69%\n",
            "iter 2140: loss 0.0987, time 527.31ms, mfu 0.69%\n",
            "iter 2150: loss 0.0975, time 528.07ms, mfu 0.69%\n",
            "iter 2160: loss 0.0917, time 527.80ms, mfu 0.69%\n",
            "iter 2170: loss 0.1011, time 526.40ms, mfu 0.69%\n",
            "iter 2180: loss 0.0964, time 524.53ms, mfu 0.70%\n",
            "iter 2190: loss 0.0904, time 528.79ms, mfu 0.70%\n",
            "iter 2200: loss 0.0887, time 527.25ms, mfu 0.70%\n",
            "iter 2210: loss 0.0887, time 526.86ms, mfu 0.70%\n",
            "iter 2220: loss 0.0906, time 525.83ms, mfu 0.70%\n",
            "iter 2230: loss 0.0930, time 527.18ms, mfu 0.70%\n",
            "iter 2240: loss 0.0933, time 528.90ms, mfu 0.70%\n",
            "step 2250: train loss 0.0606, val loss 1.7767\n",
            "iter 2250: loss 0.0968, time 74514.25ms, mfu 0.63%\n",
            "iter 2260: loss 0.0977, time 530.01ms, mfu 0.64%\n",
            "iter 2270: loss 0.0926, time 529.38ms, mfu 0.65%\n",
            "iter 2280: loss 0.0936, time 532.67ms, mfu 0.65%\n",
            "iter 2290: loss 0.0927, time 527.80ms, mfu 0.66%\n",
            "iter 2300: loss 0.0857, time 525.11ms, mfu 0.66%\n",
            "iter 2310: loss 0.0936, time 526.92ms, mfu 0.67%\n",
            "iter 2320: loss 0.0911, time 528.58ms, mfu 0.67%\n",
            "iter 2330: loss 0.0933, time 528.60ms, mfu 0.67%\n",
            "iter 2340: loss 0.0923, time 527.23ms, mfu 0.68%\n",
            "iter 2350: loss 0.0898, time 530.33ms, mfu 0.68%\n",
            "iter 2360: loss 0.0879, time 529.65ms, mfu 0.68%\n",
            "iter 2370: loss 0.0888, time 530.33ms, mfu 0.68%\n",
            "iter 2380: loss 0.0859, time 527.48ms, mfu 0.69%\n",
            "iter 2390: loss 0.0864, time 524.73ms, mfu 0.69%\n",
            "iter 2400: loss 0.0850, time 527.67ms, mfu 0.69%\n",
            "iter 2410: loss 0.0845, time 526.60ms, mfu 0.69%\n",
            "iter 2420: loss 0.0798, time 526.77ms, mfu 0.69%\n",
            "iter 2430: loss 0.0886, time 528.13ms, mfu 0.70%\n",
            "iter 2440: loss 0.0866, time 528.57ms, mfu 0.70%\n",
            "iter 2450: loss 0.0843, time 527.81ms, mfu 0.70%\n",
            "iter 2460: loss 0.0842, time 527.97ms, mfu 0.70%\n",
            "iter 2470: loss 0.0783, time 529.66ms, mfu 0.70%\n",
            "iter 2480: loss 0.0821, time 530.15ms, mfu 0.70%\n",
            "iter 2490: loss 0.0847, time 529.34ms, mfu 0.70%\n",
            "step 2500: train loss 0.0546, val loss 1.7551\n",
            "iter 2500: loss 0.0894, time 74644.05ms, mfu 0.63%\n",
            "iter 2510: loss 0.0827, time 529.14ms, mfu 0.64%\n",
            "iter 2520: loss 0.0845, time 529.22ms, mfu 0.64%\n",
            "iter 2530: loss 0.0809, time 530.99ms, mfu 0.65%\n",
            "iter 2540: loss 0.0803, time 530.25ms, mfu 0.66%\n",
            "iter 2550: loss 0.0920, time 528.95ms, mfu 0.66%\n",
            "iter 2560: loss 0.0854, time 529.56ms, mfu 0.67%\n",
            "iter 2570: loss 0.0807, time 528.57ms, mfu 0.67%\n",
            "iter 2580: loss 0.0886, time 527.64ms, mfu 0.67%\n",
            "iter 2590: loss 0.0826, time 528.28ms, mfu 0.68%\n",
            "iter 2600: loss 0.0762, time 526.11ms, mfu 0.68%\n",
            "iter 2610: loss 0.0784, time 529.01ms, mfu 0.68%\n",
            "iter 2620: loss 0.0793, time 528.78ms, mfu 0.68%\n",
            "iter 2630: loss 0.0858, time 527.94ms, mfu 0.69%\n",
            "iter 2640: loss 0.0771, time 525.87ms, mfu 0.69%\n",
            "iter 2650: loss 0.0721, time 529.97ms, mfu 0.69%\n",
            "iter 2660: loss 0.0809, time 527.33ms, mfu 0.69%\n",
            "iter 2670: loss 0.0787, time 529.77ms, mfu 0.69%\n",
            "iter 2680: loss 0.0797, time 530.31ms, mfu 0.69%\n",
            "iter 2690: loss 0.0769, time 526.52ms, mfu 0.70%\n",
            "iter 2700: loss 0.0760, time 524.67ms, mfu 0.70%\n",
            "iter 2710: loss 0.0741, time 528.76ms, mfu 0.70%\n",
            "iter 2720: loss 0.0768, time 526.91ms, mfu 0.70%\n",
            "iter 2730: loss 0.0716, time 528.04ms, mfu 0.70%\n",
            "iter 2740: loss 0.0817, time 527.01ms, mfu 0.70%\n",
            "step 2750: train loss 0.0525, val loss 1.7758\n",
            "iter 2750: loss 0.0719, time 74429.05ms, mfu 0.63%\n",
            "iter 2760: loss 0.0788, time 527.26ms, mfu 0.64%\n",
            "iter 2770: loss 0.0809, time 527.99ms, mfu 0.65%\n",
            "iter 2780: loss 0.0719, time 528.44ms, mfu 0.65%\n",
            "iter 2790: loss 0.0786, time 525.79ms, mfu 0.66%\n",
            "iter 2800: loss 0.0752, time 528.40ms, mfu 0.66%\n",
            "iter 2810: loss 0.0724, time 528.35ms, mfu 0.67%\n",
            "iter 2820: loss 0.0737, time 529.58ms, mfu 0.67%\n",
            "iter 2830: loss 0.0764, time 529.60ms, mfu 0.67%\n",
            "iter 2840: loss 0.0676, time 529.98ms, mfu 0.68%\n",
            "iter 2850: loss 0.0790, time 528.40ms, mfu 0.68%\n",
            "iter 2860: loss 0.0778, time 529.32ms, mfu 0.68%\n",
            "iter 2870: loss 0.0805, time 531.25ms, mfu 0.68%\n",
            "iter 2880: loss 0.0726, time 528.79ms, mfu 0.69%\n",
            "iter 2890: loss 0.0734, time 530.19ms, mfu 0.69%\n",
            "iter 2900: loss 0.0678, time 526.95ms, mfu 0.69%\n",
            "iter 2910: loss 0.0727, time 525.56ms, mfu 0.69%\n",
            "iter 2920: loss 0.0666, time 525.90ms, mfu 0.69%\n",
            "iter 2930: loss 0.0710, time 528.98ms, mfu 0.69%\n",
            "iter 2940: loss 0.0699, time 525.43ms, mfu 0.70%\n",
            "iter 2950: loss 0.0749, time 528.63ms, mfu 0.70%\n",
            "iter 2960: loss 0.0738, time 527.58ms, mfu 0.70%\n",
            "iter 2970: loss 0.0682, time 526.18ms, mfu 0.70%\n",
            "iter 2980: loss 0.0704, time 529.66ms, mfu 0.70%\n",
            "iter 2990: loss 0.0696, time 529.35ms, mfu 0.70%\n",
            "step 3000: train loss 0.0502, val loss 1.7744\n",
            "iter 3000: loss 0.0673, time 74473.42ms, mfu 0.63%\n",
            "iter 3010: loss 0.0674, time 525.95ms, mfu 0.64%\n",
            "iter 3020: loss 0.0689, time 527.02ms, mfu 0.65%\n",
            "iter 3030: loss 0.0703, time 529.06ms, mfu 0.65%\n",
            "iter 3040: loss 0.0754, time 526.08ms, mfu 0.66%\n",
            "iter 3050: loss 0.0724, time 523.71ms, mfu 0.66%\n",
            "iter 3060: loss 0.0656, time 522.95ms, mfu 0.67%\n",
            "iter 3070: loss 0.0680, time 526.07ms, mfu 0.67%\n",
            "iter 3080: loss 0.0649, time 524.57ms, mfu 0.68%\n",
            "iter 3090: loss 0.0717, time 525.25ms, mfu 0.68%\n",
            "iter 3100: loss 0.0607, time 530.04ms, mfu 0.68%\n",
            "iter 3110: loss 0.0667, time 528.97ms, mfu 0.68%\n",
            "iter 3120: loss 0.0705, time 532.06ms, mfu 0.69%\n",
            "iter 3130: loss 0.0671, time 529.42ms, mfu 0.69%\n",
            "iter 3140: loss 0.0631, time 531.69ms, mfu 0.69%\n",
            "iter 3150: loss 0.0653, time 530.00ms, mfu 0.69%\n",
            "iter 3160: loss 0.0642, time 532.24ms, mfu 0.69%\n",
            "iter 3170: loss 0.0641, time 526.93ms, mfu 0.69%\n",
            "iter 3180: loss 0.0632, time 528.98ms, mfu 0.69%\n",
            "iter 3190: loss 0.0607, time 530.24ms, mfu 0.70%\n",
            "iter 3200: loss 0.0654, time 527.83ms, mfu 0.70%\n",
            "iter 3210: loss 0.0673, time 529.62ms, mfu 0.70%\n",
            "iter 3220: loss 0.0659, time 527.21ms, mfu 0.70%\n",
            "iter 3230: loss 0.0685, time 529.15ms, mfu 0.70%\n",
            "iter 3240: loss 0.0630, time 527.95ms, mfu 0.70%\n",
            "step 3250: train loss 0.0484, val loss 1.8183\n",
            "iter 3250: loss 0.0682, time 74536.49ms, mfu 0.63%\n",
            "iter 3260: loss 0.0652, time 528.93ms, mfu 0.64%\n",
            "iter 3270: loss 0.0629, time 532.03ms, mfu 0.64%\n",
            "iter 3280: loss 0.0659, time 530.96ms, mfu 0.65%\n",
            "iter 3290: loss 0.0623, time 527.18ms, mfu 0.66%\n",
            "iter 3300: loss 0.0648, time 526.28ms, mfu 0.66%\n",
            "iter 3310: loss 0.0646, time 529.34ms, mfu 0.67%\n",
            "iter 3320: loss 0.0679, time 527.00ms, mfu 0.67%\n",
            "iter 3330: loss 0.0623, time 525.85ms, mfu 0.67%\n",
            "iter 3340: loss 0.0646, time 525.19ms, mfu 0.68%\n",
            "iter 3350: loss 0.0627, time 528.61ms, mfu 0.68%\n",
            "iter 3360: loss 0.0600, time 530.52ms, mfu 0.68%\n",
            "iter 3370: loss 0.0681, time 531.54ms, mfu 0.68%\n",
            "iter 3380: loss 0.0604, time 525.89ms, mfu 0.69%\n",
            "iter 3390: loss 0.0632, time 528.63ms, mfu 0.69%\n",
            "iter 3400: loss 0.0606, time 527.50ms, mfu 0.69%\n",
            "iter 3410: loss 0.0620, time 527.85ms, mfu 0.69%\n",
            "iter 3420: loss 0.0613, time 528.43ms, mfu 0.69%\n",
            "iter 3430: loss 0.0607, time 524.78ms, mfu 0.70%\n",
            "iter 3440: loss 0.0621, time 521.49ms, mfu 0.70%\n",
            "iter 3450: loss 0.0597, time 522.77ms, mfu 0.70%\n",
            "iter 3460: loss 0.0640, time 523.66ms, mfu 0.70%\n",
            "iter 3470: loss 0.0663, time 528.35ms, mfu 0.70%\n",
            "iter 3480: loss 0.0646, time 526.15ms, mfu 0.70%\n",
            "iter 3490: loss 0.0581, time 526.53ms, mfu 0.70%\n",
            "step 3500: train loss 0.0453, val loss 1.8972\n",
            "iter 3500: loss 0.0625, time 74586.82ms, mfu 0.63%\n",
            "iter 3510: loss 0.0595, time 528.61ms, mfu 0.64%\n",
            "iter 3520: loss 0.0600, time 526.80ms, mfu 0.65%\n",
            "iter 3530: loss 0.0607, time 526.85ms, mfu 0.65%\n",
            "iter 3540: loss 0.0555, time 523.96ms, mfu 0.66%\n",
            "iter 3550: loss 0.0612, time 528.41ms, mfu 0.66%\n",
            "iter 3560: loss 0.0622, time 526.46ms, mfu 0.67%\n",
            "iter 3570: loss 0.0609, time 526.50ms, mfu 0.67%\n",
            "iter 3580: loss 0.0608, time 528.73ms, mfu 0.68%\n",
            "iter 3590: loss 0.0565, time 526.08ms, mfu 0.68%\n",
            "iter 3600: loss 0.0588, time 525.75ms, mfu 0.68%\n",
            "iter 3610: loss 0.0554, time 527.36ms, mfu 0.68%\n",
            "iter 3620: loss 0.0578, time 525.86ms, mfu 0.69%\n",
            "iter 3630: loss 0.0571, time 528.83ms, mfu 0.69%\n",
            "iter 3640: loss 0.0543, time 526.99ms, mfu 0.69%\n",
            "iter 3650: loss 0.0579, time 528.45ms, mfu 0.69%\n",
            "iter 3660: loss 0.0577, time 529.14ms, mfu 0.69%\n",
            "iter 3670: loss 0.0576, time 527.60ms, mfu 0.69%\n",
            "iter 3680: loss 0.0555, time 529.59ms, mfu 0.70%\n",
            "iter 3690: loss 0.0601, time 525.86ms, mfu 0.70%\n",
            "iter 3700: loss 0.0578, time 524.48ms, mfu 0.70%\n",
            "iter 3710: loss 0.0570, time 527.48ms, mfu 0.70%\n",
            "iter 3720: loss 0.0534, time 527.33ms, mfu 0.70%\n",
            "iter 3730: loss 0.0556, time 525.00ms, mfu 0.70%\n",
            "iter 3740: loss 0.0627, time 527.57ms, mfu 0.70%\n",
            "step 3750: train loss 0.0442, val loss 1.8883\n",
            "iter 3750: loss 0.0573, time 74418.79ms, mfu 0.63%\n",
            "iter 3760: loss 0.0561, time 528.35ms, mfu 0.64%\n",
            "iter 3770: loss 0.0558, time 527.24ms, mfu 0.65%\n",
            "iter 3780: loss 0.0552, time 529.04ms, mfu 0.65%\n",
            "iter 3790: loss 0.0582, time 528.96ms, mfu 0.66%\n",
            "iter 3800: loss 0.0553, time 527.00ms, mfu 0.66%\n",
            "iter 3810: loss 0.0607, time 529.62ms, mfu 0.67%\n",
            "iter 3820: loss 0.0538, time 531.63ms, mfu 0.67%\n",
            "iter 3830: loss 0.0529, time 530.21ms, mfu 0.67%\n",
            "iter 3840: loss 0.0531, time 526.99ms, mfu 0.68%\n",
            "iter 3850: loss 0.0573, time 533.29ms, mfu 0.68%\n",
            "iter 3860: loss 0.0576, time 531.81ms, mfu 0.68%\n",
            "iter 3870: loss 0.0544, time 524.97ms, mfu 0.68%\n",
            "iter 3880: loss 0.0599, time 525.62ms, mfu 0.69%\n",
            "iter 3890: loss 0.0567, time 529.34ms, mfu 0.69%\n",
            "iter 3900: loss 0.0566, time 529.66ms, mfu 0.69%\n",
            "iter 3910: loss 0.0572, time 527.52ms, mfu 0.69%\n",
            "iter 3920: loss 0.0536, time 530.34ms, mfu 0.69%\n",
            "iter 3930: loss 0.0576, time 529.24ms, mfu 0.69%\n",
            "iter 3940: loss 0.0552, time 527.33ms, mfu 0.70%\n",
            "iter 3950: loss 0.0565, time 527.74ms, mfu 0.70%\n",
            "iter 3960: loss 0.0548, time 528.77ms, mfu 0.70%\n",
            "iter 3970: loss 0.0528, time 530.72ms, mfu 0.70%\n",
            "iter 3980: loss 0.0567, time 528.56ms, mfu 0.70%\n",
            "iter 3990: loss 0.0537, time 530.82ms, mfu 0.70%\n",
            "step 4000: train loss 0.0425, val loss 1.9416\n",
            "iter 4000: loss 0.0565, time 74528.78ms, mfu 0.63%\n",
            "iter 4010: loss 0.0540, time 527.84ms, mfu 0.64%\n",
            "iter 4020: loss 0.0567, time 531.30ms, mfu 0.64%\n",
            "iter 4030: loss 0.0566, time 530.09ms, mfu 0.65%\n",
            "iter 4040: loss 0.0558, time 527.71ms, mfu 0.66%\n",
            "iter 4050: loss 0.0509, time 526.35ms, mfu 0.66%\n",
            "iter 4060: loss 0.0508, time 524.54ms, mfu 0.67%\n",
            "iter 4070: loss 0.0499, time 525.13ms, mfu 0.67%\n",
            "iter 4080: loss 0.0531, time 526.94ms, mfu 0.67%\n",
            "iter 4090: loss 0.0494, time 529.79ms, mfu 0.68%\n",
            "iter 4100: loss 0.0494, time 529.41ms, mfu 0.68%\n",
            "iter 4110: loss 0.0521, time 529.82ms, mfu 0.68%\n",
            "iter 4120: loss 0.0515, time 530.36ms, mfu 0.68%\n",
            "iter 4130: loss 0.0527, time 531.15ms, mfu 0.69%\n",
            "iter 4140: loss 0.0536, time 531.25ms, mfu 0.69%\n",
            "iter 4150: loss 0.0542, time 529.25ms, mfu 0.69%\n",
            "iter 4160: loss 0.0548, time 527.75ms, mfu 0.69%\n",
            "iter 4170: loss 0.0538, time 525.86ms, mfu 0.69%\n",
            "iter 4180: loss 0.0553, time 524.19ms, mfu 0.69%\n",
            "iter 4190: loss 0.0546, time 526.47ms, mfu 0.70%\n",
            "iter 4200: loss 0.0543, time 527.42ms, mfu 0.70%\n",
            "iter 4210: loss 0.0513, time 525.80ms, mfu 0.70%\n",
            "iter 4220: loss 0.0523, time 527.17ms, mfu 0.70%\n",
            "iter 4230: loss 0.0531, time 525.16ms, mfu 0.70%\n",
            "iter 4240: loss 0.0520, time 528.28ms, mfu 0.70%\n",
            "step 4250: train loss 0.0418, val loss 1.9359\n",
            "iter 4250: loss 0.0543, time 74533.28ms, mfu 0.63%\n",
            "iter 4260: loss 0.0496, time 531.43ms, mfu 0.64%\n",
            "iter 4270: loss 0.0508, time 525.92ms, mfu 0.65%\n",
            "iter 4280: loss 0.0482, time 522.48ms, mfu 0.65%\n",
            "iter 4290: loss 0.0544, time 525.87ms, mfu 0.66%\n",
            "iter 4300: loss 0.0525, time 523.49ms, mfu 0.66%\n",
            "iter 4310: loss 0.0501, time 525.40ms, mfu 0.67%\n",
            "iter 4320: loss 0.0475, time 525.42ms, mfu 0.67%\n",
            "iter 4330: loss 0.0519, time 527.11ms, mfu 0.68%\n",
            "iter 4340: loss 0.0545, time 531.11ms, mfu 0.68%\n",
            "iter 4350: loss 0.0549, time 526.22ms, mfu 0.68%\n",
            "iter 4360: loss 0.0487, time 529.96ms, mfu 0.68%\n",
            "iter 4370: loss 0.0531, time 530.99ms, mfu 0.69%\n",
            "iter 4380: loss 0.0514, time 528.19ms, mfu 0.69%\n",
            "iter 4390: loss 0.0547, time 530.14ms, mfu 0.69%\n",
            "iter 4400: loss 0.0466, time 531.96ms, mfu 0.69%\n",
            "iter 4410: loss 0.0516, time 530.29ms, mfu 0.69%\n",
            "iter 4420: loss 0.0506, time 526.21ms, mfu 0.69%\n",
            "iter 4430: loss 0.0539, time 525.64ms, mfu 0.70%\n",
            "iter 4440: loss 0.0490, time 529.77ms, mfu 0.70%\n",
            "iter 4450: loss 0.0515, time 527.70ms, mfu 0.70%\n",
            "iter 4460: loss 0.0494, time 528.24ms, mfu 0.70%\n",
            "iter 4470: loss 0.0505, time 530.37ms, mfu 0.70%\n",
            "iter 4480: loss 0.0512, time 527.25ms, mfu 0.70%\n",
            "iter 4490: loss 0.0504, time 530.66ms, mfu 0.70%\n",
            "step 4500: train loss 0.0408, val loss 1.9607\n",
            "iter 4500: loss 0.0522, time 74644.80ms, mfu 0.63%\n",
            "iter 4510: loss 0.0495, time 528.35ms, mfu 0.64%\n",
            "iter 4520: loss 0.0490, time 528.55ms, mfu 0.64%\n",
            "iter 4530: loss 0.0537, time 528.21ms, mfu 0.65%\n",
            "iter 4540: loss 0.0497, time 524.79ms, mfu 0.66%\n",
            "iter 4550: loss 0.0497, time 524.69ms, mfu 0.66%\n",
            "iter 4560: loss 0.0496, time 524.06ms, mfu 0.67%\n",
            "iter 4570: loss 0.0512, time 528.01ms, mfu 0.67%\n",
            "iter 4580: loss 0.0514, time 528.62ms, mfu 0.67%\n",
            "iter 4590: loss 0.0471, time 523.60ms, mfu 0.68%\n",
            "iter 4600: loss 0.0487, time 526.71ms, mfu 0.68%\n",
            "iter 4610: loss 0.0483, time 530.98ms, mfu 0.68%\n",
            "iter 4620: loss 0.0490, time 530.63ms, mfu 0.69%\n",
            "iter 4630: loss 0.0472, time 529.31ms, mfu 0.69%\n",
            "iter 4640: loss 0.0490, time 528.21ms, mfu 0.69%\n",
            "iter 4650: loss 0.0512, time 525.81ms, mfu 0.69%\n",
            "iter 4660: loss 0.0494, time 528.52ms, mfu 0.69%\n",
            "iter 4670: loss 0.0489, time 528.65ms, mfu 0.69%\n",
            "iter 4680: loss 0.0508, time 530.01ms, mfu 0.69%\n",
            "iter 4690: loss 0.0497, time 529.98ms, mfu 0.70%\n",
            "iter 4700: loss 0.0494, time 529.35ms, mfu 0.70%\n",
            "iter 4710: loss 0.0504, time 528.95ms, mfu 0.70%\n",
            "iter 4720: loss 0.0490, time 531.32ms, mfu 0.70%\n",
            "iter 4730: loss 0.0490, time 530.44ms, mfu 0.70%\n",
            "iter 4740: loss 0.0509, time 533.24ms, mfu 0.70%\n",
            "step 4750: train loss 0.0405, val loss 1.9845\n",
            "iter 4750: loss 0.0509, time 74744.42ms, mfu 0.63%\n",
            "iter 4760: loss 0.0486, time 529.60ms, mfu 0.64%\n",
            "iter 4770: loss 0.0470, time 530.37ms, mfu 0.64%\n",
            "iter 4780: loss 0.0471, time 526.76ms, mfu 0.65%\n",
            "iter 4790: loss 0.0485, time 529.18ms, mfu 0.66%\n",
            "iter 4800: loss 0.0503, time 534.16ms, mfu 0.66%\n",
            "iter 4810: loss 0.0489, time 527.33ms, mfu 0.66%\n",
            "iter 4820: loss 0.0509, time 528.70ms, mfu 0.67%\n",
            "iter 4830: loss 0.0480, time 527.16ms, mfu 0.67%\n",
            "iter 4840: loss 0.0469, time 527.55ms, mfu 0.68%\n",
            "iter 4850: loss 0.0486, time 528.84ms, mfu 0.68%\n",
            "iter 4860: loss 0.0514, time 528.62ms, mfu 0.68%\n",
            "iter 4870: loss 0.0482, time 526.73ms, mfu 0.68%\n",
            "iter 4880: loss 0.0478, time 527.29ms, mfu 0.69%\n",
            "iter 4890: loss 0.0534, time 529.33ms, mfu 0.69%\n",
            "iter 4900: loss 0.0483, time 527.21ms, mfu 0.69%\n",
            "iter 4910: loss 0.0508, time 524.47ms, mfu 0.69%\n",
            "iter 4920: loss 0.0474, time 525.97ms, mfu 0.69%\n",
            "iter 4930: loss 0.0486, time 525.03ms, mfu 0.70%\n",
            "iter 4940: loss 0.0510, time 528.51ms, mfu 0.70%\n",
            "iter 4950: loss 0.0500, time 524.99ms, mfu 0.70%\n",
            "iter 4960: loss 0.0466, time 526.86ms, mfu 0.70%\n",
            "iter 4970: loss 0.0466, time 527.11ms, mfu 0.70%\n",
            "iter 4980: loss 0.0487, time 528.30ms, mfu 0.70%\n",
            "iter 4990: loss 0.0495, time 527.09ms, mfu 0.70%\n",
            "step 5000: train loss 0.0400, val loss 1.9852\n",
            "iter 5000: loss 0.0494, time 74455.75ms, mfu 0.63%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#generates sample to file out-code-generation\n",
        "!python sample.py --out_dir=out-code-generation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CzeYiO3xpO1s",
        "outputId": "43e8e605-24b9-465d-e8c1-a59e3c069ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: out_dir = out-code-generation\n",
            "number of parameters: 10.66M\n",
            "Loading meta from data/code_generation/meta.pkl...\n",
            "\n",
            "        if (text[0] != '\\0') {\n",
            "                 }\n",
            "          }\n",
            "\n",
            "     // read in continuation byte, so we've read not writa preatuve preature (byte\", byte_val b.sstr_buffer.cannn_layers\n",
            "            max_i = -10000000;\n",
            "             for (int i = 0; i < head_size; i++) {\n",
            "                   float val = probs_bt[i] - index[i];\n",
            "                  float val +== ix * (probs_bt[i] - max_val);\n",
            "                     float val = 0.0f + pos * frob + i * n;\n",
            "                   float val = 0.0f;\n",
            "                  fl\n",
            "---------------\n",
            "\n",
            "                                                                                                                  int B, int T, int OC, int C) {\n",
            "             for (int o = 0; o < OC; i++) {\n",
            "                                     float val = 0.0f;\n",
            "                     float val = 0.0f + val * (in + i * C) * weight[o + b * C + t * OC;\n",
            "          }\n",
            "            }\n",
            "\n",
            "\n",
            "                                        // dout: hrward pass 1, the input, the value bytes\n",
            "                  float* dout_bt = dout + b * T *\n",
            "---------------\n",
            "\n",
            "                                                                     fprint_btht = beak;\n",
            "                                        //  only for this this to this layer\n",
            "                        float* datt_bth = dout_bth[t2];\n",
            "              float* datt_bth = datt_bth[t2];\n",
            "                  inp + br* T * C + t * C + t * C + t * C + t * C3 + h *2;\n",
            "\n",
            "\n",
            "                                  int ix = inp_bt[i] += datt_bth[t2] * dout_bth[t2];\n",
            "                  }\n",
            "\n",
            "                          }\n",
            "\n",
            "                    \n",
            "---------------\n",
            "\n",
            "     float fci = size;\n",
            "     int i = * fd, file) { fprintf(stderr, \"failed!\\n\"); exit(EXIT_FAILURE); }\n",
            "    *data = MAP_FAILTE *fd, 0);\n",
            "     if (*data == MAP_FAILED) { fprintf(stderr, \"failed!\\n\"); exit(EXIT_FAILURE); }\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "\n",
            "int map_state(RunState* state* state, size_e) {\n",
            "    // forward the state vectors *targets, targets, weights from *targets)\n",
            "    read_size_t* ptr;\n",
            "    return state_file_sizes(&sampler, config, const const sizeof(Tokenizeof(Tokenizer_path), const void* b));\n",
            "    retur\n",
            "---------------\n",
            "\n",
            "                          float* value_t2 = expected_logits = model.mean_logits;\n",
            "                  for (int b = 0; bt < B; b++) {\n",
            "                for (int t = 0; t < T; t++) {\n",
            "                          float* att_bth = att_bth[t2] += att_bth[t2] * T * C;\n",
            "                           float* dlogits_bth[t2] = acts_mean_bt) * rstd + b * T * C;\n",
            "                     float* dpreatt_bth[t2] = attt_bth[t2];\n",
            "                       float* dpreatt_bth[t2] = act_score + all_deres[i];\n",
            "                    }\n",
            "\n",
            "  \n",
            "---------------\n",
            "\n",
            "          // thbe user the activation mean backward passs\n",
            "         int next;\n",
            "       // merge the tat (argv[i], buffer[start)\n",
            "                if (piece == NULL) { printf(\"\\n\"); }\n",
            "                   break; // ad position in mer starts, so weighte position\n",
            "                      read_stdin(\"Enter start start = 0);\n",
            "                 // the Ation as 1: \"end the resystem prompt in chat (seq_len, seq_len, system_prompt));\n",
            "                 }\n",
            "                  }\n",
            "\n",
            "                   // ent the transformer.n\n",
            "---------------\n",
            "\n",
            "   free(t->weights.wv);\n",
            "    free(t->weights.w1);\n",
            "    free(t->weights.wk);\n",
            "     free(t->weights.wv);\n",
            "    free(t->weights.w1);\n",
            "    free(t->weights.w1);\n",
            "     free(t->weights.wcls);\n",
            "   free(t->weights.wo);\n",
            "    free(t->weights.w2);\n",
            "    free(t->weights.wk);\n",
            "    free(t->weights.w1);\n",
            "    free(t->weights.wq);\n",
            "    free(t->weights.w1);\n",
            "    free(t->weights.wk);\n",
            "    free(t->weights.wk);\n",
            "    free(t->weights.w2);\n",
            "    free(t->weights.w1);\n",
            "    free(t->weights.wk));\n",
            "    free(t->weights.w2);\n",
            "    free(t->weights.wk\n",
            "---------------\n",
            "\n",
            "            if(pos == 0 && str_len < T) {\n",
            "                 for (int i = 0; i++) {\n",
            "                     max_i = inp[bt[i];\n",
            "              }\n",
            "                  }\n",
            "          }\n",
            "\n",
            "      return - -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "// I Halloc\n",
            "\n",
            "void read_stding(Transformer, Tokenizer *tokenizer, Tokenizer, &next, compare_path,\n",
            "---------------\n",
            "\n",
            "        if(next == 0 && prompt_tokens < num_prompt_tokens - 1) {\n",
            "                          // encode the aing string prompt from this and excted prompt\n",
            "                            strcpy(system_prompt);\n",
            "                 } else {\n",
            "                             // the user prompt from passsem prompt\n",
            "                       float* pos = 0.0f + t * Vp;\n",
            "\n",
            "                       float indice = 0.0f;\n",
            "                 float p = 0.0f;\n",
            "                  float indicator = 0.0f;\n",
            "                float indicator \n",
            "---------------\n",
            "\n",
            "               const float best_idx = i;\n",
            "                           bt[t * T + t] = inp + b * T * C + t * C + t];\n",
            "                   // calculate the accumulation bth[t2]\n",
            "                  float t_btht2 = 0.0f;\n",
            "                 for (int i = 0; i < hs; i++) {\n",
            "                         float val  = 0.0f;\n",
            "                   float val = 0.0f + val * 1.0f * x * 1expv;\n",
            "                  float val = val * fcr - v[i];\n",
            "                  vec[i] = v0 * fci + i * fci + v1 * fcr - v1 * fci + v1 * fcr;\n",
            "       \n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}