{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jV7SXYovFX6Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1f5c5640-9151-4940-933e-957256a193d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "Found existing installation: scipy 1.10.1\n",
            "Uninstalling scipy-1.10.1:\n",
            "  Successfully uninstalled scipy-1.10.1\n",
            "Found existing installation: scikit-learn 1.2.2\n",
            "Uninstalling scikit-learn-1.2.2:\n",
            "  Successfully uninstalled scikit-learn-1.2.2\n",
            "Found existing installation: pmdarima 2.0.3\n",
            "Uninstalling pmdarima-2.0.3:\n",
            "  Successfully uninstalled pmdarima-2.0.3\n",
            "Found existing installation: statsmodels 0.13.5\n",
            "Uninstalling statsmodels-0.13.5:\n",
            "  Successfully uninstalled statsmodels-0.13.5\n",
            "Found existing installation: tensorflow 2.12.0\n",
            "Uninstalling tensorflow-2.12.0:\n",
            "  Successfully uninstalled tensorflow-2.12.0\n",
            "Collecting numpy==1.23.5\n",
            "  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Collecting scipy==1.10.1\n",
            "  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.2.2\n",
            "  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\n",
            "Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m128.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m130.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.14.5 requires statsmodels>=0.14.0, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "bigframes 2.1.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.30 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "blosc2 3.3.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\n",
            "orbax-checkpoint 0.11.12 requires jax>=0.5.0, but you have jax 0.4.30 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5 scikit-learn-1.2.2 scipy-1.10.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "fbf08fccc1744e7fb5ee48edf404d228"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pmdarima==2.0.3\n",
            "  Using cached pmdarima-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl.metadata (7.8 kB)\n",
            "Collecting statsmodels==0.13.5\n",
            "  Using cached statsmodels-0.13.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (1.10.1)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (2.4.0)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.11/dist-packages (from pmdarima==2.0.3) (75.2.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.11/dist-packages (from statsmodels==0.13.5) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels==0.13.5) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima==2.0.3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima==2.0.3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.19->pmdarima==2.0.3) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.22->pmdarima==2.0.3) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19->pmdarima==2.0.3) (1.17.0)\n",
            "Using cached pmdarima-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (1.9 MB)\n",
            "Using cached statsmodels-0.13.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
            "Installing collected packages: statsmodels, pmdarima\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "plotnine 0.14.5 requires statsmodels>=0.14.0, but you have statsmodels 0.13.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pmdarima-2.0.3 statsmodels-0.13.5\n",
            "Collecting tensorflow==2.12.0\n",
            "  Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (25.2.10)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.71.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.13.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (18.1.1)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.25.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.12.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.12.0) (0.45.1)\n",
            "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.30)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (0.4.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax>=0.3.15->tensorflow==2.12.0) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0) (3.2.2)\n",
            "Using cached tensorflow-2.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (586.0 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.12.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.12.0\n"
          ]
        }
      ],
      "source": [
        "# Complete clean installation\n",
        "!pip uninstall numpy scipy scikit-learn pmdarima statsmodels tensorflow -y\n",
        "!pip install numpy==1.23.5 scipy==1.10.1 scikit-learn==1.2.2 --no-cache-dir\n",
        "!pip install pmdarima==2.0.3 statsmodels==0.13.5\n",
        "!pip install tensorflow==2.12.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn\n",
        "import pmdarima\n",
        "import statsmodels\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "Ifayxcx0NfW3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"NumPy: {np.__version__}\")\n",
        "print(f\"SciPy: {scipy.__version__}\")\n",
        "print(f\"scikit-learn: {sklearn.__version__}\")\n",
        "print(f\"pmdarima: {pmdarima.__version__}\")\n",
        "print(f\"statsmodels: {statsmodels.__version__}\")\n",
        "print(f\"TensorFlow: {tf.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay9wSSpzNjVR",
        "outputId": "ebda299e-dac6-4aed-a812-4e0e5b9b605a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy: 1.23.5\n",
            "SciPy: 1.10.1\n",
            "scikit-learn: 1.2.2\n",
            "pmdarima: 2.0.3\n",
            "statsmodels: 0.13.5\n",
            "TensorFlow: 2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle imports with version checking\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "l3DZmTnhNlK9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle imports with proper error handling\n",
        "try:\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from pmdarima.arima import auto_arima\n",
        "    ARIMA_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    ARIMA_AVAILABLE = False\n",
        "    print(f\"ARIMA packages not available: {e}\")\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Model, Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Input, Embedding, Concatenate, Flatten\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    LSTM_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    LSTM_AVAILABLE = False\n",
        "    print(f\"LSTM packages not available: {e}\")"
      ],
      "metadata": {
        "id": "awY29fAmNm2X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rest of your TrafficAnalysisSystem class code...\n",
        "\n",
        "class TrafficAnalysisSystem:\n",
        "    def __init__(self):\n",
        "        self.df = None\n",
        "        self.kmeans = None\n",
        "        self.scaler = None\n",
        "        self.arima_models = {}\n",
        "        self.lstm_model = None\n",
        "        self.road_to_idx = {}\n",
        "    def load_data(self, data_path):\n",
        "        \"\"\"Load and preprocess traffic data with validation\"\"\"\n",
        "        try:\n",
        "            self.df = pd.read_csv(data_path)\n",
        "\n",
        "            # Validate required columns exist\n",
        "            required_cols = ['Date', 'Hour', 'Road', 'Average_Speed',\n",
        "                            'Average_Occupancy', 'Total_Volume']\n",
        "            if not all(col in self.df.columns for col in required_cols):\n",
        "                missing = [col for col in required_cols if col not in self.df.columns]\n",
        "                raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "            # Create datetime index\n",
        "            self.df['datetime'] = pd.to_datetime(\n",
        "                self.df['Date'].astype(str) + ' ' + self.df['Hour'].astype(str) + ':00:00'\n",
        "            )\n",
        "            self.df.set_index('datetime', inplace=True)\n",
        "\n",
        "            # Add temporal features\n",
        "            self.df['day_of_week'] = self.df.index.dayofweek\n",
        "            self.df['hour'] = self.df.index.hour\n",
        "            self.df['is_weekend'] = self.df['day_of_week'].isin([5,6]).astype(int)\n",
        "\n",
        "            # Ensure categorical types\n",
        "            self.df['Road'] = self.df['Road'].astype('category')\n",
        "\n",
        "            # Validate numeric columns\n",
        "            numeric_cols = ['Average_Speed', 'Average_Occupancy', 'Total_Volume']\n",
        "            for col in numeric_cols:\n",
        "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
        "\n",
        "            print(f\"Data loaded successfully with {len(self.df)} rows\")\n",
        "            return self.df\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            return None\n",
        "\n",
        "    def cluster_traffic_states(self):\n",
        "        \"\"\"Fixed clustering with proper data alignment\"\"\"\n",
        "        try:\n",
        "            # Create flow rate feature safely\n",
        "            self.df['flow_rate'] = np.where(\n",
        "                self.df['Average_Occupancy'] > 0,\n",
        "                self.df['Total_Volume'] / self.df['Average_Occupancy'],\n",
        "                self.df['Total_Volume'] / 0.01\n",
        "            )\n",
        "\n",
        "            # Select features and drop NA\n",
        "            features = ['Average_Speed', 'Average_Occupancy', 'Total_Volume', 'flow_rate']\n",
        "            valid_idx = self.df[features].notna().all(axis=1)\n",
        "            X = self.df.loc[valid_idx, features]\n",
        "\n",
        "            if len(X) == 0:\n",
        "                raise ValueError(\"No valid data for clustering\")\n",
        "\n",
        "            # Scale features\n",
        "            self.scaler = StandardScaler()\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "            # Cluster using KMeans\n",
        "            self.kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "            clusters = self.kmeans.fit_predict(X_scaled)\n",
        "\n",
        "            # Initialize traffic_state with NA\n",
        "            self.df['traffic_state'] = np.nan\n",
        "            self.df.loc[valid_idx, 'traffic_state'] = clusters\n",
        "\n",
        "            # Label states\n",
        "            state_labels = {\n",
        "                0: 'Free Flow',\n",
        "                1: 'Light Congestion',\n",
        "                2: 'Heavy Congestion',\n",
        "                3: 'Severe Jam'\n",
        "            }\n",
        "            self.df['traffic_state_label'] = self.df['traffic_state'].map(state_labels)\n",
        "\n",
        "            # Visualize clusters\n",
        "            self._plot_clusters()\n",
        "            print(f\"Successfully clustered {len(X)} samples\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error in clustering: {e}\")\n",
        "            return False\n",
        "\n",
        "    def build_arima_models(self):\n",
        "      \"\"\"ARIMA modeling with enhanced validation\"\"\"\n",
        "      if not ARIMA_AVAILABLE:\n",
        "          print(\"ARIMA functionality not available\")\n",
        "          return False\n",
        "\n",
        "      try:\n",
        "          successful_models = 0\n",
        "\n",
        "          for road in self.df['Road'].cat.categories:\n",
        "              road_data = self.df[self.df['Road'] == road]\n",
        "\n",
        "              # Check if traffic_state exists and has enough data\n",
        "              if 'traffic_state' not in road_data.columns:\n",
        "                  print(f\"Skipping {road} - no traffic_state column\")\n",
        "                  continue\n",
        "\n",
        "              ts_data = road_data['traffic_state'].dropna()\n",
        "\n",
        "              if len(ts_data) < 24:\n",
        "                  print(f\"Skipping {road} - only {len(ts_data)} valid samples (need >=24)\")\n",
        "                  continue\n",
        "\n",
        "              try:\n",
        "                  model = auto_arima(\n",
        "                      ts_data,\n",
        "                      seasonal=False,\n",
        "                      suppress_warnings=True,\n",
        "                      error_action='ignore',\n",
        "                      trace=True\n",
        "                  )\n",
        "\n",
        "                  self.arima_models[road] = model\n",
        "                  successful_models += 1\n",
        "                  print(f\"Built ARIMA{model.order} for {road}\")\n",
        "\n",
        "              except Exception as e:\n",
        "                  print(f\"ARIMA failed for {road}: {str(e)}\")\n",
        "\n",
        "          print(f\"\\nBuilt ARIMA models for {successful_models} roads\")\n",
        "          return successful_models > 0\n",
        "      except Exception as e:\n",
        "          print(f\"ARIMA modeling error: {e}\")\n",
        "          return False\n",
        "\n",
        "\n",
        "    def build_lstm_model(self):\n",
        "        \"\"\"Build and train LSTM model - FIXED VERSION\"\"\"\n",
        "        if not LSTM_AVAILABLE:\n",
        "            print(\"LSTM functionality not available\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Check if traffic_state exists\n",
        "            if 'traffic_state' not in self.df.columns:\n",
        "                print(\"Error: traffic_state column missing\")\n",
        "                return False\n",
        "\n",
        "            # Rest of your LSTM implementation...\n",
        "            # [Keep your existing LSTM code but ensure traffic_state exists]\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error in LSTM modeling: {e}\")\n",
        "            return False\n",
        "    def _train_lstm(self, X_seq, X_road, y_categorical):\n",
        "        \"\"\"Train the LSTM model\"\"\"\n",
        "        split = int(0.8 * len(X_seq))\n",
        "\n",
        "        history = self.lstm_model.fit(\n",
        "            [X_seq[:split], X_road[:split]],\n",
        "            y_categorical[:split],\n",
        "            epochs=15,\n",
        "            batch_size=32,\n",
        "            validation_data=([X_seq[split:], X_road[split:]], y_categorical[split:]),\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Plot training history\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "        plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "        plt.title('Model Training History')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "        # Evaluate\n",
        "        y_pred = np.argmax(self.lstm_model.predict([X_seq[split:], X_road[split:]]), axis=1)\n",
        "        y_true = np.argmax(y_categorical[split:], axis=1)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "PsREyGgBNpDq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the analysis system\n",
        "    analyzer = TrafficAnalysisSystem()\n",
        "\n",
        "    # Load your data - replace with your actual data path\n",
        "    data_path = \"aggregated_hourly_data.csv\"  # Update this path\n",
        "    if analyzer.load_data(data_path) is None:\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Cluster traffic states\n",
        "    if not analyzer.cluster_traffic_states():\n",
        "        print(\"Failed to cluster traffic states\")\n",
        "\n",
        "    # Build ARIMA models if available\n",
        "    if ARIMA_AVAILABLE:\n",
        "        if not analyzer.build_arima_models():\n",
        "            print(\"Failed to build ARIMA models\")\n",
        "\n",
        "    # Build LSTM model if available\n",
        "    if LSTM_AVAILABLE:\n",
        "        if not analyzer.build_lstm_model():\n",
        "            print(\"Failed to build LSTM model\")\n",
        "\n",
        "    print(\"\\nAnalysis completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am1PCLIFN4nx",
        "outputId": "304b4a9f-1689-47d1-e436-e1333fff6686"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading data: [Errno 2] No such file or directory: 'aggregated_hourly_data.csv'\n",
            "Failed to load data. Exiting.\n",
            "Error in clustering: 'NoneType' object is not subscriptable\n",
            "Failed to cluster traffic states\n",
            "ARIMA modeling error: 'NoneType' object is not subscriptable\n",
            "Failed to build ARIMA models\n",
            "Error in LSTM modeling: 'NoneType' object has no attribute 'columns'\n",
            "Failed to build LSTM model\n",
            "\n",
            "Analysis completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf"
      ],
      "metadata": {
        "id": "VliP0rwyN7qz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Package availability handling\n",
        "try:\n",
        "    from statsmodels.tsa.arima.model import ARIMA\n",
        "    from pmdarima.arima import auto_arima\n",
        "    ARIMA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ARIMA_AVAILABLE = False\n",
        "    print(\"ARIMA functionality not available\")\n",
        "\n",
        "try:\n",
        "    from tensorflow.keras.models import Model, Sequential\n",
        "    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, Embedding, Concatenate, Flatten\n",
        "    from tensorflow.keras.utils import to_categorical\n",
        "    LSTM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LSTM_AVAILABLE = False\n",
        "    print(\"LSTM functionality not available\")"
      ],
      "metadata": {
        "id": "fbBLIwAXN9nl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrafficAnalysisSystem:\n",
        "    def __init__(self):\n",
        "        self.df = None\n",
        "        self.kmeans = None\n",
        "        self.scaler = None\n",
        "        self.arima_models = {}\n",
        "        self.lstm_model = None\n",
        "        self.road_to_idx = {}\n",
        "\n",
        "    def load_data(self, data_source):\n",
        "        \"\"\"Load data from file path or URL with validation\"\"\"\n",
        "        try:\n",
        "            if isinstance(data_source, pd.DataFrame):\n",
        "                self.df = data_source.copy()\n",
        "            elif str(data_source).startswith('http'):\n",
        "                self.df = pd.read_csv(data_source, parse_dates=['Date'])\n",
        "            else:\n",
        "                self.df = pd.read_csv(data_source, parse_dates=['Date'])\n",
        "\n",
        "            # Validate required columns\n",
        "            req_cols = ['Date', 'Hour', 'Road', 'Average_Speed', 'Average_Occupancy', 'Total_Volume']\n",
        "            if not all(col in self.df.columns for col in req_cols):\n",
        "                missing = [col for col in req_cols if col not in self.df.columns]\n",
        "                raise ValueError(f\"Missing required columns: {missing}\")\n",
        "\n",
        "            # Create datetime index\n",
        "            self.df['datetime'] = pd.to_datetime(self.df['Date'].astype(str) + ' ' + self.df['Hour'].astype(str) + ':00:00')\n",
        "            self.df.set_index('datetime', inplace=True)\n",
        "\n",
        "            # Add temporal features\n",
        "            self.df['day_of_week'] = self.df.index.dayofweek\n",
        "            self.df['hour'] = self.df.index.hour\n",
        "            self.df['is_weekend'] = self.df['day_of_week'].isin([5,6]).astype(int)\n",
        "\n",
        "            # Convert categorical columns\n",
        "            self.df['Road'] = self.df['Road'].astype('category')\n",
        "            self.df['Lane'] = self.df['Lane'].astype('category')\n",
        "            self.df['Direction'] = self.df['Direction'].astype('category')\n",
        "\n",
        "            print(f\"Data loaded successfully with {len(self.df)} rows\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading data: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _plot_clusters(self):\n",
        "        \"\"\"Interactive cluster visualization using Plotly\"\"\"\n",
        "        fig = go.Figure()\n",
        "\n",
        "        for state in sorted(self.df['traffic_state'].dropna().unique()):\n",
        "            state_data = self.df[self.df['traffic_state'] == state]\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=state_data['Average_Speed'],\n",
        "                y=state_data['Average_Occupancy'],\n",
        "                mode='markers',\n",
        "                name=f'State {state}: {state_data[\"traffic_state_label\"].iloc[0]}',\n",
        "                marker=dict(\n",
        "                    size=8,\n",
        "                    opacity=0.7,\n",
        "                    line=dict(width=0.5, color='white')\n",
        "            )))\n",
        "\n",
        "        fig.update_layout(\n",
        "            title='Traffic State Clusters',\n",
        "            xaxis_title='Average Speed',\n",
        "            yaxis_title='Average Occupancy',\n",
        "            hovermode='closest',\n",
        "            height=600\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    def cluster_traffic_states(self):\n",
        "        \"\"\"Classify traffic states using clustering with enhanced visualization\"\"\"\n",
        "        try:\n",
        "            # Feature engineering\n",
        "            self.df['flow_rate'] = np.where(\n",
        "                self.df['Average_Occupancy'] > 0,\n",
        "                self.df['Total_Volume'] / self.df['Average_Occupancy'],\n",
        "                self.df['Total_Volume'] / 0.01  # Avoid division by zero\n",
        "            )\n",
        "\n",
        "            # Select features and drop NA\n",
        "            features = ['Average_Speed', 'Average_Occupancy', 'Total_Volume', 'flow_rate']\n",
        "            valid_idx = self.df[features].notna().all(axis=1)\n",
        "            X = self.df.loc[valid_idx, features]\n",
        "\n",
        "            # Scale features\n",
        "            self.scaler = StandardScaler()\n",
        "            X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "            # Cluster using KMeans\n",
        "            self.kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\n",
        "            clusters = self.kmeans.fit_predict(X_scaled)\n",
        "\n",
        "            # Initialize column with NA values\n",
        "            self.df['traffic_state'] = np.nan\n",
        "            # Only assign clusters to rows with valid data\n",
        "            self.df.loc[valid_idx, 'traffic_state'] = clusters\n",
        "\n",
        "            # Label states\n",
        "            state_labels = {\n",
        "                0: 'Free Flow',\n",
        "                1: 'Light Congestion',\n",
        "                2: 'Heavy Congestion',\n",
        "                3: 'Severe Jam'\n",
        "            }\n",
        "            self.df['traffic_state_label'] = self.df['traffic_state'].map(state_labels)\n",
        "\n",
        "            # Interactive visualization\n",
        "            self._plot_clusters()\n",
        "\n",
        "            # Cluster statistics heatmap\n",
        "            cluster_stats = self.df.groupby('traffic_state_label')[features].mean()\n",
        "            plt.figure(figsize=(12, 5))\n",
        "            sns.heatmap(cluster_stats.T, annot=True, cmap='YlGnBu', fmt='.1f')\n",
        "            plt.title('Average Features by Traffic State')\n",
        "            plt.show()\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Clustering failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _plot_arima_results(self, road, actual, fitted):\n",
        "        \"\"\"Interactive ARIMA visualization\"\"\"\n",
        "        fig = go.Figure()\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=actual.index, y=actual,\n",
        "            name='Actual',\n",
        "            line=dict(color='blue')\n",
        "        ))\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=fitted.index, y=fitted,\n",
        "            name='Fitted',\n",
        "            line=dict(color='red', dash='dot')\n",
        "        ))\n",
        "        fig.update_layout(\n",
        "            title=f'ARIMA Model Fit - {road}',\n",
        "            xaxis_title='Date',\n",
        "            yaxis_title='Traffic State',\n",
        "            hovermode='x unified',\n",
        "            height=500\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    def build_arima_models(self):\n",
        "        \"\"\"Build ARIMA models with enhanced visualization\"\"\"\n",
        "        if not ARIMA_AVAILABLE:\n",
        "            print(\"ARIMA functionality not available\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            road_stats = []\n",
        "\n",
        "            for road in self.df['Road'].cat.categories:\n",
        "                road_data = self.df[self.df['Road'] == road].copy()\n",
        "\n",
        "                # Resample to hourly frequency\n",
        "                numeric_cols = road_data.select_dtypes(include=np.number).columns\n",
        "                road_df = road_data[numeric_cols].resample('h').mean().ffill()\n",
        "\n",
        "                if len(road_df) < 24:  # Minimum data requirement\n",
        "                    print(f\"Skipping {road} - insufficient data ({len(road_df)} samples)\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Use auto_arima to find best parameters\n",
        "                    model = auto_arima(\n",
        "                        road_df['traffic_state'],\n",
        "                        seasonal=False,\n",
        "                        suppress_warnings=True,\n",
        "                        error_action='ignore',\n",
        "                        trace=True\n",
        "                    )\n",
        "\n",
        "                    self.arima_models[road] = model\n",
        "\n",
        "                    # Generate predictions and plot\n",
        "                    fitted = model.predict_in_sample()\n",
        "                    self._plot_arima_results(road, road_df['traffic_state'], fitted)\n",
        "\n",
        "                    # Store model statistics\n",
        "                    road_stats.append({\n",
        "                        'Road': road,\n",
        "                        'Order': str(model.order),\n",
        "                        'AIC': model.aic(),\n",
        "                        'RMSE': np.sqrt(np.mean((fitted - road_df['traffic_state'])**2))\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"ARIMA failed for {road}: {str(e)}\")\n",
        "\n",
        "            # Display model comparison\n",
        "            if road_stats:\n",
        "                stats_df = pd.DataFrame(road_stats)\n",
        "                print(\"\\nARIMA Model Performance Comparison:\")\n",
        "                display(stats_df.sort_values('AIC'))\n",
        "\n",
        "                # Plot ACF/PACF for best model\n",
        "                best_road = stats_df.loc[stats_df['AIC'].idxmin(), 'Road']\n",
        "                best_data = self.df[self.df['Road'] == best_road]['traffic_state'].dropna()\n",
        "\n",
        "                fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
        "                plot_acf(best_data, ax=ax1, lags=24, title=f\"ACF - {best_road}\")\n",
        "                plot_pacf(best_data, ax=ax2, lags=24, title=f\"PACF - {best_road}\")\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "            print(f\"\\nSuccessfully built ARIMA models for {len(self.arima_models)} roads\")\n",
        "            return len(self.arima_models) > 0\n",
        "        except Exception as e:\n",
        "            print(f\"ARIMA modeling error: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _plot_lstm_training(self, history):\n",
        "        \"\"\"Interactive LSTM training history\"\"\"\n",
        "        fig = make_subplots(rows=1, cols=2, subplot_titles=('Accuracy', 'Loss'))\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                y=history.history['accuracy'],\n",
        "                name='Train Accuracy',\n",
        "                mode='lines',\n",
        "                line=dict(color='blue')\n",
        "            ), row=1, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                y=history.history['val_accuracy'],\n",
        "                name='Validation Accuracy',\n",
        "                mode='lines',\n",
        "                line=dict(color='orange')\n",
        "            ), row=1, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                y=history.history['loss'],\n",
        "                name='Train Loss',\n",
        "                mode='lines',\n",
        "                line=dict(color='blue')\n",
        "            ), row=1, col=2\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                y=history.history['val_loss'],\n",
        "                name='Validation Loss',\n",
        "                mode='lines',\n",
        "                line=dict(color='orange')\n",
        "            ), row=1, col=2\n",
        "        )\n",
        "\n",
        "        fig.update_layout(\n",
        "            title_text='LSTM Training History',\n",
        "            showlegend=True,\n",
        "            height=400,\n",
        "            width=900\n",
        "        )\n",
        "        fig.show()\n",
        "\n",
        "    def build_lstm_model(self):\n",
        "        \"\"\"Build LSTM model with comprehensive visualization\"\"\"\n",
        "        if not LSTM_AVAILABLE:\n",
        "            print(\"LSTM functionality not available\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            # Prepare road embeddings\n",
        "            roads = self.df['Road'].cat.categories\n",
        "            self.road_to_idx = {road: i for i, road in enumerate(roads)}\n",
        "            self.df['road_idx'] = self.df['Road'].map(self.road_to_idx)\n",
        "\n",
        "            # Features and target\n",
        "            features = ['Average_Speed', 'Average_Occupancy', 'Total_Volume',\n",
        "                       'day_of_week', 'hour', 'is_weekend']\n",
        "            target = 'traffic_state'\n",
        "\n",
        "            # Scale features\n",
        "            feature_scaler = MinMaxScaler()\n",
        "            X_scaled = feature_scaler.fit_transform(self.df[features].fillna(0))\n",
        "\n",
        "            # Create sequences\n",
        "            seq_length = 24  # 24-hour lookback\n",
        "            X_seq, X_road, y_seq = [], [], []\n",
        "\n",
        "            for road in roads:\n",
        "                road_data = self.df[self.df['Road'] == road]\n",
        "                if len(road_data) > seq_length + 1:\n",
        "                    for i in range(len(road_data)-seq_length-1):\n",
        "                        X_seq.append(X_scaled[i:i+seq_length])\n",
        "                        X_road.append(road_data.iloc[i:i+seq_length]['road_idx'].values)\n",
        "                        y_seq.append(road_data.iloc[i+seq_length][target])\n",
        "\n",
        "            if len(X_seq) < 100:\n",
        "                raise ValueError(f\"Only {len(X_seq)} sequences (need >=100)\")\n",
        "\n",
        "            # Convert to arrays\n",
        "            X_seq = np.array(X_seq)\n",
        "            X_road = np.array(X_road)\n",
        "            y_categorical = to_categorical(np.array(y_seq), num_classes=4)\n",
        "\n",
        "            # Build model\n",
        "            num_roads = len(roads)\n",
        "            main_input = Input(shape=(seq_length, len(features)), name='main_input')\n",
        "            lstm_out = LSTM(32)(main_input)\n",
        "            road_input = Input(shape=(seq_length,), name='road_input')\n",
        "            road_embed = Embedding(num_roads, 4)(road_input)\n",
        "            road_flatten = Flatten()(road_embed)\n",
        "            combined = Concatenate()([lstm_out, road_flatten])\n",
        "            output = Dense(4, activation='softmax')(combined)\n",
        "\n",
        "            self.lstm_model = Model(inputs=[main_input, road_input], outputs=output)\n",
        "            self.lstm_model.compile(\n",
        "                optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "            )\n",
        "\n",
        "            # Train model\n",
        "            split = int(0.8 * len(X_seq))\n",
        "            history = self.lstm_model.fit(\n",
        "                [X_seq[:split], X_road[:split]],\n",
        "                y_categorical[:split],\n",
        "                epochs=15,\n",
        "                batch_size=32,\n",
        "                validation_data=([X_seq[split:], X_road[split:]], y_categorical[split:]),\n",
        "                verbose=1\n",
        "            )\n",
        "\n",
        "            # Visualizations\n",
        "            self._plot_lstm_training(history)\n",
        "\n",
        "            # Confusion matrix\n",
        "            y_pred = np.argmax(self.lstm_model.predict([X_seq[split:], X_road[split:]], verbose=0), axis=1)\n",
        "            y_true = np.argmax(y_categorical[split:], axis=1)\n",
        "\n",
        "            cm = confusion_matrix(y_true, y_pred)\n",
        "            fig = px.imshow(\n",
        "                cm,\n",
        "                labels=dict(x=\"Predicted\", y=\"True\", color=\"Count\"),\n",
        "                x=['Free', 'Light', 'Heavy', 'Severe'],\n",
        "                y=['Free', 'Light', 'Heavy', 'Severe'],\n",
        "                text_auto=True,\n",
        "                aspect=\"auto\",\n",
        "                color_continuous_scale='Blues'\n",
        "            )\n",
        "            fig.update_layout(title='Confusion Matrix')\n",
        "            fig.show()\n",
        "\n",
        "            # Classification report\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(y_true, y_pred,target_names=['Free Flow', 'Light Congestion','Heavy Congestion', 'Severe Jam']))\n",
        "\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"LSTM modeling error: {e}\")\n",
        "            return False"
      ],
      "metadata": {
        "id": "XLnqEqmqOAGw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    analyzer = TrafficAnalysisSystem()\n",
        "\n",
        "    # Load data (use either option)\n",
        "    data_source = \"https://raw.githubusercontent.com/sytong12/SEEM3650project/refs/heads/main/aggregated_hourly_data.csv\"  # or local path\n",
        "    if not analyzer.load_data(data_source):\n",
        "        print(\"Failed to load data. Exiting.\")\n",
        "        exit()\n",
        "\n",
        "    # Cluster traffic states\n",
        "    if not analyzer.cluster_traffic_states():\n",
        "        print(\"Warning: Clustering failed or produced suboptimal results\")\n",
        "\n",
        "    # Build ARIMA models if available\n",
        "    if ARIMA_AVAILABLE:\n",
        "        if not analyzer.build_arima_models():\n",
        "            print(\"Warning: ARIMA modeling failed for all roads\")\n",
        "\n",
        "    # Build LSTM model if available\n",
        "    if LSTM_AVAILABLE:\n",
        "        if not analyzer.build_lstm_model():\n",
        "            print(\"Warning: LSTM modeling failed\")\n",
        "\n",
        "    print(\"\\nAnalysis completed\")"
      ],
      "metadata": {
        "id": "lAAyhUKmOQ6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}